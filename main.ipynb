{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Algo Project test3",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheIndianCoder/Choose-the-best-Optimization-Algorithm-in-keras/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "AKia3N-oVuVa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ***Project to choose the best Optimization Algorithm for a given Keras Model ***\n",
        "\n",
        "Hi, let's get staright to the algorithm to choose algorithm:\n",
        "        \n",
        "        For each optimisation algorithm  {\n",
        "\t                     \tFor i in range(-5,1){\n",
        "\t\t                                \tRun 1 epoch with learning rate 10i \n",
        "                                    }\n",
        "                       Choose the value of i which gives the best result\n",
        "                       Run 50 epochs with the 10i as learning rate and store results.\n",
        "                }\n",
        "        Compare the results and choose the best performer. \n"
      ]
    },
    {
      "metadata": {
        "id": "59XRJiIfXqX3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, as proof of concept, we show the results of our algorithm on the[CIFAR 10](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "\n",
        "\n",
        "# Now, \n",
        "  There are 7 optimization Algorithms we're concerned with becasue they are present in the    [Keras.Optimizers](https://keras.io/optimizers/) class. Namely\n",
        "   \n",
        "   1. [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
        "   \n",
        "   2. [RMS Prop](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n",
        "   \n",
        "   3.  [ADAGRAD](https://arxiv.org/abs/1705.08292)\n",
        "  \n",
        "   4. [ADADELTA](https://arxiv.org/abs/1212.5701)\n",
        "  \n",
        "   5. [ADAM](https://arxiv.org/abs/1412.6980)\n",
        "   \n",
        "   6. [ADAMAX](https://arxiv.org/abs/1412.6980)\n",
        "   \n",
        "   7. [NADAM](http://cs229.stanford.edu/proj2015/054_report.pdf)\n",
        "   \n",
        "# As an example\n",
        "We use the following CNN model as described [here](https://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/ ):\n",
        "\n",
        "![](https://github.com/TheIndianCoder/Choose-the-best-Optimization-Algorithm-in-keras/blob/master/utils/images/git1.png?raw=true)\n",
        "\n",
        "   "
      ]
    },
    {
      "metadata": {
        "id": "1sM0Oca4fckO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 1\n",
        "  \n",
        "  Lets import the required libraries"
      ]
    },
    {
      "metadata": {
        "id": "auMaOUVffk-Y",
        "colab_type": "code",
        "outputId": "2e13c6f4-4b10-4b3a-8f48-6c0dcd8d044f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Dec  5 09:45:29 2018\n",
        "\n",
        "@author: AMY\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "from pylab import rcParams\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from \"C:\\Users\\AMY\\Desktop\\Algo project\" import cifar10 as cifar10\n",
        "from matplotlib import pyplot\n",
        "from scipy.misc import toimage\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.constraints import maxnorm\n",
        "from keras import optimizers\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "import time"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "a_WzqZuefmaA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 2\n",
        "\n",
        "1. Import the **CIFAR 10 dataset**:\n",
        "\n",
        "2. Although the dataset has 60,000 images, we reuire only 20,000 for demonstration purpose\n",
        "\n",
        "3. Also, we reshape the input dataset to divide it into **X_train ,y_train,X_test and y_test**\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Jqw_rjNUg70p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "\n",
        "# create a grid of 3x3 images\n",
        "for i in range(0, 9):\n",
        "  pyplot.subplot(330 + 1 + i)\n",
        "  pyplot.imshow(toimage(X_train[i]))\n",
        "  \n",
        "# show the plot\n",
        "pyplot.show()\n",
        "# Simple CNN model for CIFAR-10\n",
        "\n",
        "K.set_image_dim_ordering('th')\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "\n",
        "\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "\n",
        "# normalize inputs from 0-255 to 0.0-1.0\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "\n",
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "\n",
        "#Reshaaping the 50,000 examples to 4096 for train and 500 for test\n",
        "X_train=X_train[0:20000,:,:,:]\n",
        "y_train=y_train[0:20000,:]\n",
        "X_test=X_test[0:2000,:,:,:]\n",
        "y_test=y_test[0:2000,:]\n",
        "print(\"shapes: Xtrain \",X_train.shape,\" Y_train :\",y_train.shape,\" Xtest \",X_test.shape,\" Y_test :\",y_test.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SjIfSrgGhQU0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Output**:\n",
        "The output should contain the following images as an indication that the dataset has been loaded and is ready to use:\n",
        "![alt text](https://github.com/TheIndianCoder/Choose-the-best-Optimization-Algorithm-in-keras/blob/master/utils/images/output1.PNG?raw=true)\n"
      ]
    },
    {
      "metadata": {
        "id": "DqIuxD6WjW_4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 3\n",
        " **Create CNN models for each of the 7 optimization algorithms**\n",
        "  \n",
        "   Each moel returns the accuracy, time taken and a [Keras History object](https://keras.io/callbacks/#history) which we'll need to plot the graphs."
      ]
    },
    {
      "metadata": {
        "id": "Hhm5bNhWjlgP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "def ConvModel_SGD(alpha=0.01,epochs=1,loss_fun='categorical_crossentropy'):\n",
        "    print(\"\\nUsing SGD with alpha : \",alpha)\n",
        "    \n",
        "    start=time.time()\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    # Compile model\n",
        "    lrate = alpha\n",
        "    decay = lrate/epochs\n",
        "    sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    \n",
        "    #print(model.summary())\n",
        "    \n",
        "    # Fit the model\n",
        "    hist=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=128)\n",
        "    \n",
        "    # Final evaluation of the model\n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    end=time.time()\n",
        "    \n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "    \n",
        "    return [scores[1]*100,(end-start),hist]\n",
        "\n",
        "def ConvModel_RMS(alpha=0.01,epochs=1,loss_fun='categorical_crossentropy'):\n",
        "    print(\"\\nUsing RMS with alpha : \",alpha)\n",
        "    \n",
        "    start=time.time()\n",
        "    \n",
        "    RMS=keras.optimizers.RMSprop\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    # Compile model\n",
        "    lrate = alpha\n",
        "    decay = lrate/epochs\n",
        "    rms=RMS(lr=lrate,decay=decay)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])\n",
        "   \n",
        "  # print(model.summary())\n",
        "    \n",
        "    # Fit the model\n",
        "    hist=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=128)\n",
        "    \n",
        "    # Final evaluation of the model\n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    end=time.time()\n",
        "    \n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "    \n",
        "    return [scores[1]*100,(end-start),hist]\n",
        "\n",
        "\n",
        "  \n",
        "def ConvModel_ADAGrad(alpha=0.01,epochs=1):\n",
        "    print(\"\\nUsing ADAGrad with alpha : \",alpha)\n",
        "    start=time.time()\n",
        "    ADAGRAD=keras.optimizers.Adagrad\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    # Compile model\n",
        "    \n",
        "    lrate = alpha\n",
        "    decay = lrate/epochs\n",
        "    ADA=ADAGRAD(lr=lrate,decay=decay)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=ADA, metrics=['accuracy'])\n",
        "   # print(model.summary())\n",
        "    \n",
        "    # Fit the model\n",
        "    hist=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=128)\n",
        "    # Final evaluation of the model\n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    \n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "    #print(\"Scores shape :\",scores.shape)\n",
        "    \n",
        "    end=time.time()\n",
        "    \n",
        "    return [scores[1]*100,(end-start),hist]\n",
        "\n",
        "\n",
        "  \n",
        "def ConvModel_ADADelta(alpha=0.01,epochs=1):\n",
        "    print(\"\\nUsing ADDelta with alpha : \",alpha)\n",
        "    start=time.time()\n",
        "    \n",
        "    ADADEL=keras.optimizers.Adadelta\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    # Compile model\n",
        "    \n",
        "    lrate = alpha\n",
        "    decay = lrate/epochs\n",
        "    ADADEL=ADADEL(lr=lrate,decay=decay)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=ADADEL, metrics=['accuracy'])\n",
        "    \n",
        "    #print(model.summary())\n",
        "    \n",
        "    # Fit the model\n",
        "    hist=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=128)\n",
        "    \n",
        "    # Final evaluation of the model\n",
        "    \n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "    \n",
        "    end=time.time()\n",
        "    \n",
        "    return [scores[1]*100,(end-start),hist]\n",
        "#ConvModel_ADADelta()\n",
        "\n",
        "\n",
        "def ConvModel_ADAM(alpha=0.01,epochs=1):\n",
        "    print(\"\\nUsing ADAM with alpha : \",alpha)\n",
        "    start=time.time()\n",
        "    \n",
        "    ADAM=optimizers.Adam\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    # Compile model\n",
        "    \n",
        "    lrate = alpha\n",
        "    decay = lrate/epochs\n",
        "    ADAM=ADAM(lr=lrate,decay=decay)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=ADAM, metrics=['accuracy'])\n",
        "    \n",
        "    #print(model.summary())\n",
        "    # Fit the model\n",
        "    \n",
        "    hist=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=128)\n",
        "    \n",
        "    # Final evaluation of the model\n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "    \n",
        "    end=time.time()\n",
        "    \n",
        "    return [scores[1]*100,(end-start),hist]\n",
        "#ConvModel_ADAM()\n",
        "\n",
        "\n",
        "\n",
        "def ConvModel_ADAMAX(alpha=0.01,epochs=1):\n",
        "\n",
        "    print(\"\\nUsing ADAMAX with alpha : \",alpha)\n",
        "    start=time.time()\n",
        "    \n",
        "    ADAMAX=keras.optimizers.Adamax\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    # Compile model\n",
        "    \n",
        "    lrate = alpha\n",
        "    decay = lrate/epochs\n",
        "    ADAMAX=ADAMAX(lr=lrate,decay=decay)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=ADAMAX, metrics=['accuracy'])\n",
        "    #print(model.summary())\n",
        "    \n",
        "    # Fit the model\n",
        "    hist=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=128)\n",
        "    \n",
        "    # Final evaluation of the model\n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "    \n",
        "    end=time.time()\n",
        "    \n",
        "    return [scores[1]*100,(end-start),hist]\n",
        "  \n",
        "  #ConvModel_ADAMAX()\n",
        "\n",
        "  \n",
        "def ConvModel_NADAM(alpha=0.01,epochs=1):\n",
        "    \n",
        "    print(\"\\nUsing NADAM with alpha : \",alpha)\n",
        "    start=time.time()\n",
        "    \n",
        "    NADAM=optimizers.Nadam\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    # Compile model\n",
        "    \n",
        "    lrate = alpha\n",
        "    NADAM=NADAM(lr=lrate)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=NADAM, metrics=['accuracy'])\n",
        "   # print(model.summary())\n",
        "    \n",
        "    # Fit the model\n",
        "    hist=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=128)\n",
        "    \n",
        "    # Final evaluation of the model\n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "    end=time.time()\n",
        "    return [scores[1]*100,(end-start),hist]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "thyZuHmYlD97",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After Compiling the models successfully, we now decide which **learning rate to choose for each Optimization algorithm** we choose the best from\n",
        "                        \n",
        "                        [0.00001, 0.0001, 0.001, 0.01, 0.1 , 1, 10] \n",
        "\n",
        "\n",
        "   by     **training each model for one epoch and comparing the results**"
      ]
    },
    {
      "metadata": {
        "id": "08nDQx-5zPJ3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Choose the best learning rates for each Algorithm by passing through one epoch each\n",
        "\n",
        "'''\n",
        "def CompareAlgos_alpha():\n",
        "    lr_list=[0.00001,0.0001,0.001,0.001,0.01,0.1,1,10]\n",
        "    '''\n",
        "      every algorithm in the deictionary has corresponding two values in the list : accuracy and time\n",
        "      \n",
        "      100*accuracy  - time_spent is used as a metric here to compare the results of different learning rates for each Algorithm. The reader is free to use another metric\n",
        "    '''\n",
        "    result_lr_list={'SGD':[0,0],'NADAM':[0,0],'ADAMAX':[0,0],'RMS':[0,0],'ADAM':[0,0],'ADAGrad':[0,0],'ADADelta':[0,0]}\n",
        "    \n",
        "    \n",
        "    for i in lr_list:\n",
        "        print(\"Calling SGD with lr= \",i)\n",
        "        [acc,time_spent,_]=ConvModel_SGD(alpha=i)\n",
        "        if(result_lr_list['SGD'][0]<(100*acc-time_spent)):\n",
        "            result_lr_list['SGD'][0]=100*acc-time_spent\n",
        "            result_lr_list['SGD'][1]=i\n",
        "    \n",
        "    for i in lr_list:\n",
        "        print(\"Calling RMS with lr= \",i)\n",
        "        [acc,time_spent,_]=ConvModel_RMS(alpha=i)\n",
        "        if(result_lr_list['RMS'][0]<(100*acc-time_spent)):\n",
        "            result_lr_list['RMS'][0]=100*acc-time_spent\n",
        "            result_lr_list['RMS'][1]=i\n",
        "   \n",
        "    for i in lr_list:\n",
        "        print(\"Calling ADAGrad with lr= \",i)\n",
        "        [acc,time_spent,_]=ConvModel_ADAGrad(alpha=i)\n",
        "        if(result_lr_list['ADAGrad'][0]<(100*acc-time_spent)):\n",
        "            result_lr_list['ADAGrad'][0]=100*acc-time_spent\n",
        "            result_lr_list['ADAGrad'][1]=i\n",
        "\n",
        "    for i in lr_list:\n",
        "        print(\"Calling ADADelta with lr= \",i)\n",
        "        [acc,time_spent,_]=ConvModel_ADADelta(alpha=i)\n",
        "        if(result_lr_list['ADADelta'][0]<(100*acc-time_spent)):\n",
        "            result_lr_list['ADADelta'][0]=100*acc-time_spent\n",
        "            result_lr_list['ADADelta'][1]=i\n",
        "            \n",
        "    for i in lr_list:\n",
        "        print(\"Calling ADAM with lr= \",i)\n",
        "        [acc,time_spent,_]=ConvModel_ADAM(alpha=i)\n",
        "        if(result_lr_list['ADAM'][0]<(100*acc-time_spent)):\n",
        "            result_lr_list['ADAM'][0]=100*acc-time_spent\n",
        "            result_lr_list['ADAM'][1]=i\n",
        "    \n",
        "    for i in lr_list:\n",
        "        print(\"Calling ADAMAX with lr= \",i)\n",
        "        [acc,time_spent,_]=ConvModel_ADAMAX(alpha=i)\n",
        "        if(result_lr_list['ADAMAX'][0]<(100*acc-time_spent)):\n",
        "            result_lr_list['ADAMAX'][0]=100*acc-time_spent\n",
        "            result_lr_list['ADAMAX'][1]=i\n",
        "    \n",
        "    for i in lr_list:\n",
        "        print(\"Calling NADAM with lr= \",i)\n",
        "        [acc,time_spent,_]=ConvModel_NADAM(alpha=i)\n",
        "        if(result_lr_list['NADAM'][0]<(100*acc-time_spent)):\n",
        "            result_lr_list['NADAM'][0]=100*acc-time_spent\n",
        "            result_lr_list['NADAM'][1]=i\n",
        "    print(result_lr_list)\n",
        "    return result_lr_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y0RMvTTQ0Ldk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We call the above function to get the best learning rates for each Algorithm."
      ]
    },
    {
      "metadata": {
        "id": "Up3mi-J10VJm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "result_lr=CompareAlgos_alpha()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NgpJHxhR0sqq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 4:\n",
        "  Now that we have the ebst learning rates.,\n",
        "        we choose the best optimization algorithm among the seven.\n",
        "    \n",
        "   **For this , we create the function CompareAlgos_main() as shown below**"
      ]
    },
    {
      "metadata": {
        "id": "bT3oSK5SIVi1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def CompareAlgos_main(result_dict):\n",
        "  \n",
        "    '''\n",
        "    Input: result_dict: A python dictionary conataining the result returned from ComapareAlgos_alpha().\n",
        "    \n",
        "    Output: Dictionary containing the list which is of the form [accuracy, time_spent, model_history]\n",
        "  \n",
        "    '''\n",
        "    result=result_dict\n",
        "    result_algos={'SGD':[0,0],'NADAM':[0,0],'ADAMAX':[0,0],'RMS':[0,0],'ADAM':[0,0],'ADAGrad':[0,0],'ADADelta':[0,0]}\n",
        "\n",
        "    temp=ConvModel_ADAMAX(result['ADAMAX'][1],epochs=50)\n",
        "    result_algos['ADAMAX'][0]=temp[0]\n",
        "    result_algos['ADAMAX'][1]=temp[1]\n",
        "    result_algos['ADAMAX'].append(temp[2])\n",
        "\n",
        "    temp=ConvModel_SGD(result['SGD'][1],epochs=50)\n",
        "    result_algos['SGD'][0]=temp[0]\n",
        "    result_algos['SGD'][1]=temp[1]\n",
        "    result_algos['SGD'].append(temp[2])\n",
        "\n",
        "    temp=ConvModel_RMS(result['RMS'][1],epochs=50)\n",
        "    result_algos['RMS'][0]=temp[0]\n",
        "    result_algos['RMS'][1]=temp[1]\n",
        "    result_algos['RMS'].append(temp[2])\n",
        "\n",
        "    temp=ConvModel_ADAM(result['ADAM'][1],epochs=50)\n",
        "    result_algos['ADAM'][0]=temp[0]\n",
        "    result_algos['ADAM'][1]=temp[1]\n",
        "    result_algos['ADAM'].append(temp[2])\n",
        "\n",
        "    temp=ConvModel_ADAGrad(result['ADAGrad'][1],epochs=50)\n",
        "    result_algos['ADAGrad'][0]=temp[0]\n",
        "    result_algos['ADAGrad'][1]=temp[1]\n",
        "    result_algos['ADAGrad'].append(temp[2])\n",
        "\n",
        "    temp=ConvModel_ADADelta(result['ADADelta'][1],epochs=50)\n",
        "    result_algos['ADADelta'][0]=temp[0]\n",
        "    result_algos['ADADelta'][1]=temp[1]\n",
        "    result_algos['ADADelta'].append(temp[2])\n",
        "\n",
        "    temp=ConvModel_NADAM(result['NADAM'][1],epochs=50)\n",
        "    result_algos['NADAM'][0]=temp[0]\n",
        "    result_algos['NADAM'][1]=temp[1]\n",
        "    result_algos['NADAM'].append(temp[2])\n",
        "\n",
        "    return result_algos\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtGNxWHH2NkH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we call the above fnction to get the best Algorithm for our model."
      ]
    },
    {
      "metadata": {
        "id": "LKn4hJ0X2Wjw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "result_algo=CompareAlgos_main(result_lr)\n",
        "\n",
        "print(\"\\n\\nThe accuracy order is \")\n",
        "for i in sorted(result_algos.items(),key=lambda kv:kv[1][0],reverse=True):\n",
        "   print(i[0],\" :\" ,i[1][0])\n",
        "\n",
        "print(\"\\n\\n\\nThe time order is \")\n",
        "for i in sorted(result_algos.items(),key=lambda kv:kv[1][1],reverse=True):\n",
        "   print(i[0],\" :\" ,i[1][1]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lrLx6zt44z1Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 5:\n",
        "   **Plot the result**\n",
        "            The history objects will come in handy now\n",
        "            \n",
        "            First, lets plot the accuracy graph."
      ]
    },
    {
      "metadata": {
        "id": "sSHXxN4d5AX0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hist_ADADelta=result_algo['ADADelta'][2]\n",
        "hist_ADAGrad=result_algo['ADAGrad'][2]\n",
        "hist_ADAM=result_algo['ADAM'][2]\n",
        "hist_ADAMAX=result_algo['ADAMAX'][2]\n",
        "hist_NADAM=result_algo['NADAM'][2]\n",
        "hist_RMS=result_algo['RMS'][2]\n",
        "hist_SGD=result_algo['SGD'][2]\n",
        "\n",
        "\n",
        "plt.plot(hist_ADADelta.history['acc'])\n",
        "#plt.plot(hist_ADADelta.history['val_acc'])\n",
        "plt.plot(hist_ADAGrad.history['acc'])\n",
        "plt.plot(hist_ADAM.history['acc'])\n",
        "plt.plot(hist_ADAMAX.history['acc'])\n",
        "plt.plot(hist_NADAM.history['acc'])\n",
        "plt.plot(hist_RMS.history['acc'])\n",
        "plt.plot(hist_SGD.history['acc'])\n",
        "\n",
        "\n",
        "#plt.plot(hist_ADAGrad.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['ADADelta', 'ADAGrad','ADAM','ADAMAX','NADAM','RMS','SGD'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ttp7MXoT5ciq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, lets plot the error/loss graph:"
      ]
    },
    {
      "metadata": {
        "id": "sX2OQf6G5kxx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(hist_ADADelta.history['loss'])\n",
        "#plt.plot(hist_ADADelta.history['val_acc'])\n",
        "plt.plot(hist_ADAGrad.history['loss'])\n",
        "plt.plot(hist_ADAM.history['loss'])\n",
        "plt.plot(hist_ADAMAX.history['loss'])\n",
        "plt.plot(hist_NADAM.history['loss'])\n",
        "plt.plot(hist_RMS.history['loss'])\n",
        "plt.plot(hist_SGD.history['loss'])\n",
        "\n",
        "\n",
        "#plt.plot(hist_ADAGrad.history['val_acc'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['ADADelta', 'ADAGrad','ADAM','ADAMAX','NADAM','RMS','SGD'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "85irfz4x52X1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Great..! The best optimisation algorithm can now be chosen easily, just by a look at the graph!"
      ]
    },
    {
      "metadata": {
        "id": "eui6vpJr6PYs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Thanks to **Google Colab** for the online GPUs for training.\n",
        "\n",
        "Again, the CNN model used here as an example has been taken from  [Jason Brownie at Machine Learning Mastery website](https://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/ ).\n"
      ]
    },
    {
      "metadata": {
        "id": "9QMe6mBY6YTL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}