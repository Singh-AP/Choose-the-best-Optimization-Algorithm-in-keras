{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Algo Project test3",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheIndianCoder/Choose-the-best-Optimization-Algorithm-in-keras/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "AKia3N-oVuVa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ***Project to choose the best Optimization Algorithm for a given Keras Model ***\n",
        "\n",
        "Hi, let's get staright to the algorithm to choose algorithm:\n",
        "        \n",
        "        For each optimisation algorithm  {\n",
        "\t                     \tFor i in range(-5,1){\n",
        "\t\t                                \tRun 1 epoch with learning rate 10i \n",
        "                                    }\n",
        "                       Choose the value of i which gives the best result\n",
        "                       Run 50 epochs with the 10i as learning rate and store results.\n",
        "                }\n",
        "        Compare the results and choose the best performer. \n"
      ]
    },
    {
      "metadata": {
        "id": "59XRJiIfXqX3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, as proof of concept, we show the results of our algorithm on the[CIFAR 10](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "\n",
        "\n",
        "# Now, \n",
        "  There are 7 optimization Algorithms we're concerned with becasue they are present in the    [Keras.Optimizers](https://keras.io/optimizers/) class. Namely\n",
        "   \n",
        "   1. [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
        "   \n",
        "   2. [RMS Prop](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n",
        "   \n",
        "   3.  [ADAGRAD](https://arxiv.org/abs/1705.08292)\n",
        "  \n",
        "   4. [ADADELTA](https://arxiv.org/abs/1212.5701)\n",
        "  \n",
        "   5. [ADAM](https://arxiv.org/abs/1412.6980)\n",
        "   \n",
        "   6. [ADAMAX](https://arxiv.org/abs/1412.6980)\n",
        "   \n",
        "   7. [NADAM](http://cs229.stanford.edu/proj2015/054_report.pdf)\n",
        "   \n",
        "# As an example\n",
        "We use the following CNN model as described [here](https://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/ ):\n",
        "\n",
        "![](https://github.com/TheIndianCoder/Choose-the-best-Optimization-Algorithm-in-keras/blob/master/utils/images/git1.png?raw=true)\n",
        "\n",
        "   "
      ]
    },
    {
      "metadata": {
        "id": "1sM0Oca4fckO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 1\n",
        "  \n",
        "  Lets import the required libraries"
      ]
    },
    {
      "metadata": {
        "id": "auMaOUVffk-Y",
        "colab_type": "code",
        "outputId": "2e13c6f4-4b10-4b3a-8f48-6c0dcd8d044f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Dec  5 09:45:29 2018\n",
        "\n",
        "@author: AMY\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "from pylab import rcParams\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from \"C:\\Users\\AMY\\Desktop\\Algo project\" import cifar10 as cifar10\n",
        "from matplotlib import pyplot\n",
        "from scipy.misc import toimage\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.constraints import maxnorm\n",
        "from keras import optimizers\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "import time"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "a_WzqZuefmaA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 2\n",
        "\n",
        "1. Import the **CIFAR 10 dataset**:\n",
        "\n",
        "2. Although the dataset has 60,000 images, we reuire only 20,000 for demonstration purpose\n",
        "\n",
        "3. Also, we reshape the input dataset to divide it into **X_train ,y_train,X_test and y_test**\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Jqw_rjNUg70p",
        "colab_type": "code",
        "outputId": "76ea37ee-47b6-442e-ec03-acac3f3af8a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "cell_type": "code",
      "source": [
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "\n",
        "# create a grid of 3x3 images\n",
        "for i in range(0, 9):\n",
        "  pyplot.subplot(330 + 1 + i)\n",
        "  pyplot.imshow(toimage(X_train[i]))\n",
        "  \n",
        "# show the plot\n",
        "pyplot.show()\n",
        "# Simple CNN model for CIFAR-10\n",
        "\n",
        "K.set_image_dim_ordering('th')\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "\n",
        "\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "\n",
        "# normalize inputs from 0-255 to 0.0-1.0\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "\n",
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "\n",
        "#Reshaaping the 50,000 examples to 4096 for train and 500 for test\n",
        "X_train=X_train[0:20000,:,:,:]\n",
        "y_train=y_train[0:20000,:]\n",
        "X_test=X_test[0:2000,:,:,:]\n",
        "y_test=y_test[0:2000,:]\n",
        "print(\"shapes: Xtrain \",X_train.shape,\" Y_train :\",y_train.shape,\" Xtest \",X_test.shape,\" Y_test :\",y_test.shape)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `toimage` is deprecated!\n",
            "`toimage` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use Pillow's ``Image.fromarray`` directly instead.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbMAAAFNCAYAAACKdYHuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzsvXmcHdV1Lrqqzjyfnie1uqXWgJAQ\nIMwkMYMHHMLgCaI4uTg/Y3Jx4ji5DuZx/exLyPO1wSY/O9cJCX7gFye+kSPnEpvgCIgBMwgJZJCQ\nhNDULbV6nk6f+Zw6der9sffZ32roVktCaum09vfP2V1dp2rXqVW79vr2Wt8yHMdxSENDQ0NDo4ph\nnu4OaGhoaGhofFDol5mGhoaGRtVDv8w0NDQ0NKoe+mWmoaGhoVH10C8zDQ0NDY2qh36ZaWhoaGhU\nPdwn+sVvfvObtH37djIMg+6//35avXr1yeyXhsa00HanMdfQNlcdOKGX2datW+nQoUO0YcMGOnDg\nAN1///20YcOGk903DY0p0HanMdfQNlc9OCGacfPmzXTDDTcQEVFXVxdNTk5SOp0+qR3T0HgvtN1p\nzDW0zVUPTsgzGx0dpZUrV6q/a2traWRkhMLh8LT7X3H1NURE9A9PPEE333YLERH5zDK+74UIycK6\noGo31IaIiKg+juN6XR503hfASVziUsYnEmpTsYTj1sRjqm3aFhERFQoFtS2fz6u2P+BXbZtsIiLK\n5mDAsXgU53XE/++89wf09w/eJbpC6KPL5VLtCPt9QiFxbR4PzpUrFHFYg80zTHFtRfb/kmOo9hcf\nfJTOBhyv3T328+fok9dcRj974TU6smcbERGNdL+j/m/bMP+mheeo9sKuFUREVNO8UG3zB7Dv3l2v\nqvah/TuIiMhKwT5c7LjRGtid2y9s+5J1V6ltS5bhvPnJcdXetfNNIiIql3HPixZsdPeut+m//dn/\nTd995EFKJkaJiKhQhD1bRdjd+FhWtdNZcYySjX0bGmpVu6YWv6XtpMS+ltpE+RyeqSf/dRPNdxyv\nzb0X5XJ59p3mCkzryTAwfuQysI+xcWFLtbU1aptdhN0FgkFy+wJUKuTI5fWJw7Kxqkw4Lizw5MI0\np/fBTnjNjGM2Rax/eOIJWrx4ERER7dy+42Sc8ozE/X/zzOnuwlmF2ezuk9dcRrXRMN118w1EN99w\n0s77W5etOGnHmoL2VtVcvWrVMX3l4YfOjonMmYLjVf+baeA9kxCMhKZtA9H3bfEEpn+Zn6oXWAVH\nmxyc0MussbGRRkdH1d/Dw8PU0NAw4/6//7nPERHRyy++QKvOF4un880z+8pfPUXfvOcjoivaMzsl\nOF67+9kLr9FdN99Aj/38uXnpmT380KP05/f+ofbMTiGO1+bei/nomXkCYbJy6dPmmc2EE3qZrVu3\njv76r/+a7rjjDtq1axc1NjYe1e3etXsX2jveJiKiWozhZNThj3o7gu2BRiIiypTxkKdt3BHH8Kp2\nNi8e+myOPdA2DGnUhR/Z7xbHKJXwf5eJn8Ln87HjZsS+bFAx8nWqbbI7lhk9TEREATeuJ81eQON2\nSbWDQfEyM0y8+Az2oiY2o8vmxWhSsjCquNzo49mC47W75MS4+qyLiwHbaWhS/3fcmJS0LFys2nZZ\n/M5mGQ95OYt7l58YwzFy4kFvq29U2xa2L1Ht9iUdqt3atoCIiBob0QePB/exFMdErn1Bs9hWgv3k\n8znVTkyIl2dr6yIaHRXX6fbyhwqGWVOHc/hD4hiTyQm1zeeH7ZcdXKdH2lhykk0QC2eXLvnx2tx7\nUQ2eWSE7qdrjRw4SEVHvO9g2mcyo9rrrridPIEy5TJqiatKPazTYy2yur/yEXmZr1qyhlStX0h13\n3EGGYdA3vvGNk90vDY33QdudxlxD21z14ITXzL7yla8c874BN97WcTlJ7GDeWGcTqJhGRnkEKt4L\nd4kLcHnzFrwwR+7jDTDqkdGMThn7xmrFDLhk4f9eD75n2zhExZUuMFfbKqE/QS9mvYFQnIiI/Gxb\nycCsxnTgCZbkDIY5jBQOYWaeZq6/JXkek+2bSmLmdDbheOyOKp6sZVGxINrZLDydzmVtqp3O4D5V\n6LzaekYRejDPXLp0mWqvvexDRETU1rRAbYvFQENZbhhT0C/sws3pnhI8oVwGVGVB9j0YgE3UxOH9\ndS0+V32+88678mDw3AsF2E8sCsrII8mMyeSQ2uYQfpNyGZ2bmBC/SS7LnrOzyzEjouO0uffgTKqw\nxftiGmgP9nar9o7NvyYiIisH+/GEYT+55CRF65vFZ60Yqzm1yCnHU3Xl/H3Aceb7wBoaGhoaGrNA\nv8w0NDQ0NKoeJyU0fzb4DVApjRFxymVtcF3rAlis9pRB56XHBf1hl/HOzbGFeBPxHxSVEY9uRvEl\nJlOq7WZXWhsR1E2KLWwW82jn8qBrHOlCh0MIWbWKWIg3WeSaV0Yu2ja+72Y8YqGA7V7J95hlXE8h\njUV5YoEuPvnzlFhk1GQG1I/G9CjJgIlSPkdGSdB9Pi/o5EkWpVbXDJpw4UoRwNHIQuU9HmZsLLzP\nKgl73TOAoJDswRH83wSF9+7b24mI6OIV56ptV11ysWpzGigpaeTDh/rVNi+LfPV6o+qzvkHQpYd7\n9+H/fkZZ52DbyaS4ZrcHdhmNYt8co5cq8Uo8UMrnY7+DxqyYiRI7HXAI99FiNHR/7yHVjgbF8xGM\nIxBveALj6NhAHzUtXi4+22W0L4uC49SiYc7ttWvPTENDQ0Oj6qFfZhoaGhoaVY85oRlrfDhNQ1jQ\ngLEQ6J6GKPKr7DKivyotl5slc7G8jUKZ0XmSR3SziEG7ADrQceF7w8Mib8a2cK5UFm531gY1FA7I\nXKQC9nUxd51HBbl8ggbKZUCVBj3IZXIzGikv8+JyFmjGMnPSE2kcI5EV15nmuU6WnofMhkI2oz7D\nMicmWotIwzXnX6Da7YuXqnZKRhi+e7BXbUsy+0gnkHc1lhD04sAgKOIoi2YkE3TwUxt+RkREns/g\n3l19+RWq7fHAnpubJcXpgApNMLrnN2/uoI997Gr6zZs7yC1z1UIR2FqJ0dTFNPpbeQx4orTN7L2S\nNEtEZJKgH92Mo48z8QGN6kCFvuZj1cg4aPGensOqXZDbI36Ww5tOqvae7W/Sueuuoz3b36Tmzi4i\nIoo3IyqYh7s6MyRpnyroEVFDQ0NDo+qhX2YaGhoaGlWPOaEZG+KIwmppEFEyfj+oQ9MFfzTAkp4t\nGYE2JSnPYVp1LCnaLgqKpuywSERGnzhuuM2poqCfbBt9yDLpqxJrpzLieH3jiAjzMF3JaBp927VP\nUDS5SVBSC+shbdTYiIg5IyKi1QpMGimdxjkmU6AZRycFXdrTi0Rp2zUnt66q4fN51KflEnaXYwKp\n3UnQ0G+9vFW1x8dE8nJfPxKLPSwqld//gpSbqtDGREQtDbg3w4MsUkxGAqYSoG32diNhtaWlHufw\niGO0tDerba2sfXhQUKCdS1ro3bdFu7EF9GbPYdCFZKG/5aJo2yyZmyf5+9yg/HN5sU80yqjys1BG\nrfohxknHwT3vO3JEtbsPo927X8hZ1UfwnCyoRyT3wOFD6vPtN14nIqIPXRNX/w9GGQ09x4Gc2jPT\n0NDQ0Kh6zMn0vrUBb/a2RvHGDwfhKRnMm+KZCoYM5iiw3BeTve7rIpgFhELC+0tOYkYaYzPKFMsd\nO9Qn9kkX4Jl5mbh1WxA/i9sjvaIxLKIXHJYXxxZVx8aF57T23A+pbckBzIacLPaN1YsZcCGLc6XT\nmFv4PJghtzcLr4IL1A4l4blpTI9gsEl9DidEUMf+XgR17N61U7VND+6DLfMBcyl4yi7mjeUK8KwS\nKdFOMSmqniNQ5g8FkK+zvGu5aDDx4FdeekG1OxYtUu1ly4VkVl0dbJwLAseiPvVploTdZQo8HxOB\nJ7kEAkdsW9iNPwD7Sifx/ygLIvFJ9qRYxLOTZYEwGseCit3M5KbM4r44vMn/kMdlgRXGjL6J2KfM\nclotliuZymIsOTIkRKuHhiDubtuQUVvQKM6Ry2Roz+uCzWhsblH/X3bxJey8sFdTVvkweCIa6y4r\nAqLG/RlhTH+d2jPT0NDQ0Kh66JeZhoaGhkbVY05oxtoIgjr8hpQVYrRO0Ac5nUKOSQVJtzgeh/QV\nl/wp2ngXW1LpPMhqDfWPgGo5cAjBEyMpcVyWtkUdTFLr1iuRf7SgRRxv47aDatvm/YOqzeucuU3R\nt1QCckbZNPoQibB6Zbbwq/1+bPOyoJigge0lqSu0kMkrRcZBDWlMj3htvfrc37uXiIgGehBwEfTg\n3kxmkCeWTg4TEZHB5MMSrPhmIgdaxi2DTOqbQMUEGP3d1nm+arfL+9u9fbPa5jJgPxYr1zAyKgKD\nzjsPVa2XLEXNtXYZ7NHe0kDhyy4kIqIde1i+UB5BVwUPCwCRVYN53bLBQSaZxWr5xWoq18Sk3nII\nmtE4FhxdO96ZjmbkX5mSt8XaJO7fFGpxCuXI2+9vLezsVO0go5aTGXl/GZW3s3dYtQMyAGhsPElu\nGfS069UX1f/r2rAUUrMA9mrIYD2D8Yn82ssmU/SfRW5/ppQ17ZlpaGhoaFQ99MtMQ0NDQ6PqMSc0\nY2NtnWoHZGl302BRfFlQi7ki6A+3LP2eZbJT/O2bs0DRxGuEq1xkMj4Hj4A+GU+yqEKZc+ZiEldR\nP/7f6AaF5x8XVNTSKHJ8BmrxvaEEXPCCLPz45t696C9THLdCcOcpJt1xk0WoxUC3RliRxLyMJnOK\niKLrZBGiGtPjwIGtRHQ1HTiwlfYc2E9ERP0DB9T/bRatGInh91y+tJOIiFatWKW2DYyAXjs0gu81\nNIv72NGFSMRIHSjHoQns64wKivPwIdCBIwnkGTIxffrwMkEvZtI4L1N6I6dYVJ+7XhO05dLloMeb\n2pD789rWX6v24JCwIYvJqOVzeI4mmGRWICyOUWbRZZksrkfjWHB0f8GYhlKbUtCTjQNllidmyYhY\nr5dFhU85GKfzKpuwjFFTg5zGK666RrXffmsPERH1dCM/0i7hvPtdYoll/+FB8neKZQ/7XVRrePvF\nV1T70t9G3mMgKJZrbB61yNus56VpqFlOm8700tKemYaGhoZG1UO/zDQ0NDQ0qh7HRDPu3buX7rnn\nHrrzzjvps5/9LA0MDNC9995Ltm1TQ0MDPfzww1Pc3feiph7uZutCIe9kmojWSyQRSWax5FPTrshZ\ngeZwWBRkOIyILYtE+52DoPgyBVAifj+itPxecYxACLRejQu0y7b9kDEqFcW+hRhoxoYanNcgUIcN\nshJAlhXvzLBE6WIJ5zAqFCnzrz2smJ3DCt55pGp5qYDoO8eeJeSnyvFBbY6I6LVfP0t075/Ta79+\nltxNImG5a8V56v+BIuxqxblQzV++TMiO2XlWdNBk95SQmO+WBTNdLtB6Vgm2lkkh+TQmKXSuaH94\nGLbvD/dh36iI4F3c1Yk+sLlnLpFVn3u2vCX+n8P1rProx1T7vNWIKsu9IWjGA/t71LZgEBHAsTiW\nBCp1K5Ls+SwU5nfS9MmwuymoRO/NlBs9JULRed+uJRZ1um8/6LycLLh6zgpEu/p8TCJwmpC/MhN7\nKLOhf+26K1X7cLewwR8++kP0gdHQh0cS6tMXFHa+lC27vPvSG6rdwKIZz1knkqmzhOvxsKLLXtbf\n8ayU+itivONU56ImUPocs3pm2WyWHnzwQbr88svVtu9///u0fv16+slPfkIdHR20cePG2Q6joXHM\n0DancTqg7a66Matn5vV66bHHHqPHHntMbduyZQs98MADRER07bXX0uOPP07r16+f+SDMCzOYTFMF\nPpZrFSQsxLvlu9ZkNcws5qX5AsjnGR0UC9fZUcwiF9eyXBum/uSXHtnyLtThMdkOJRf6U5mVul3I\nU4t40ce6mi7VvuW3biQiou7Dr6tte/Zitu11M8/KER5oqcQkX5gYsseLPpRlvhMXXDZmkHSZDzgp\nNkdEw72j6vPC83+LiIh8PrAEtaxMXksrPOxxKf/Uux9eVbEMb8s0WG07txTudXBvqcSlsVhNPSlg\nHY5h8X2MiUubzK7KasbOgwHQDPuj6rOztZ2IiPxMsNskMBznrcJMNh4XHuTPc8+obYMDeGbaGpHL\naBvimfAwNiSZRBDSfMPJsjuOyn3ksRlT8sVsxtZUHmnmpfT2IVjoF08/pdrJpJTOG0UA2rVXX6fa\nPpYvWOkDF4niYurhCCTXbrrlJiIi2v8uGK7nfvkszisDh5KWRXv6RDBIjcHyiPMYl177D9iYu054\n/2YTGIxMAmOqh0U3DSSF8PFkCv/P5zE+L/r4F2g6zPoyc7vdU4rzEYnEyYqrXVdXRyMjI9N9VUPj\nhKBtTuN0QNtddeMDh+ZPCSOdAZ/80v9DtU1iHeLzD/3sg57yjMWtX//B6e7CWYFjsTkiop/+7KdE\nRPSb7b85ld05AfzJBz5C0xVriYho1RVradUVT82yN/ChS8XnbZ/84w/ch7MNx2p3HKb5/rUrHmZO\nrqOvvy3qRAmpe//bV4/7/ERElS64GRPhC08/9Ld3Ci//u3/zV0c95jsH9pxQX6agYfrNq+miGb/y\nD0///Yz/O6GXWTAYpHw+T36/n4aGhqixsfGo+//jQ/cSEdGXvvsT+usvfVxuhXudYWW58xbcVNMU\nNGGaKXUns8iDaWtHfbDsqHBJc0NQLO9qBc2YzcOA2paJxUivA9d1YhJUSyCOXzkl1fLbmTJ0IgNq\naPE5Ijnoiv/6CO34568REdHoCJTZX3gOdbImJuE2dy4SFKftgA6wmKvtZg+BLYNFHBv5eIYHwSv3\n/xjySPMVx2tzRER3/pc/pJdfeY6uWHcDrbr6t4mIKJnHgNRUC8qjtalWtfPy/hYy+L3LZdyPVB72\n6HjECBGsw7H8rFrD8BFIn0Xywnb37oWN7hsFlbm8o0O1//gLnyYioq4OlqNpgoYcPXiQzr38Etq9\neSv95//3LSIiqmtGH1ouRNJa0UZ/A0Fh2zzv8o3tPaqdZov9+YLwQjIpPHMulhf5ox89SfMdJ2J3\nHBWler4sMD6O3MLJCdx/Q9bMGxwBdbj5DYwfW95AcEVyXIxLq87Fff6jP/4j1W5sAJXtkrUPJ1md\nxUQCVUA6F2AcNSWtvfttVJR48H88gD5s30GHenqoo7OTXDJvbVkIz875zVi6cQJ4ZlovFjmbXevW\nqG0TadhVNot3QMEQfStaoO7L5dknEie08LJ27VratGkTERE988wzdOWVV87yDQ2NDwZtcxqnA9ru\nqgezemY7d+6kb3/729TX10dut5s2bdpE3/nOd+i+++6jDRs2UGtrK916661z0VeNswTa5jROB7Td\nVTdmfZmtWrWKfvzjH79v+xNPPHHMJ7FZ9Jclo3c4/xzwIxomHAF91i8lhLqPYNHV7cH3vEOQq8oP\niX2WNiIK8PprkDt0oA/ufKRNUC31dcgdGx5Bblk8DjrHLIvjeVne1/AIIhTdfrjrIwkhldQ3gEgy\nD6MD41FW4DEnS5m74RwbjFosM8qxkjNisKjO+ZxmdjJsjoioZeEi9Vn57fJ50BlDSZi/Nw5axiqJ\ndQweeZtL455aDu6DW6qIl1ygi4OMZmysg30448Kei0xKymC5NoEAnoOKuXF1e5up6puS3jQ9LnKk\nLFs6A9qGK/77mN0kpZ0HgqCGrrp8tWq/ewAyRjt3C4o0nQSt7vWAup9vOFl2R1QgIh8RFfAcs2Wy\nySTyFF969WXVPtQvovhGk7CZCXZPzRDW1/wFMUYNj/FjvaTanXLtiwiRjX1sHLWKoJNzWZwvnRJt\nFsBKKy5Gvthb+98WxwwZVEyJQehIAs9U0IvnYEEMttL9hli3dvnYMlIrbHCyBApUjbQOrrfAcmxn\nwvyN79bQ0NDQOGugX2YaGhoaGlWPOVHNj8chlxORCXrpNCIJHaaKzxPlDh0ekvuC4gn48f4d6IZ7\n2+QXLmlbGyLC4q1IFvWkWMqgTNJecP4l2DQI6jBQgjtuk+hnJoP+tgQR7VjkyYe1YvuCECuiGQeV\nmRpDZNvwkIhoslgRzjyTb+EV6kI+4a4Xc4y+9L4/+VxjKhwZbeUYLqUSn2WReT5G66WSLEE6L+5D\nNol9PYwmioRApTTUCKokWgtquiGO49puJPbnfKIP4x2wj4I9gANboFpsqYrOoyhtE7ZmSJrR8Lgo\nXiukr8osatFmVGYshv54ZfZuIsXoTwt2dcEK2Gs8Iq7zqaeQ/DoyBFpLY3rsemc7rVxxCe16Zzu5\n3eI55bTeBIskTKQx3h0eEGNQrBERrLXs3tUxWcCRA8Ju3tn5ttr27HNIbo5F8T2XjMkvFJm0HhOJ\n+I9NaHvk8Nq6AFGbwXqMNedfcI76fPPld4mIKMvSsfeOYbkmYOOZqCmJcX//a9vUtkQDaMhxZtue\notheYjacZRHt9F9pWmjPTENDQ0Oj6jEnnlmK1Wyy5GKjh8sxsWQ+twt/ZOWspSaCN3w8hLd5bgKe\nWWOrmM20rb5abdt5BLOhvfvRXtsiZtOJBLY1daG8vUmYBRRlrk2c1XRKDuN6AkXkInWdI+pJJWzM\n3D2ra9DfBGbhrzz9cyIiOtILL9A1xdvCjFzGipDF5h6mZZHGLJDeDZWK5C6LNluTpvYYfuNzFiNH\nKywDklzMRjNsUT6fxWw6EBL3YflSLGa3d7C8HQ+YgrSckbe3IGdxeTdyiqJMfq1W1udzM4kznmpT\n0Yx1XJBnK+Uxk+Wl5z0sACRPwuusqwdbwvM4MwmwB20NwhO49bc/orY9+e/PkcbR8erWV2nlikvo\n1a2vUk4Gz4T8GMNuuukW1S6xPNNtb4tE5FiEjRlleE2tjU2qbQ2JYKLJDO5ddt+7ql3DAi1CslZf\nuAaenT+E8SwWx5gbk8FL0SjsIxBGENs1112qPidlbu/OnQfV/20Lz9ThBPP4ZDCVexA2mppAuxRh\nwU8BEYzV14vxMpmcvY6e9sw0NDQ0NKoe+mWmoaGhoVH1mBOa0cUWz20ZxOAwGs1k0lY2K+09IZm0\nZJKpTBdADbawUvcXX3stEREtWH6Z2vavTzyu2s0huM0uWW+s7+AB/H8xZGH8ddBDCzlSjX8cdFCg\nDBqgmIObP5oSfY83IPCkrrlTtXNp5B+Zsml74YrzPDPLwnUaspaPwcqmc7V9jelx9eUXqc/F5woa\nub8PgT5tLM9l2VJUP2huEIvfLodJWLGAiQIL1Kjcs3AItsjr7Lm8oE88kurMZUAtr1kFGrJzWadq\nW2Vh/LyGWamM58SRD5XjMsglk4IsJtVVZovnJs9l9FeE+rCtwChrN6sYYRfFNTcwSvKKKy8mjaPj\nYM9B9Tkp69UtXYSc10AAttLfj3HlULdQyA+HYDNTbC2JCgy5hLy/bMxY0oV8sK4GBB5FJGU9PAx6\nvIbVIGtpR39SSXE+L4+XK2NMjsrjNjfU04c/JsbccbbcM3QE1zNawEGCk2KfRpaD6WalBNoieBZD\nTSIIqa+nR20rMhnDmaA9Mw0NDQ2Nqod+mWloaGhoVD3mhKvihelsSWlwaSbGeJCTY8rw0kutrUM0\nTXMQ9MmaDy1T7RVrBb04MYycGV8JbvVipgxdlgdubkR0D48Ey7Iox2JJbLdyrOAigXY50CckaFrO\nJ3r+FSFNs/YyfL+uGTkjyRRc8IrKVX0nK8jI5aqKjFKU1OrkCKO6UvhNNKbHRavPUZ8rLxQ0Y24V\n6MRQDJQHL1zoSPkwk1FutSHkXzE1KzUbLDP5KJ4fQ4zCK8hCnV1LFqptAVaQM5eBvToVdXoDduew\nB0kVXHQcsmV/ubJ4MQdKyi4zeTa3vDY2j02Ngco61I2KD+uuuJCIiLIWKJ6gn60ZaEyLjKyOkZmc\npGxe3AdfENTzlFza3h7Vjkt7tFlOq5FH7unA4H60+0W+n2Hi/5/55CdUu5xG3uSvXn5BnGsHKPa6\nGKJkB/exiiKtwjYnLeSLkQfjVm2diKgc6Rmk85YLJfzirbDRx/9fyIHlUriO/oQcl1l0bqGIZyY9\nigjxVvk7eAN4/uobEW08E7RnpqGhoaFR9dAvMw0NDQ2Nqsec0IzlEiiznIxw8bLoworkCxGRywRF\nt6RZRA36A3jndnZADfr8K65V7ZblQvn7rc1QuF7YjqjD5pXnqba3QVBN7iAifrJ50JM5JmM01C9o\nl4mhI2qbzSKMAhHQB/VS9qW3/021rakFxepKWZzDyQl6wMigKKjtgBrilFLAJ5X7m/E7JX2a7pkN\nARlhGAiFKOwXyamhIDN5VnqXJyQbFZrRYFUMWNJ82WJtSfdx2rzESEteaNiRSdjhOCK3SkwOzWZR\nYyRlrBxiSvn8YLahPm35/DjELqLEomFZBQafPIfHZkm1eZzXGYINjhwUVNOC5aDoR03YsMb0KEo6\nuVjIUbYgkn33d4Mi/D9P/ky1X37xRdU2ZPTsUBK/8cgh0L4exoVXCvl6mzGGvfJrqOYXmDL/7n17\niYgoMwT6OzECm4jXYQwbkUnNyUkkKdcwebaivZf+8EtEL256iV54QSjhB6JYSqmphwzWqAXqMFsQ\nx+1j1KPDxrAgO59LFieN1+HaKgVGjwbtmWloaGhoVD3mxDPzsLfqREp4NXYeb+VAkIliMh2eRhn4\n0TuAwIeuNR9T7QXnoU0kvDArhTd8LII3e8OyC1Q74xYz411vvq62FXL4XpJJF432idwPl42Zrt+P\n62lbBM9r9TKRn1ZyYcHd48LCpcfL8nnyYoaSPYRFWe7Bltg0Iy0lvoJ1OG5TK2ZDGtMjEqtVn44M\n5siyPEWH1UgqsO2ZtLCFIsv1KxRw70olTJEtGeDB8wK5KGqW1aMqySCRSC3LAYrBPuIR1FTze8VC\nuV3GcclguWMyN9OkEkUkOzA2jH3zTJS6zPIiDRLHLdu49mgEkkodCyGZlMuK38Fh+W0xJi2nMT1i\n8v7GamNkyec4mUYu1u633lLtoe5u1TblcBxkTJXXRMCEw8SKTZmnu4AxP7VMBmsiCw97cedyIiI6\nZIMFSozDa7J9sMEhGXySzdpsXwSDGHIs6j8ySnlDHC+RRb6uyfIqyy7Wd6/4HhclttlzFGLfC8fE\ndbhcGATLLMd2JmjPTENDQ0Ok51bqAAAgAElEQVSj6qFfZhoaGhoaVY9johkfeugh2rZtG5VKJbr7\n7rvpvPPOo3vvvZds26aGhgZ6+OGHyev1zvj9Qg6LfkGfOKXhx6Kzx2QyPTbagbDY5+bbb1bb1t54\nvWpH60GJDB18h4iIXOxYCZbPMdIDRen+lHBZX3jySbUtzHIa8gVQNM1NgjKIMnql+wgWZYvsfMm0\ncP2XnXeR2kZMQX88gSCSrKRZJ3L4vuHgduRzLAdDBhk4rAbcitnTLqoaH9TmiIie/Pkv6SsrltGT\nP/8l2R6xOD4xAcokPYlFcq4yX6Ech4awr80iRGobsMhdUy/oXh+j0jPjoKn37ntHtZOyLl/7IkhY\nuTywu2gE1PGiRSLfZ0E78tsWLWaUklw8d+w8RWR9vjLLmyNWfcJiz5RLJnW62OJ7UyejN6OwV0tS\nO4wtotpado55iJNhd2FJM4ZrY+SW40ZxDMsYo3sxfrSHQTkbklJMsfEyz8YXI4BADZ+U/RsZQj7Z\nti3bVbtJ1o0kIhqbEPY4yXIP0yyYJDcKCrRSrcPNbnrAA9vPS6ozVzJoRFaBsE3YWtANupAHRZlq\nvGcndkDdZzLoW1LKdtXUsUGuPHvA26wvs9dee4327dtHGzZsoImJCbrtttvo8ssvp/Xr19ONN95I\njzzyCG3cuJHWr18/68k0NI4F2uY0Tge03VU3ZqUZL774Yvre975HRETRaJRyuRxt2bKFrr9eeEjX\nXnstbd68+dT2UuOsgrY5jdMBbXfVjVk9M5fLRcGgiCrcuHEjXXXVVfTyyy8rV7uuro5GRkaOdggq\nOywiS+ZHGCySpcTcTYPlV/l9gtK44CLQdj5Gy+x+C/lcE/0ioqbAyoGnJuCC9+7frdppR7jCHhv7\nhlnOUZQV0muoETTAwBCKFpaYRFE2BUqyV6peE+3CudKIZvO7cW0ln6CqxkqgbQKMRgiyYnUBt6B+\nUlnQAVxBfb7hZNgcEdGzz79KX/nqn9Czz79K8QUiosuxcb/efPV51e5gcmf1dYLu6zvC7jnL1QrW\ngv4oynLvQ4x6vv6Sy1X7gtUrVTsrbdP04LHrPnxItffuQ1TY2zuFbcdjyMf85KduU+11K4WUm2ER\neaW+1oIW5GAWGc3IqzFU8uIsnr/mZnlocdhgQNJEZReeX14+dr7hZNld2WuqT0fmA3pZZJ7Hwu+9\nMMpyDiVdl2J0oIsVyTS9rDDxkFhCKSQQOZsaw1gzWsb5EgWxT+ea1Wrb4AiiGRMTWI4Jh8XYl8+C\nFrU8OG9e5ovlHZNyMt+S5z/6WR8dA+OkLelFlxu2b5aYPBuTgxuWsn0suJvc3mPIq3WOEc8++6zz\nqU99ykkmk85ll12mtvf09Di33377Ub870nfwWE+joaHwQWzOcRynu/vwqeyexjzFB7W7odGBU9m9\nsxofv/3WGf93TAEgL730Ej366KP0wx/+kCKRCAWDQcrn8+T3+2loaIgaGxuP+v1/fPDzRET05b/9\nT/oft4kZsskEVonlEHDPLBgXXsstv/8Fta1lyYWqfbAbM+eKZ9a38xW1LTWwT7WXnbtCtSue2bZX\nXlXb6uLvF2MlImpqEbN07pmNsbpCkTpx7X/2yHP0r3/7p0RE1L4IOW0zeWY7tglR4jfeeFltm+KZ\n+VgZ8Wk8swVL4dF99VvbaL7hg9ocEdHdf/gV2vQfG+ijH7v9hDyzAwfgKXHPbNlq1L6raxFBSBN9\nCBbhnhmPLDkez2x0TASnHM0zW3HxOnrn9VcosU8Et/jKmAlzz8xVw2a18vniQsM+NvO2mbqJOY1n\nViIIXF903R/RfMPJsLu//edH6BtffIge+MG9VJDC6cY4PKiRHQdV22/BFiqe2V421lgsB5d7ZsWK\nZ8ZcZZcNG42csGcm7I17Zn6mBJ9Np2nH7h20+tzVNDwqmC+TBYA0Nreods6azjODLdol5Dp6GOMW\njgvmo5YxIMfimc36MkulUvTQQw/Rj370I4rLk6xdu5Y2bdpEt9xyCz3zzDN05ZVXznIUJv8jZXbc\nHjwUNvMni6xQZ5NMntv086fUttomUHiNnFbJihvi8SAaKxxiheDYDx6SP1xzI6LHcikkFAZcOMbY\niBhULKZiH/EzeZc0Bsd9b75BREQDe/aqbYUSXnzkQR8qEUChBeylHmJJkT5QoH5JKdYQzrtiJQqA\nzjecHJsj+vTv/L769DWK4ojZFAaKfW8j+qulGbZUGcQDfthPsYz7uGwVCi3WtIjBLVuPhNWbbrxB\ntTldnJEvMx6YVWIyWfkS7vnwsBgoDnX341hB9GfwyBituFh89uwSkzYzj+8fHITS+SUf+ZBqd3S2\nEtHUCEfTz6LzPGxiWaGyDWzzGry+wPzCybK7RCKlPgtZ8UyHinj2G5pbVXvsEO7T/h4xsRmxcB9r\na0FDmmzcyZTFeGVbMKZSFi+HfIEJMMgJzMggonczabxcHYs5ED4xLhdZRKXhw3hYkir+JbeHvFIu\nzmGSbHkmPlBmE7miHPd9Htia18/G6iAmbQHZtli/THP2LLJZX2ZPP/00TUxM0Je//GW17Vvf+hZ9\n7Wtfow0bNlBrayvdeuuts55IQ+NYoW1O43RA2111Y9aX2e2330633377+7Y/8cQT0+w9PcpsKuqV\ngRZ+N5vhsQVEh0lBlYvCTR0dxWw6PYJ2wALtViZx3NoaeFvxVlavjMn39PWLY3BhVtPET1GpYUZE\n5DKEFxfyw5NksSvkYn9UaES7CLfdZNeezML7K/rETD/Sin5lAshPSjEZo3xGzErqoiiLXt84f+Ws\nTobNERH55EK8z2vS3j07iYgoOQn7cRzcf4tJBaWlnJXBhIb9PtAgFivhPjkijjF0GAEgv9z0S9We\nSLF908IuIqx0fKyGlYtnOV5HjgiPrLEeuWX+KCiul/79l3TtbTfTS88+T+P7dhARkV0ErbN/ELTn\nESaptXSF8CpjUdhzrAa5TgFWdysWEtfsYTmhwSD6ON9wsuyOch58yse7ZMAjyTA96QEDfwzIsSTN\n6nzRGMYSl4fJpMmACYeNLzk2bjls6cYrvaG+EXhmXODaIBxjZEKOUcz2HUZfegIB9RmtSK4xZo0/\nUy5GTwZk6JDJA2GYl2aw3D1HXpvB9jUNLTSsoaGhoXEWQL/MNDQ0NDSqHnOimm8aoCb8MkrPYYEe\noQAojxBTDs/KhdC6CFxQN/tecRJUSllKwWRZ0Z+mJgRJlBmNtHy1iFx79fn/xLEcuPAe5mLn5EJp\nNAJqyMtyJVxsQbwk3fzuAaZOnUB/CwYihBqWiXlEG68V5OA6J0bRH29eUp1tLGAlO7uK9NmO1Nig\n+vzVv/07ERH1DkJSzLQQ1LFjB5P0kfe/xGgbYvf52ad+pdpeGXB0wYVr1LaiF1JCyQLu48HDYrF/\nbAwSV8U8jts/2KPa3T1inw9diBzLL33xz1R762ub1WdpckyeC5R1jlHoB98ABfrStgEiIgq5QUl6\nvCzykS32RyTNuKCjU2275ZN3qDYTbdNgcMulCbfhIUvSbukc7s14ErY2XsT2koxydUq4H3keiMGC\nKyynkuPFAtuYnJmLR7PK8cphrssUOpDvK9s8d4zHXpTlH26Ph0y5r4stGdks6tfhx1DHxcE4jU8G\nU8iXx+CP35RncQZoz0xDQ0NDo+qhX2YaGhoaGlWPOaEZvSyqpSipEBeTjCqzvK4so35cUq3Zxwq3\neTz4njeIKKxYVGwfHAH1mG1DImxj+xLV7hsWUT0rL16ntqVHkM9zcC9y2TJpEWHodqFfMebOGyyH\nbqBPHOPwIRbN6EN/o02gUxuk+rjBcoOMcexbM8EKgDaKiLcFcVzP/t2IyrsWubQaDC1NLepzaaeg\nnB12v9wm2i6DUyLCXh2mlO9l9kosybi1VUQbXvPRj6ptkSCLFPQj/2z3TpHXtnc/kqOb2zpVO894\nIJek3nfu3YPv70X+YrBzhfrs7xfnqInjXI0sOiwYxvMzPihymcb69qttI6N4ZvI2i/CUkXIDCdji\n2uuPQVboLEdaStylU2lKJsXSQiaN8SOTYc88+zmjUiTCF5g+YpSr0Afc4v56vNiX04UelphfoRlt\nJhnFaUZilHRls4tzi0zIwpaRjYFAQFF/U6KCGR1oE49sFH1zsyUa/j2/n1UEqNCtjLL0+WaPotWe\nmYaGhoZG1UO/zDQ0NDQ0qh5zQjM2NeCdGfKLyKscS9rLIMiPHBOuZcUljUYRxedlGl65DKKCAhW3\nuohLeuNVaC8uXs6SSKUaOo/YCbKkWBejPQMBQS9xmiDHVK1LJUQYhSU9sPbCZWqbn0VBllzMBbdE\nlFuuF5SDmYKr3RhERNyFy4TyemMcxUi3DXSTxtExPjKuPi+7dC0REa29+mr1f58PtIybJ2hWNAmZ\n1JSLWLFLJm2WK4r7OHYE92M8j0jB8VFUbjgo6cX+YVDE4UZIG5EP99/wSlkhpl/37IvQ8ezoOo+I\niDx1TdReK6hOP0v8DzJZt0IeSdMHk4JCDzO7tB3Y5eAE5Nnq6zuJiCjL9Bp/9eJW1f78Xb9PGu/H\n6NiY+qzYSj6PcaLIIqs9fg9rC+qQjy/mFLtk2day7ThMzopLlPGEZZnozmlKYhQfpx8r4JGGPKm6\ngmg0StmssH2bJVW7Gb3Joxkr5+bHnUp1snPIzX4m36VpRg0NDQ2NswJz4pktbMdi9MoVYva5vxf5\nN0MjTJDSZuKTYdG9TBYBFXYZM0cXexePSxXoVBqzk7zFpGActCNhsVA+NIhZ8xG2KFtms52mBuEV\nGkyRfCKBPDJfCP1d0CZmyLx2UaHIi/JgFpYpiH2KaWwLMaXrJe3Nqt3aLPrQewTe5dgIfj+N6RGS\nM9JQ0EdjSXF/39yBCgONjQiYaGpEfqMl1b4nJiAvRixQx81soW2R8Kzaa+BJ9+0dUO1MGp5VY5O4\np0FWDt7FxIyzLKeopWUhEREN9iMvbpRJG7W0CjqjmMuQUcllKqBf5IZdWnwhXTINPjZDLo6xGl0m\n7LFJBqcUWX7TlMm0xrSwrCI+ZVCPmz373MnwBeB9VJwTrtzEgzpYPBLZcoziXpGLeW4uljtoekQf\nvKwP3Cvix3CmucHMfBRr4XK5lBizxdTxC8zrtFngSMUj48fnuWOlErNdu9Kevo8zQXtmGhoaGhpV\nD/0y09DQ0NCoeswJzRitgXvr9Ql6rKaRLWaGkJczOsRq8kiX1e1ldaXgxVKZlR+3pCr+ZA4UYIjl\na+SzoHByeZFnVmTfty3uaqNv6aSUs4qCDohGkd+Wy4HuGx0T566UHieauuhqsDLhXrc4HlvzJy+j\nBjqXdOIcWfG9X/96t9q2Yy/qIGlMD5+UNvN5ylTIC8rw1VchYeawulFRVgTRsmRpeLYQ72bzvo5O\n1D5bdZko1Nm1EIEciV5Qg4MTUCr3SnvsqgOFPDIC2vy85atUe+V5opjoP//jP7A+gK63JC1uZfJU\nLIq2w+vM+1nlB8ZrdS4SlReGe9/FvoyeCjDafMUKEciUz6KP7S2zF6c821Eni7vW1dWRKdXibZ6/\nxyptcCounxf2ZrhY4MQUmSd8rygD6FxlNo4yTKUn7fedd7qgDnG+yrk4HQi7KsvryOfyKneM04U8\nz8wqs4AU2Z+ZAkB4f01JL3JqsTxNkMp7oT0zDQ0NDY2qh36ZaWhoaGhUPeaEZnT7cRp/VFAltWG8\nR91MUdoTgDuZrEg62Sxnwg+aw2YK+XZB0EjeIM7lcYOWcblAZRZk/lDR4lFazLVnAT2OpHBsMFLk\nYVFBxORkMimZO8aKJMbioEjdjHI0Zd+yrArA0CjygSZYVGYqI6LYnnsB0kZDOphxVmQlBZzNZZX0\n90dvvEn9v1xEgqPLwu9dlhSOM0V5HLbkZ7T4YEJQQ6kEpKbGcziWwWR63n3rIBERjW1G9ODiRctV\n++IlS1W7UrY+wOzLYVFjlcjHbC5PpkvYPKvTSDlGy7hZ/lHHAkEz5tNjatu5UdDiW7e9qdr9hwQV\nmWOJoA4rMKsxPaKy+Go0GqWyLW+KwyOccR+TjMJ1e6QKvQd2NyWKjzU90p5L7D6XOS3HinNWFOkN\nNsZNCY0kvtmRx2LSV8znqeRelp0yFXOVqE1cT5lFIE4pulz5P5fUYvsG2XNSKeBsMkqSy2DNBO2Z\naWhoaGhUPfTLTENDQ0Oj6jGr75bL5ei+++6jsbExKhQKdM8999A555xD9957L9m2TQ0NDfTwww+T\nl6l0vxdplhhMrjAREYVD4O08AbibIRbeF4sJlzSdRFRZOonE4TQrUGnlRTvihfSVn0lflVjhQreU\nevGyV7mHSRvxCKKgTNxmSkFTZGO8AfwjGhf00/g46MIUk0SK1qJvWSmDta8HdM+et1FEsakW9GTT\nAklrMZX3+hiSdOcbTobNERGFwl71GZMmFmmA1FiB2YSfzeu8hviewxJafUGcq5wHNZRKCUk1VxD3\nq7ELSdFdQUQz7uuWavkGUzcPgkbsGzis2nX1NVM+iUSCNPo+qT4rKuwFRllZrCio2w9atKm1gYiI\nDg3gORo6DBX/fBqJ2Qd2vSX6UNegtjk1tTRfcbLszpC2ZJBJhlyzKFosSruA8cxiSx2ViD++HOEw\nuq/IIgULMsLQmEYyimgqRafk2Vg09QxCUqqmhMO+P0VtXxapdYwymW6xj8fFxncGzmpWIhd5VOcU\nppONk2Zl/GXbStbsSdOzvsyef/55WrVqFd11113U19dHf/AHf0Br1qyh9evX04033kiPPPIIbdy4\nkdavXz/ryTQ0jgXa5jROB7TdVTdmfZl9/OMfV+2BgQFqamqiLVu20AMPPEBERNdeey09/vjjR73B\nR0QJJTqfiI4cFp5XpAGzDH+ABUyE8b3aWtG9dAazzEQC7YkxL2uLT553UZ5BsqWiz8I5Vj7DcbHF\nxpwMPmFarORhckalLCSx7KzwyGwWIJJIo79c2Wpceps9++GZJcYw8y5msHNzTOQlrehoU9uYszrv\ncDJsjogom9qLTykV5jFgYEND8EL27e5Rbb/MAfTG4GHVM+mr1nrkGVZm0XUxeN1sMk15lvfY2Ci8\nt7ZWeDcDgxAd3rv3HdXuLIr6a9x7TKXQ32xWeFZ9fd2UnBTeIffM7CKrC8hq6u3aKWS7uERVYyME\nrNtWI9etsUFsr29AXpyfHWu+4WTZXSXIoVwuU6HApK0kKnmBRFPvQ1EGIXGBa54PxnOx/DJ30HSz\nYBHmufEcrkp/DJZPyI9rMo/O63p/3lqeSblVcsoMt6lqnvF+8fNy281mZQ4d8/h4DTNeP61UFN8z\nGUPm988uNHzM0Yx33HEHDQ4O0qOPPkqf+9znlKtdV1dHIyMjs3xbQ+P4oW1O43RA212VwjkO7N69\n27npppucSy+9VG3r6elxbr/99qN+LznSezyn0dBQOFGbcxzHmRgbOpVd05jH+CB21z/Qfyq7dlbj\n5k99Ysb/zeqZ7dy5k+rq6qilpYVWrFhBtm1TKBSifD5Pfr+fhoaGqLHx6BI3z//T94iI6OY/eZg2\n/MV/ISKiUAQuaMCHRfJoHdzQeIN0Q024z5ksXPDEOBboE6PC1c1lcEl2iS3UsjyPspR1yTOVcr6o\n62Kueyov9s2lsa/fATUQMcX23/+bZ+h7d15GREQ9FvrgY0lrIVZjKitzNI6w3LJQCBTO9Vedr9qr\nVy0hIqLnf/Wa2nakH5TSX/74lzSfcDJsjojoqf/9V/TZL/5P+scf/F9kSVLZ48dvPD6Ce/r3f/dj\n1R4cEvZosPt1ySUXqfYVl39ItScnBfWXY7aUYbTM3sMI6jnY0yP2zYJ65vmN/igCLdwuYY8pJoeV\nSYKyNIioUEiTzxcmt5Q/ikUQ6NHahCoANXUtqt3Yulj+H+eqZXlmnGZS9BELWOHP0Zfv/xOaTzhZ\ndveXDz1IP3jkb+iLf3aPotq45BOvZ1ZgQRklGRHBAzKmyDzxwBC5LMLloziFN0UJXx7X5cIYx2uj\nTScx5bCgDx4AUiqV6JcbNtCNt98+hX6swMOC7ngfMjJXkW/jY26Q0YhBWRON/w6VPvLf4L2YNTT/\njTfeoMcff5yIiEZHRymbzdLatWtp06ZNRET0zDPP0JVXXjnbYTQ0jhna5jROB7TdVTdm9czuuOMO\n+u///b/T+vXrKZ/P09e//nVatWoVffWrX6UNGzZQa2sr3XrrrXPRV42zBNrmNE4HtN1VNwzH0eX2\nNDQ0NDSqG1oBRENDQ0Oj6qFfZhoaGhoaVQ/9MtPQ0NDQqHrol5mGhoaGRtVDv8w0NDQ0NKoe+mWm\noaGhoVH1mJNK09/85jdp+/btZBgG3X///bR69eq5OO0pxUMPPUTbtm2jUqlEd999N5133nnHXSpC\n49RivtmdtrkzH/PN5oiqyO5OtZbWli1bnC984QuO4zjO/v37nc985jOn+pSnHJs3b3Y+//nPO47j\nOOPj487VV1/t3Hfffc7TTz/tOI7jfPe733X+6Z/+6XR28azHfLM7bXNnPuabzTlOddndKacZN2/e\nTDfccAMREXV1ddHk5CSl0+lZvnVm4+KLL6bvfU/oTUajUcrlcrRlyxa6/vrriUiUiti8efPp7OJZ\nj/lmd9rmznzMN5sjqi67O+Uvs9HRUaqpQS2o2traqi+j4HK5KBgUoq4bN26kq666inK5nC4VcQZh\nvtmdtrkzH/PN5oiqy+7mPADEmUfqWc899xxt3LiRvv71r0/ZPp+ucb5gvtwTbXPVg/l0T6rB7k75\ny6yxsZFGR1HGYnh4mBoaGo7yjerASy+9RI8++ig99thjFIlEKBgMqpIIx1oqQuPUYT7anba5Mxvz\n0eaIqsfuTvnLbN26daqEwq5du6ixsZHC4fAs3zqzkUql6KGHHqK/+7u/o3g8TkSkS0WcYZhvdqdt\n7szHfLM5ouqyu1Memr9mzRpauXIl3XHHHWQYBn3jG9841ac85Xj66adpYmKCvvzlL6tt3/rWt+hr\nX/uaLhVxhmC+2Z22uTMf883miKrL7k64BMx8zKfQOPOh7U5jrqFtrjpwQp7Z1q1b6dChQ7RhwwY6\ncOAA3X///bRhw4aT3TcNjSnQdqcx19A2Vz04oZfZTPkUM/HD7a0hIiJ67lev02//1lVERGQYBjph\nulTbNLGMVyrbJHdW2xKTSdX2m8g6D5niUlKFHI4V9Kl2wMf2DYn+xGJxtW1iYly1i5mCalfcVqto\n4YLQHXK5Rd//7Zcv0Kdvvk4cN+RX/29pQKhu39CQameK4tqiUfy/ZMFJzmQmVXtBW5SIiDwe3C63\nG+2f/uItOhtwvHb3L/++mT5y1fn0zK+3U7lcJiKigA824fXjPpVd2F6S6Zdugl26bBzXU2YnkcSG\n44bdWgbuI6c9TFv+5XhwLnbPbZOdhNkYTuVMaV93+QL61eYjat9ymR2LHYD3oXKMyu9BRGTb7Lx8\n30ofp5wX3/uDm1dO+735hOO1OSKiH939Bbrl69+gf/uLByiXKRIRkYvZh9HeotqJYEC1V8fEGHV4\nx5tq2y8249lOFDAGuVzieHwc9fhgz7UN9aodDYh9ly5EMMo16y5R7ZKF445Oirw4TwTj0jv7D6n2\nf76wmf7XE/9Ef/S53yWS1+Tz4NpiHti21w27KspzlCxm2MyWfOz5yzriN5vIw+5MNvz+4pXXaDqc\nUADIieZTLD/n3BM5XVVg6bJzTncX5j1OxO5ikeCp7tZpQzR8BkgIzXOc6FhX09Z2Krt1WtGxuOt0\nd2FanJQAkNmW3Z771evqRbb3wOhR961mvLWn/3R34azCbHb3kavOJyKiT//W5XPRndOCWz+8+HR3\n4azCsYQY3PJ1Efhx59/9/Qc+310f+AgfHB9l7UoYyC9enN47OtX47XWXzfi/E3qZHW8+xY0fFh04\n2Jukro4YERGVbeZiMpHKUqmk2hUKj9OMpol2VNKFRETFVIaIiEYToOfCNTHVjocjql3JaI+wbT09\nh1XbskEz+v3C/Y1GcayJiQn8PyBc+9fe7KFPfFy47i5G7LQ01uJ7k5C26e7tn9IXIqJgCO18Fvs2\n14tzG4w6ymQzqv3TX+6gswHHa3f/8dJ2uv3jl9OGpzeTW9qSw6g4y4INekJR1XZ5BPXjchj97eC3\nN9iAZucEJZLPg972+kGZ2IRzpHPinpoG/h8Owa4cRjOWJcXOaaT30oWf/shS+pdn9imr4DRjmdOM\n7IsVepEPypxm5OcryzPabF9OT9512/wPhDiR3LEf3X0n/elTz9Jf3fRhcstxLuDGb9hXC5pxXw78\n2e+tEBOTnt69atsbh+AFJtm+FWvg9ytbwLhVYuOkyxD3t64W493v3vFp1bayWLoZHhXna2prVdve\n2bdPtV/btpN++swW+sxHLlU21hjDs7Nq8RLVHhnuU+1cLkVENFXeywQl6WNvokiz6KflRf7a/t09\nREQUDM3MRpwQzTgf8yk0znxou9OYa2ibqx6ckGd2vPkUXrb46XWJ2W5NfZ3alsllVdtjYzZc8dL4\nTLilGW/r5gYco3v/ASIiqndjptvc2qzaZgl9MOVsJhrAgmldDLMWx4VF2VhMHI97TS4T3mNDExZa\nY1FxvFQS3mHJwWwqFkff2krimlzsDrg9uE6+IFquBItEMANyLB6FcHbgeO0umUmrT0suQI+OjKn/\nH+kbVm2XH15+WC5++0zcA+aYUbGEe1q2hC1kU5hxBjz4Hpm4T6mimJ0WizjY4kVLVXtJVweOIYNT\nuCfE2xXHyzGIHPlHmQWecDfuvYEjRwOf6ZuV49LZZ2sVnEjuWHfepz6zOTEWeI08drAxDpgGPI3R\nQyJAbFv/EbVtzzBYIKeAcadyn/wsiMkqsUAeFkjnD4j+JHK4j1vfhrfVUof+FEqV+8/GIjZGeTwG\nPuXhlndhDa1zIWw4ztarBwd6iIiobOF3CNfAQ7U9GHODPvEstdZj0tDrmn3t+4TXzL7yla+c6Fc1\nNE4Y2u405hra5qoDutiurVMAACAASURBVNK0hoaGhkbVY04qTceioPAaG+vkJ+jC4TFQP36WBzQ5\nkSAioqZ6LLj6fKAhAwEsILa1C0oxxIJCrCLcci/Bnfd5xTmyOSzat7eiPw5LJPLK/LRisai21TO3\n3M1opEJBBGVEonCJcyzvLTU5wfaVi7L1+G0CIZZHZoAycBdFH/IZHKtU4IvBGtPh1dc2012f/jC9\n+tpmSkvK0STYTK4AKiVvwwY9XtF2lTHXsxnNmHdKbLs4RsgLuidg4D76mb3aprChTAb37g2WUzQ8\nimjYxYsWERFRfT1o7AALFqoEsjimqQI4yixvx2B9p+MQ+XEYlelIKsuZIQBEY3rkXIb6HJdBPQYL\nKqtjOaJhlmeal7mliRT2TeZhKzxAqHLPXez/bu6b8JzVojhemN3HrdsRNLZsCYI2zulaKI7lha11\ndoJGzJTF83P+BRfQ0IAIFkmmMC4Ro+s/dBUChN56/UUiIsqxAL+UhXOMZVj6Q05QkW2ulNqWT0+T\nePkeaM9MQ0NDQ6PqoV9mGhoaGhpVjzmhGetZ5GIwIKJWinlEtTSxCMWgH1EtPhn52MLyOiwLkY9j\no4hGi0gq082kVcpFlkfk5rlqwt3OsfwKLh9k+kENFYo5+QnX38eo0HQSrrBpiPPxvJ2xcVCLPg9c\n8ErQWJEdN8VyMEzWoWLSlvuCUggzOlVjeiTSOfXpyHBEg0Vpub2gHIOMGnRJaTROTecJ97TE5oAp\nme+XyyDvz2fAfsIObKUSuerxwcbzaTwHB3qRl3NoYJCIiOIsv7F9wQLVbqivI6Iu6hscpLhUqOCy\ncC5GOU4XwWjz3LMpOWnvl64qT6EZz5xijGcqfMa4+mwJClotzujt2hrc/24H40coUJbfw2/M7dIK\nwZYsGVGbZ7llNrNLTkl7feLczUxGq3VBu2qPMhscTIpn5tJLIXc1PjSo2p/45Dr5+Tv09FMiZWHz\nq0igXrhqjWpft/oi1T7Qd1Bc7yuvq22TRSyxpEuw1xUXi2PkLIyd9fWg8WeC9sw0NDQ0NKoe+mWm\noaGhoVH1mBOa0WRJl6WCcGntIpdeYRGBedCIbqkMnUxA0d5gdI/D6Ly+gQEiIopx2So3aKJkAYnM\nFSrF62cuPEuEtVjfDJl8WGYJiWUmoe5jVFWFwcrm8H2vj7n7LJk26BfUjo+p+U8mEqzNZLn8Us7K\nBRopyOgnjemRkzRzrlhmFQcYpWazSDFC25D3l+cgF1myp8Wld4IisTOVhN0mi4juKrDoP6+UbYt4\ncWCXC/c/U4LdVCIpC6Owg0QCNHQoHCD63XW05Y3fUEuLkB7qWgSdxrAXtsbl4irJ4zzn3mHVAcrT\n0JOcpbQ1yzgrvDIq2Rty0+KIWEJZ5MBoYizylSaRIB2Mi3uW8cKWyh6MNR+6ABRek4wGP7h/v9rW\nexg0tenCuOSUhO36WTTk5ZfiWCM4HW198QUiInr33YVqm81ELShUQ0uIqH9onBKyukjagk+0fwBR\nwZky7CojRSuGE7Dxgh9J0Us7YLvxJmHPIyzK/brrZq/QoD0zDQ0NDY2qx5x4ZnzR3esVp+QLzSU2\nQy4wwdaagAhy8DDRTDcTp8wX8eb3ylo+xQLywYpJLMp7w1h0rcyQDQ/LAWKz4gALQqnUMYtEUfuM\nS8gYLB+s4oBaRSbcyrwx/j2SM+RCli3gFpnslxuzlmhtrfwKcjSSGTZb0pgWOckC5Ap5Kljvr//E\n78cUEd9KfTDmmvF2JgMPyR+QHja3JVazKc/yDEsyQMhhx/KyoI2pU0uxT0Ug+b3fS0kh6lQ2TZP7\n3iEiotExCOJG/PDcF7QhcKRSzsTLglC4t1pmeUCVNXke8GI709c+0wDSRY/6jLnEGGaNIpihNwEP\n6orzUToqVxTjVRvzmv1B3PPL4gj6OlfWK8uygJxRFpiWZTmtthwS3UUEm3Qc7lbtQAL3vLZBjHPW\nTuQ/ci9v8+536Kobb6PNP/8Fvdsv8iLzbOzsOwxPc3gMIsmXXCjE5jviCDz5/k+eVO1iDkEm214X\ndjw0dEBtW3P97CW2tGemoaGhoVH10C8zDQ0NDY2qx9wEgJhcsV60AyGWa2OwRXKWP2XLBUZiuRbN\nTU2qXRpj5FBJ+NIhtvBdYErmsWbUFctm30/R1Tchl62QBlXpMoSL7eF0Ic8TyuEcPq/YbnpBEU5m\n4IJbFigaly1c+zyToyG2YBpgFJhb0qJ5C/0aGZ292u3ZjqIMZig6ZTJkXakpKvTmDBI5Pinj5GI5\ni6xSAlMjIksGe3jduF/hAAIuskUEjpRIHIOpaFGhxNTJTZbrJoMyHDbftMqMApSBUHk7r56vwXHk\nXfYXsHi+/xBq9TVIeqq1FXRPmAVN+X2MepUUqOUwmtHWNONsaHD51WebvI9RJun31gSouAkWmNbR\nLPLAPjW8SG3zsKWSun34nu+ACHizyxg/Opk5e5j+milt0zZAFxa2/ka1Y4wmLNeL8ddmeV+UxD2P\nusTYdv7QGBVkbmUtY8qDDmj15OAh1W5bsYyIiCIh2NclXajGPTyJsW0wLcbnbBaBfwdZTbWZoD0z\nDQ0NDY2qh36ZaWhoaGhUPeaEZuwbgSs9MC4iakIFuLHhGKjFPIsEDEt3va0Fisq+ICsHjoAdqgkK\naicehBsbaYbieMEEnbN3UEThxOModlnI4GD5LOgcj+yDlcQ2LiFTZtJFLtlOpxE1VGKC0kWWpNMQ\nF/lntUw1e1/qoGrX1WB75RRRRs2WLdAWGtOjJGnGEsudshlVl2f3yc24wwpD4zZBffDinJUChURE\n7sojNKVwJu5zmOUhVurDckF7i32vZON8FTreYXSPzXIsbZc4R8HOU2UzzwczGKVUYkllyX5h54cG\netQ2H8t7CjIZpEq0J89T83hYXiWtJo334xxZlPKcSJBCMsLUxXJplzFZstQQWy6QRtbG5ay8bLxj\ntJshoxhhMUQFtpxDbLnFIw3DzWzJY7K82giLmJXR1SXGhdss2rVJPhNNZpGuk9HmRVZg1G7FMpC/\np0e1s5VdGN268hyo9bdkcSUtMmp7WVer2rakfvbq3toz09DQ0NCoeuiXmYaGhoZG1eOYaMa9e/fS\nPffcQ3feeSd99rOfpYGBAbr33nvJtm1qaGighx9+WCUiT4cCc28HR0SUVTCLKK9aFqXnYV3yh4Ub\nm2fq9mlGAXKle5dM9iywwnYNEbim7+5DkmDYL2iAcAC0XYElt9a0IPLRsAWtUmLJzUwFi1J5UD9B\nGQk2OIQii1TGOcIxJF7npURMyYK7H2Bq/ZEQfs9xGZWZL+A3i4Rnd7urGR/U5oiICtKuClZRJUtz\n1fcpifvs/ucK4t54GEXoMjDv87mZVJCMxDUcJgnFC1yWmQxaRe7Mhg0XmdSbyRKoi7K/HsZvOoyq\nsqQ0UTafVvSiyeTOyICtcPapcsVlxnUWWURuMsOiFSu0ZwH/50nnRL9H8w0nw+7G+w+qz0KpUqgT\n9y4bw7MbyOL5z78jkoRtJpdXYgV7TRfGSZ8cUw0CRVxitmJzG5TU8BRhANZ2N0JKKpIQdpFn+g7F\nDix51JSELdQs66RQXvSnxJKu08NYUsr2v6LaA29sJyKi6MplatvYICjWYhBjbmVpJjuGpZ+khxOq\n02NWzyybzdKDDz5Il19+udr2/e9/n9avX08/+clPqKOjgzZu3DjriTQ0jhXa5jROB7TdVTdm9cy8\nXi899thj9Nhjj6ltW7ZsoQceeICIiK699lp6/PHHaf369TMeo7EWi35RGagRCWOB0imxvC43q8kj\n83X4wnY2x+SqSmy2LN2lFcuxqDg4OKTaBbagWS/ro3EZrTKrNxQMIyClmBUzHFeALcQywc7MOGYi\nw7LuTyyKwJJ0li2kspwQn5wtWUw+qG0hcn94jamJpJgN8Rl/vBZ5cfMNJ8PmiIiysmZeNp8nd8U9\nKTOTZ79nLgNb8Uoh4NomLNQHmMNiMs/KVbFRtqA+OYEcr1warELHouVERJSyYF8TE7AfHxOltqRX\nyYW1eV0xKuGzsp2LAHuZcLLpYvlpUmrL5lEozOt0CshrKid6iYhorA+BSeTM35WJk2V3Y+mE+uzN\nCBssscAjr9Gs2sEaBKmN5URAUrMLY2Mgz3L8kkz2r1LbsB7fDy3D2JcvwZtOjwob9JXZGMaC2Aoj\nCIQin/DCjDi8RzeXdUuK6zGTeQqslB4dy6sNDoPhyPRBtiuxRwgilw/jOYuw98J4HM/i2KDo+8Aw\n8uoWeVGLbSbM+jJzu91TIr2IiHK5nHK16+rqaGREJ/BqnDxom9M4HdB2V934wKH501WxfS8e+9GT\ntGjxUiIi2r1/fJa9qxc/eebt092FswLHYnNERE//4H8SEdH+f//xqezOacWb//KD092FswbHandf\n/MnTRET0F1v2z7Jn9WLh/3ritJz3H+69Z8b/ndDLLBgMUj6fJ7/fT0NDQ9Qoa+vMhD+9+5NERPTk\nszvokx+9gIimlvU2XejGYO+AatfFhBvatRDH72OLhmVW1j4kJYg6WG5ZD5Px6UuAnuxc1EFERMkU\nFhjLDlz4unpGE0q3OhrkeV1w0YcHBaX040076fduFHk30RoEelg2ro0roEciYoXVZU4fONB9uFe1\nkzlBNbnYAj9XfN/41Gaa7zhemyMi+ujdf07dm/43Lfro75BMy6IaH9TkowHYYM5mVIshbMXP5MfC\njNLm584HZLUGVg/PToM6dLH8xmCLqBFlhECZWCywiN//vBw4s2UePAWKxsokaNf/+QGtvO2L5HFE\nf92MknSVYe+WxfLpXOKayyxwoMxktByWy5TofouIiNLjeCbTafQ3Z81/aasTsbu//MR19Nfbj9Af\nn7+AOmXwlsVkqerPgwJ81wXnqbY7LSi6ZpbTGuzHeJcdBW2XkoFD9rJVOC5b6+PVPMqjIqDJ2ovx\n0GFjX74NNGHT9deIcyVQgYHe3YN2yaSux35KB+76DFGLsOMCo+49zcgNq23CbzWx9XUiIhrfCyX8\nchB0u7PiXNV+8V3Rz4Ex2OJlF3WK43tmDr45IQJ87dq1tGnTJiIieuaZZ+jKK688kcNoaBwztM1p\nnA5ou6sezOqZ7dy5k7797W9TX18fud1u2rRpE33nO9+h++67jzZs2ECtra106623zkVfNc4SaJvT\nOB3QdlfdmPVltmrVKvrxj9+/5vDEE8fOmYa9oMfqo8Kl5Tk8sThyDFjQIE3Istm73tmrtpVYFJaP\nRdHUhkQUTj+LoBkbhaucL4FWSU5K2oVHcTE1okQCLnglBY4X/QwGcT21daCt6mpFu1CCi++wvKYc\nKzzqSKqyxKIZCyzCyGb5SQHmjlfgPoq7Xe04GTZHRKqSApWKFJM0cTwIk+8bAO2S49UWZLSiwVS/\nF9WBMmlsh9r3Hlmg0GGRYsEM7nOMqYS/3StybcLNoJzCPjwH3Xt3q7Yt7Tm+FJJR4VZEq2UOiYKc\nRW+IXDJiMuoggi0rI+qIiLIpqOl7PeKZSeZZhYY4ImPr2AOYrkREsmfSMOdvNOPJsrv29gXq0+wW\n41GAydrZRVYpgcmOTWTEfXy1F1F8rXlQxOcQDlKJZsyx8a74G9hPjmWSGW3CXvPLEEWZLYFiX90F\nii9jCvvI9feobd5JVvkhKit4uEpUPCyeH2sI9uxphK1lGc3okWNjzfVr1LYEW1KK18Me14TFMtCz\nL2Mc9sVnj96ev5apoaGhoXHWQL/MNDQ0NDSqHnOimr+gGe5mNCxol5o4JFJcTHneU4/tzQ11RET0\nn8+/qLaVWQHLeAT8x+CAcIWbakDrxJlsTIIl840Oi+TmeA2iFkNMPirGtkdCggKNxEAnhsJMkTyH\n43pkMUeXmxVnZPRkscjahUqEIuYTBpM2CvhBe1WK6llM+spi0lYa08OUSfGmbVGzlP8amgANYjH7\ncUcQrWpKeyxZoDk61qxU7Ql2n4o1gq5xsQKyZhQ2mEiCJkpJmrmcBQVYyINmjrHv9aYFZZgZQQJ2\nRxxRsq3LV6vPxG5hC5k+0KITQ2gnMziGLaMyJ3O49kANKJxIO9olKSOXz4H+5pJbGtOjWSrHN7c2\nUapPLHUEazhXyxTtWYHYgVFxn364fZfatrwOY9iX/FhuCMphw8mAWh5/GzTjeAPGq4MyEb7IqMfW\nZYg6XFiDfYsDImI2zChAg0XGUkoWrk1Z5DOFVF8yh2LH9kEk2Dv9g6o9ERHXHFoOIYLWRV2qnWcC\nFw1yWeXCVaDV2xfhezNBe2YaGhoaGlWPOfHMHBZd4ZOBH9wjsTJYQPS5mLCq5/3SO6YJr2jKm1hK\nRXV0oOR4RbaKiGjBAGYwPrnoHmV11FzsvMPDWFRde+klRETU3IqZTMmBV5QcQx5IjayPNpbA9bhd\nmA011GMGVBG8LbMy9DEmHjwxiRm9I2dvxRzOa1tMcFljWtTK2km10QjVh0U7MY4ZYK0ftuRjNcpK\n8rdt7Fquti1ugdTYrsOYfcZ9XvkdzF4bm+FBmawOU0ZKtZkReO4TI5i9djRi9pn1iuNN2LCl8QnY\nmilz1tJmgBacexkREfUdQT5Qns2WPfyZkppXLiatVkjAWx0hVosvK45hsmfVnv+pZR8Yk/aE+nQ7\nIufQw5RFimxMSLCCh+M5sb3kYN+kB0LlfR4EbcQdYaNFE+OA48CDnizj/h8ZFjYUNeH5T+Cw9PO+\nn6v2chks0lWLfet8CBzJ9IixsZDLkp0Tx3WYvNsEs1GH6asVJdNkTSIor7hjn2oHmddYkM9lx7lg\nQ6x+MA0zQXtmGhoaGhpVD/0y09DQ0NCoeswJzXiY5U0MyTLhqRTokwpVQ0RUZGrftqwbFWSL88Uc\nXNrGBgSL+EzhrnctRg6Qjx3XZO66V9KMgQCjLNlCrJMD1VKQivVWDHRAXQvoQpPRBB0yv8Tnh1J6\nMoPFfq+XSVvJgAFez8zF5K5sFjjikgu/Tgk0QjiE3DyN6dHRXKs+P3HjdUREdOhgp/p/Kg/quZDH\n710qCBvrbF2otvF8QacetMukpBczWRxrQT0CnkqMYk9LBXWHBfeEHRYIxXILm2LCXjPDoG3SfaCO\nrII47kBPD4Wkun/rSqhTlC1WzaEfEkLZtLRtdq5oCHbnZrlMFbbLyrK8SZ50pjEtvPKee50yuSWd\nW8+WR4qsioGb0dPZvPheG18eWQR6uy/NktWk3JmXUeVGiVGZZYwVLXVC4s/NViaSjN52xmFX/WNi\nXJ4MYuxcWGAVGEb76CIi6j9ymEiOxSaTesuVMK5nbVybIynOIAs8GujDeyHI6uRlZO5tvIAO169G\nHbSZoD0zDQ0NDY2qh36ZaWhoaGhUPeaEZsyyPJVkRri0RSb5VNsAyqzMitjl88K9bW+Hq71757uq\n7XHDNW1pFq55QwPPXwPF44E3Tl6fuOxgEBE7PJqRcqCRcklBGY6PIOLLMRFVGPAzGSN5vGgElFSS\nqZA7rBhowC9oJIPlpFmMcuCK7ra8zihz/T063WdWRF159Xn5GkEZXrISNHSKKdZbrOikVZJRZVnQ\nOrk89l1UxDGyMl8wzSSsPB48VhNJUM7+ReL+5ZhsmRNHlYe+QeT27OsWUkHn1oCyPDzCyifJfMvw\n8GGy/YKGD3dAKujKrk7VHu8Fzfjub7YREdHwIJ6jkIF8OmLFOfO2OIfBlPvd2vBmRSAXVJ/9JbEk\n0cjGjJoclh7cw7jnJalkv+JcRGQvXL5Utce34561VHJzPRhrPMyGA2kWUS0jBYNBLLXsPdCj2vUZ\nfG9xpxiLj3gxVg3tRx8DKWGDieFxMuRzYtiwiTyjUItM+qyYEdvHWXWKYBD5vKkinomMLKQ83ofI\nY/dCjMkzQXtmGhoaGhpVD/0y09DQ0NCoeswJzWi6wPFVosZ8jF4rMBfT52cJ0pYsQFcEhZOaYGrg\naVA4ixYKaZSAD7RfmBXUjNXAxbZkIUWbRdvwJO76enxvWMpgDTCKZ9vOHaq9ZImgr5ZfQzQ8IvrT\nP4AItBIr5BmP4rgeKYnk84HqLLFoxkIetERFkD1Yi2TcZBrRcxrTIz0+oT6PdO8kIqIFbaBw2lqa\nVNvNbKUsI02TrOoCr6RQV1un2pmcsKVsDraUYRRPKo3I1+Vdi8X/mUhAnsmhNQSYzJGMILvo0rVq\n23gW1E/PoIhWvHLFEirKSDGbJdUTk6hqXY1rblj9YSIiKk2Awhl/Z4tqd+98XbVHD4hqFaYX/TXd\nrLyExrSYzFjq84VJQa+VYDK0jslDBYYRVei3xBLMhRddp7a1tkPS6RdbUcl+UsrZ2W4mccdkAQMO\nxsH8EXEOVy2WcxbXgN7O24h8dUtZv9VXXKK2jWMIo/FtYrklY7uoUBF+cMNuc+y8oRC76ICIyM6x\nCirlOiwJ5QnbB+VYO8kKhE7sQYL1TTQ9tGemoaGhoVH1mBPPrJnl5XS2iWCOIMsBCwSZlBDzljzy\nzR/1Y1Gxqw2z6Thb0GxtFF5L2Ic3fJTVksqbLM+sLM6dnMRx/SH83xOEJzk4IjygXpaL8e5+zGoH\nh8UM6da7iba91U1ERJYFr+ncFS2qHWY5IXYl+IAJJzsOFnP9rN6bLYNlDBduV8nWclazIS5ng/FA\niFJjYnY6wIIZ6pthdzH224Yi0gOOwVtzGZgBR5gUUEzKZDkm7JlLW72zGxJTDTJ/KBhE/lqWeXHn\ndyKw5OoPiWCOXAk2kWW3fGm7sIkPX3weDY0J765/EOzBYHevah9mskL5/7+9N4+To7rSRE9EZEbu\nWfuqHW1oAyyQQcIIy2Da4E20baSR3TO2f2DmR7/pZp7bmMY0HsbdNi0aumm7e6DhJ/qNjcd6I4/H\nC/KToI0xi5CNwDICCa2lpfYlq3JfYnl/3JvxnRpVqUqgKilT9/unbkVGRtzIOHHjnu+e8x3pgYZq\nIZ1Vu/xjXvuKxau99oxjgoH4w6vbvW39PcdI4cwoJru8v4cHxViRK8E+amfCK7rcz+xKJoLNYwFv\n8Si8qQIbGwtZ0Tb9CKTLu/jcZPZoFsVxc0OwD53JazlMXqtXPieJ/RAtDgcxRqWCUe9vSgapFaJ4\nTjjrEG5E34eKYpxMscA/vQRWorsHY6Yu82qT7DmKJOE9jgflmSkoKCgoVDzUy0xBQUFBoeIxPar5\nLN8gKF1Tvw/b/AG08ymW+1MSLmlNDPkIV1wBFz3Ecyz8wq32scASm1FKxPI8AlJWKsrqkpkscMR1\n8LP4Zd/fOYAcjwxbiCemal4OBjANLpOFxVGXSbY4uri2JAsASGXRR5/BJL4kTWCxGmZFlqukMDba\nZKn2tvoa0mSZ+aFe5Avu/cNhr/0my19skVT4ddev9bbNYPWh8glQzoZPco6M1vExCmd2Oxa5Q5Jm\nDpiw97iJfEJiavolW3wvlYOt5WzYz/5DHeL7/hIlCiLgaOUlCPpIN6MPx7oRZLD/uKA99x7FtacC\nCCxqjKM/S1sE7XnV2o96297c9RwpnBk3zYl4f/uHBC33u2Owmec6QJmFLmE1yqJirIgZuAelFKuU\noYGiy8ixIMjocZsFsZGGtiPHsCFW+8xldfTMDM5RGhbUnnvkBPrFfJ6izA0rhuP0lpTX6xjAMxVk\nQ67psNzLoOinVmKBKcOgPTMuqEqfHJdtVsliTh1sdDxM6mW2efNm2rNnD1mWRXfeeSetWLGC7rnn\nHrJtm5qamujhhx8m0zQnPpCCwiShbE7hfEDZXeViwpfZa6+9RocOHaKtW7dSIpGgW2+9lVavXk2b\nNm2im2++mR599FHatm0bbdq0aTr6q3ARQNmcwvmAsrvKxoQvs1WrVtFll4kS7fF4nHK5HO3evZse\nfPBBIiJat24dbdmy5Yw3uMgKSaaknJUegyudG4bESTkHjIgoHBKup8EonOFBuOgFRjOOSEXpMj1D\nROQy1WUufeWXpd+zNqPqWNHBIpPfCkvpqx4mNVRwESVZMNDfvJQ2Mlj0T5YpjltFROcE5OxuJA9X\nvGcQuUwuy7sgmbuhMZohFJgWhvi84FzYHBHRH978nffXHRTF/WoaQMXteRuRhgckbUdEdO26G4iI\n6AfPfN/b9skbPuS164Is6lTaqI8VTszlQSk1NUCOygkISikxDkWs8YK1ktrR/LC1w8ehMv73j/49\n3fKZj9LfP/rXNNAn6Jqrr0EfP/G5P/Haza245ois8tBu4Xl4exjckMOKPfadEL/ZwtmIIL5k8dIx\n+14NOFd2t6jd5/39soxcnRVAwd9fvQu67986MH5cMUcUAE4fQcToMKP4DLZsMlwUNtbE8iNtF2NG\niRVf7Zcq/gNhFIrNMwn9mMYieWUEr1NkobODyOcNSBtOOw6dkmPXIIuWbWW6geEIzheLyMofLBdy\ngOUP+wxG3cvI8eUuxv1oii3tjIMJA0AMw6BwWDyo27Zto7Vr11Iul/Nc7YaGBurv7z/TIRQUzgrK\n5hTOB5TdVTjcSeK5555zP/vZz7rJZNK95pprvO0dHR3uhg0bzvjdkx1HJnsaBQUP78fmXNd1jx89\nPJXdU6hSvF+7s7uV3U0Vvnv1B8b9bFJc1UsvvUSPP/44PfXUUxSLxSgcDlM+n6dgMEi9vb3U3Nx8\nxu/f9e8/TUREP3vxLfqPn/swERH5mUq9xaJe6huQaJdKCnfTsuBeF5hsEKutSXNmiCibpQvneNtM\nRtvMntvutWMyyW9oaNDbxpXOrSJc3tSgOMafP/K0t+1gJyS15jWJpOg3jhynG68QFEVDPaIv583B\n9RQtppYtIypTzO1OJBEZ6TAFbE2ywX4Nrr/ugqr6788iwbFa8H5tjoho0x//Mb385l760Acup/Ym\nQXNw2bITnaCOV12NZOH77n+AiIi++71/9rYNnUL036UzINPjl/I8ERZxa9usIkQN7n9TvaDreLQj\nDybQGd2TlknxRRb1+9h/gw3u/t1e6hwcohkN9RSQkbwZVrj2P33tv3rtFQtR2PDYPiHr1c+i2Y4X\nGBXOpIkyUsKr9Y5fJAAAIABJREFUmdmz38KSwOP/8F+o2nAu7C798Kcp/sg+Sn51ORWlFt2JPoxb\nP38T0X97OrFs0hATSySf+tyt3rYUK+i7499e9dq+jg4iIvpAHWhGLYjISJdRkj3SljJMTq+PyeX5\n4vjevGWXEhGRwaIS39rxoteeFYjTY4fepT9fuJhO1Ai7OMUKCYeY3Jmlgxr0SSqylVGdfOnH1FkE\n+ZD4TeakQMcuXyzG2Vm147+yJqQZU6kUbd68mZ544gmqrRXhkWvWrKEdO3YQEdHOnTvpuuuuO9Mh\nFBTOCsrmFM4HlN1VNib0zLZv306JRILuvvtub9tDDz1E999/P23dupXa29tp/fr1ZzzGya4ur90v\nhYIttvg3axbkrjKsxlQyXfbMsMBo6Cy4wmKyQYePigtin3edxMy7sR6BITU1wlAPHcJs2yWc41Mf\nxyw94IrZR10tZjWhJGYcg8PwtgZkAIef1XxKphEYkGG1orLy+nUTM+F8CbMaLl3lyFlWIo1ZXCPX\nVKoynAubIyKaMXeB99cm4VGUSpiRmmyBum0WpKRcTdjCrHZIPj3/0x977VQPbCksxYEDIX4/MOMM\n+EAfROWsNMxq1Zl+eGZBE8dwg+K4/Tl4Qm8ziaEbb7zB+3v5FZcTEdGTT8Fz2/WbX3rtS1qRo2OG\nhW0O9CD3bO+hg17bz2TdWuLie3aOBR6Z1auzcK7srvzsaoaPNJmL1VaLQJ4185CzmCzCHjuGxXiX\nZaxVM5O2MlhOYl6OifkU7MNXwn0y/biP5bNZvVjvizM5vEISTNSQHINq62DjtSxnzS+Dm/z5LM2Q\nQR0m84m0CMYzzQ+PT0+LsbrFh2tgpSBJZ+xAVl5TDQsKmT8bv994mPBltmHDBtqwYcNp259++ukx\n9lZQeP9QNqdwPqDsrrJRvdMsBQUFBYWLBtOSrGQxNfjBEUGVxcNwG8t0IhGRwdWcZa5VJofPmTIW\nuSxwJBYS+/Yxdfvfv3Xca0dCcLEL+TJNCFrPZLlh+w/hey1hIZ8Vi4Auam2FpNbgcdA1mlzQ7GPh\nuzNnIljAduBXFyRNkM2AJrDY5za/trigp4oOfsdMUdWVmgiWTB60yCZb/nZmADRHBHENo2ywt0/c\nv4Eh5P2d6kGwkMtyIYMBQeeUGMWDu0QUYIFFkYCwIYPVrQsF8RwEg+ibI6mmE/2o0ECsVtT6W2/1\n/q5ZI2qenTyJPLSf/OznXvvNvQiKsmU9wUQvKOviIHKgfDbo9KwlFuCPJqDAz6tdKIwNV94n19XI\ndYRdmA6WT5bWwyb620B1Z2T+ocWCwhpZXmQwCnpyWNpzqQhbtFi7YOAYuqxzFmdjJyftilyRXgaG\nuD0IUpnJaHO/IejJGqNIsZz4XrMBSjMxjOcoEANV6ZTEya0slmWSBezLWEZy5HJM21IE28ybjd9h\nPCjPTEFBQUGh4qFeZgoKCgoKFY9poRnrGkDLxWXIa5DRL0NJUG0hFulVKgrfs8gKuvn8eP+ajPIo\n2sLF7hvCsfIW9q2PIaJr5iWiPyUms5VMwf3tOAWa0GwS1JDuYt9oGOfVmlnUj8zHSQ9D/qXjeIfX\nnr8IRRmLkooo2qzUPWMOOf04Wx43FMR5eb6dwtgYGB70/pYs8Tv7GE/tMrt68w/7vPaKy6+U21Cm\nvsSVw32gVYolQeF0d6PEe55VNzAZbV4OcmVBXORnRVh5rqMtJYjSTO6svhGyUo0NDd7fVFLYW2sb\nooKHErDhnTtRXDMvi4EODiKHJ8Oi1XwhRKMZ0kbrWkDxNLfgHApjw5G/p6PpZJdl6Rg1XcPyqz4w\niy1ZpIQsWbEXUdglVuzSZJGmeXmOEstH1ZmElc1ob01WW7DYfS76uRVibNPkM2Gzqh2ks+UPS+yb\nL2TIlZRk0IYNu6ygZk8QY2pJjtUOzIv8bOkmm2WFRaXtN82GrQV9E9PbyjNTUFBQUKh4qJeZgoKC\ngkLFY1poxlSWJeXJJOP2FkSqmIxazDJplIgsBKf5mMtssIKcJksylpRiliV4miHE7EQbEDVUksrg\nFlOODtaySDKW6JqSUW4LL0FEmNUDisbKgAYaSQuaYOGChd62UycP4byM1ipLVKVZwqLD5hbRcJi1\npVxRhkV9MrVshbFha473V5O0SZrZYi6N+9jTj2jFf/ju94iI6PhhRLWmi7h3hztB4bkyqoxLWJVs\nZpesMoNRVsJnRKPG7NVlcmXeHiwSOBTBsQYHB72/5QoMyRHQ2wVWMaKjA1GOZRqJ5eeTy6IoeSRm\nOaE7EsCzk82wsDOFMWGGIt5fQ/62xWHYGqcA29m4s2JE0Hb7hxHB2tOFIpnJHO5vWgop5Blt7mfR\nzpaLc+hStTDDigNnWWSsj407TsGRf0GVa4xmJHmOvnSJ8nJcdizYWob1IR9g1SFkMeKgHzyjY7Ox\nnkV7LmgRY1udiWNlB0FZjjfyKc9MQUFBQaHiMS2eWTiC2YctZ5qFEhOhZPJPfibvYxjl7WyRk4kL\n+/yn51oV2CKoxvJ5wjU4bkrKpYSYBFF/P0p4+3x499eFxLnDtUhKigbhjbU0IfcjEhadC4fRyeZm\n5JmVF+qJiMoTfT7pidcgSCUWR9+SI2JWMjCAIANXx2xZYWyURavFX2ELuTQW1AtMzkpni+PDUnKt\noQnsQU09giAsNvt0XDG7tEqYWdpspsrzz5zS6V5cgTERDvPCSC6C68z2h5n9vPLqK7R+4yZ65dVX\naN26dURE9PY7+9EHXp+P9deQv4PDrpd7knaB1Y0qiu+dPI48MyOgGIEJUZbU0w3SNDEWsJghyjMB\nXj/zPma3iXHy2CnYRJFJ4NkOtg9LKb8BJk4dMzDeacyWNOmRjbDhsocxDdz2DZcHhsjPWdsv7WfI\nClCvHGtHWDHINDvHDDa41crnwGABei0+MGdXMknD+bPEjxXOwZstMC9OeWYKCgoKClUL9TJTUFBQ\nUKh4TAvNGAyx8tdSmilXBC0TcJi8D8sd02T+g8loSGKK0nFWKyovJVmKPrijvgB83hxTpzZkMABj\nhqiYg1venQedVz9DqKmXuiHvEtKwbzCGvi1ZKGipgUEs2tbXMM0kxpGmpZr24jbUWXNY2fNsFlRE\nNlOSxwINyVLkFMaBLRP3bHK8ygM+Zl8BJm3Fa4zV1cncHxaw4zCqTmd0Trn2HV/Mthlt57C6UmXm\nx2I3L51hVEoBBlmS+Tq2ZY/5+S+efZYe/sd/ol88+yzte0eo6b++5w3vc43Zms0CTsrScuU8NiIi\nl9ULdJiaermls0oUQXfi8vUXPRzd+1uQUnycvuMBFS6ToIpKFfrGOGxpqB/jTopJTI3IunyvsmWV\nOsZSxzXc/4ikGUs6dkiySiR54oFpAgYLLDGZvYflHklDozJ179NgP2F2DofZeVHmuoXYuWqibBAr\nseCWhDheMo5r0FieHjLzRkN5ZgoKCgoKFQ/1MlNQUFBQqHhMC81oslL18aigGXlEl8FcT4PRiLaU\nqLJYEU6XHSuVgvubk5Fe/FjBIC6vyFzekiwvnx0BbWOycKNYPeg8ksUzS1lEMBosAolLaoVD4nw8\nEjHAIiprWUScmxTRk5qO/uZTiFzKZdl1yJwzjeWJ8PwjhbGhSbVwTTPIL2XQNGZfZDM1cD8Lk5U/\nrct+7wCjWohtL1e115gOOacRbYcndIkDc5qyoRFUOZdXcyUNOJqyhE2Ucw6bWlqop1fkJc2dO8/7\nPJVhNHUOtlu+OF7JYhTlyPpb7qfOKCddPz3aTWE0yhUabMf18hA1Nm6ZTJrJzTHaVt6S5gg+f+Mt\nyKwNdiG/0ZJRjP2MQk6ycTLM7CYsdwmwPrgmzsHvb3mM8bFcW24fSTkmJ302WZICd9nno2q3Mnt2\n5Ll1H6MhCdc+nEYemSGlAwM64hY1Z+JXlfLMFBQUFBQqHuplpqCgoKBQ8ZjQd8vlcnTvvffS4OAg\nFQoFuuuuu+jSSy+le+65h2zbpqamJnr44YfJNMdXNY6YkDDxS7eYv0WDrEBhmkkMlZOmzQC+H2IJ\n2KO2ywPmRuCutjRDpZ5H7NRGxPn8TczdZ2xQiUA/WjK6KxSN4BqYaj6XQDdNQS82NiEZ12TuscFc\n90BA9MF1ca5wGN8L8XPI3yHH6KLcKOqounAubI6IyJXRoa5rkCsLn46SkuIqPYxe8yhHRhFzilfn\nX5T7GOPICpWYOIBHrXO2mCc0swi0st1xdtPPzhGSVSAaW9tpxmypSM6OlWNJsZy+LF8np71cRjny\n36H8/I1O8mYhwFWGc2V3urQf3e8nv/xpWQA0aQYbdtlva8vI1rYYxrgGPz73swoKcWnPeZbwzJOf\nLR/uY0be0xxfmbBhlwaLbCw/Hzpf2mH24crIxaRV8MzYrzHRC3ZtIdafqGxGNHY9o5TRmI3lxHIL\nC/SlsB6miTDhy+yFF16g5cuX0x133EGdnZ305S9/mVauXEmbNm2im2++mR599FHatm0bbdq0acKT\nKShMBsrmFM4HlN1VNiZ8md1yyy1eu7u7m1paWmj37t304IMPEhHRunXraMuWLWe8wX72ZjfkjNNk\nb3A+W+aLkeVZoskW5y2LzzKZULD8Xk2MSxShD0GTCQnLWWs4ymqnMVmhfI6V85aLnGGT1aVinmaG\nCdeW85ZyRfQrx47rd3Edhszd0Q14pTZzV7M5zKyGhxNENPraJ5odVjLOhc0RERXztve37Fkxh2SU\npzPKI5E5Z1wOzSW+cM2lguTCNvOq/CFW38nADJgvwAMs54g9J+V7XSpyuSvntM9tMihbLAeLsGAi\nlpczKnBIBsC4bF8e9MHtiufelREOTzxDrlScK7vT5e+m+3xklOuNcepnlGcGW/HJASuq4Z6vXYY8\n1BFW8+vNEyIXdoAJSueZZ15gduXI83Ehc5vnTWrcnuU2fewAM0Pae8jQqRzLEdJxPWGW3xhjddti\nurj+BnbpYWaXflZTzZTndlnOYz4/MRM16WjGjRs3Uk9PDz3++OP0pS99yTP6hoYG6u/vn+DbCgpn\nD2VzCucDyu4qE5N+mf3oRz+i/fv309e+9rXRHOokQsQ3/7etNGvufCIieubZ372HblYGvvW9X5zv\nLlQV3o/NERH9y/e+S0REz/38Z1PSvwsBP/1f2853F6oO79fuAnf8ExERhe579n33pZ61L3vfRzt3\neGPk/KydvviRmeN+NuHLbN++fdTQ0EBtbW20ZMkSsm2bIpEI5fN5CgaD1NvbS83NzWc8xjf//AtE\nRLTlp7voP3zig0Q0um6SLwyqzR1jUZ5Tj4mRBPZlNGONVMCPMjqQy/DkWHlxTbrYARaQEWMK6iNZ\ntvIoXV2dB5DE2b5JoQL9vf93N/3nL91AREQDI1CGTg9CpqW2lpVIz4jrCIb4Qjz6nhgCfVmuB8dV\n/nn7l7/aS9WEc2FzRER3/Omf0fO/+Cnd+IlPe1Sbxuo8GYxeCbBgIk3aG6d9/SZshVOSPpLbHVYf\nitFzfAAs2zHP1dK003N8RLt8rtPVz4kEpfjMvz5Fn//i7V5/SoxaLLJnw2GUoiXbXF6J16Pi/S1f\nB6e3+bO4/Wc/oWrCubK7wv/zVQr9xY8p93efIVsuOXAVe2L2YbGxxh4SY4LrsnvHJNe6hnF/f/eO\nqGRwuBfjSy/LLUxYuL95KRdYYF2wmH3wMbecW2hwBX52bX7HpR2DGfqjhgiVY0wiBgtsY8EgAQPP\nSVzWjmzyMzlCJlMYYhVQyiw9z9ENmRMH3k+4x+uvv05btmwhIlGCJJvN0po1a2jHjh1ERLRz5066\n7rrrJjyRgsJkoWxO4XxA2V1lY0LPbOPGjfSNb3yDNm3aRPl8nh544AFavnw5ff3rX6etW7dSe3s7\nrV+/fjr6qnCRQNmcwvmAsrvKhuZOlghWUFBQUFC4QKEUQBQUFBQUKh7qZaagoKCgUPFQLzMFBQUF\nhYqHepkpKCgoKFQ81MtMQUFBQaHioV5mCgoKCgoVj2mpNP3tb3+b9u7dS5qm0X333UeXXXYhCbO8\nN2zevJn27NlDlmXRnXfeSStWrDjrUhEKU4tqsztlcxc+qs3miCrI7twpxu7du92vfOUrruu67uHD\nh93bbrttqk855di1a5d7++23u67rukNDQ+7111/v3nvvve727dtd13XdRx55xH3mmWfOZxcvelSb\n3Smbu/BRbTbnupVld1NOM+7atYtuvPFGIiKaP38+jYyMjCrAWYlYtWoVPfbYY0REFI/HKZfL0e7d\nu+mGG4Q247p162jXrl3ns4sXParN7pTNXfioNpsjqiy7m/KX2cDAANXV1Xn/19fXV3wZBcMwvLpO\n27Zto7Vr11Iul1OlIi4gVJvdKZu78FFtNkdUWXY37QEgbhWpZz3//PO0bds2euCBB0Ztr6ZrrBZU\nyz1RNlc5qKZ7Ugl2N+Uvs+bmZhoYGPD+7+vro6ampqk+7ZTjpZdeoscff5yefPJJisViFA6HKZ/P\nExFNulSEwtShGu1O2dyFjWq0OaLKsbspf5lde+21XgmFt99+m5qbmykajU7wrQsbqVSKNm/eTE88\n8QTV1tYSEalSERcYqs3ulM1d+Kg2myOqLLub8tD8lStX0rJly2jjxo2kaRp985vfnOpTTjm2b99O\niUSC7r77bm/bQw89RPfff78qFXGBoNrsTtnchY9qszmiyrK791wCphrzKRQufCi7U5huKJurDLwn\nz+y3v/0tHT9+nLZu3UpHjhyh++67j7Zu3Xqu+6agMArK7hSmG8rmKgfvac2sGvMpFC58KLtTmG4o\nm6scvCfPbGBggJYtW+b9X86nGG+x85k3P0dERB+/9BG675+/QEREseCl3ueRcNxr+zV0KRrxExFR\nY027t60uPNNr19bUeO3ugRNERHS0f6+3LT4DRtcwI4NzBLJERJTLDHvbgkHIsRhardd2bIuIiGw7\nhT7E0YdAQORgXBa6l97JfYOIiEaSBe/zwV5cTz6N/mYL4rdyCSxvYqgbn2dxjGR6RO5rsX1xbT94\n4FW6GHC2djdrwaX0/Paf0Y23fIp0V9iSETbw+eI2r61p+F7HkS4iInIc3LtYTYy1g147aorjtbW1\netuG07CVweEE+tvQSERExUTO25buHfTadTGco3XODPG5lfe2jQxi33QqQ//9yafp39/xJTLkY1wq\n2Ng3OeK1Q3Uhr12yS+JvqeRtsx18z2Vt0y+OGwrieovFotfe+8rvqdpxtjZHRPSd5zro9tXt9NSu\nLu+3tR3H+9zP9jV1+BOaIcagogNjTBVhKwZ3PfJiDIuHA96meBT3ycJQQamSsFGdGXmJcJ8dF9s1\n1h4PX1nTSv/yas+Yn/FVK5cc/oE8F1/VGudcYyx8aazv37x57phfOycBIBMtu3380keoNjSbiIj+\n+au/ORenPAO+MMXHHx9XRbeIBrfz9jF3VTgHmMjunt/+M1q8aBGdPHxgmno0/Xj5+RfPdxcuKkwm\nxOD21e3UFDXpLz86d+o7dJ5w/02zzst5H/xlx7ifvaeX2dnmUzx74KtERPT5D/xPuuuRtURUfZ7Z\nNdHH6PX0l4lIeWZThbO1uxtv+RSdPHyAZi24tCo9s5eff5E+dOP1yjObQryX3LGndnXRX350Ln3n\nuY6q9Mzuv2kW/fXOk2N+Nh2e2Xh4Ty+za6+9lr773e/Sxo0bJ5VPYeD3pkijGIT/sAcD8KzWlV47\nFsGDly+Km5BL4epytbgoS8t67bp2cSkLZ+GScsFer51y8OJyksJoAnbE2+YGcI6SjeP6DHFd9fFG\nb1vYZPtm5AAUJUoOLBLnGkx6n584eNxrGwF2c/1iMDnVCXc9FsULNZ2CsVlWeTvOy56NiwZna3du\nyfX+lgeVnI1BvKcbL5rmRthC0CdGDV2DLfodvAQLCWZ3TWIyM7OlwdsWCcEGs8khdKggbH/Jkhne\nptY1mNRFQ3hQAlHRLjh4eRQKmEQlh8UL8yMfXeFNAPu7ICt07DgMxKzHZNEIiuuwNRw3FMcgGAzA\nBmNB8Zv4fbgex7lwFB+mA2drc0REruH3/jrlAZu9iHIFvGnyNsYzU/62mo5tPh2/veawN5Q8IH8R\nZfKY+Bga7qOmi/7o7MWp8xcJG0u08V4w/wd0A88Dtwj+vjXYdejy5VkqYVwrjTOGjfk+naqXWTXm\nUyhc+FB2pzDdUDZXOXjPa2Z/8Rd/Mel9O/skPbKUqH2eEOI0DFAq9dFL2N6YOXceO0pERMc6Qb/N\naMesOOPiGHU+Mcu24lgf0aOgZQolOPepYTHDqfeFvW0m87biNZh5xUIz5ffRr6IFz4ssTC9GegX9\nkDiKn/Xg66BiIrMws5qxQEjABCPoVzKF4xbybBamiX0GBjHzLpYwC7uYcDZ2FzB93l9XzoBtm80j\nLcwum+vgeeeHJA2dxj0IGvDSysKrRERLFi8gIqKFi+Z620YYzegPsrmqLs69dAX2nTcXFHqxACrc\n1cW5dXSRfH7YilMUM9zV65ZQKSO8rGIGVOc1+SVeW/PD89IlzWqbJbaNddHPPAVpd5yeupC0+KYL\nZ2NzREQlOSaULIdcaW/cr9DZTS2x8cNxxD3Rua/DuUWbedum8NwtRntlS7DXkJ95YT7xPXeUN4Zj\nuWNSf2zfMW656ziet+SwY3E6UNfQh/I5+LLKeKY0lo1Nxu5UpWkFBQUFhYqHepkpKCgoKFQ8plyb\nkYjo4EFJu3yYKF8UbvG8xbO9z48eOuy1M1lE6UVigv9I5RCZte/dt7x2tH2h126ICarF0uHynjoK\nmpFccCl1pqB2eHRg0ESEUn1Ni9dOj4iF1AP7sW9dBHROLI75QKlB0AeZTnze04vIyHkzQS+Eo+J7\nloN+FfO4dp+J4yaGxO+XzYBa1Bj9pDA2IrU+76/PEb9nzAblFgqgzeIhKOwT2/N50L7ZNCLa3DDu\nTV+X2PdNFjSULyIStYEpirfNFHbR1g5KM1SLPvDC8+U4jKDJFtptHngkzlFfFyQKiZ0LzGbcAp4D\n3WaPeUDQQKFmRNZaIRy3wH4IV5MRaIxGctyLMPLoLOFRaq47IT2maWPQfTy4gn2fU3ilgohyNAn3\ny/TBlnjEpPcdFukxilgcK7Zioh3GCcjgtlLifS9/Pqoe9Ni2NFbk4mTIbeWZKSgoKChUPNTLTEFB\nQUGh4jEtNOPJE8gtOHZUuMfJBiTdFXXQiLYPUVa1dfVERLRw8TxvW28f9s2wiL4/vC0oRUvHuWob\nQUOSyyLMAuJ7dfX13rZoGNRPKgk3d6BX0DlOET9VMI4oymQRZdL3joiozEI9co70ZuSZhYOgPRPD\nIv+ouwv9slj+SamAa0tnBN1lWZwWZcl7CmNi7rIW728gLygNi+UsdnYi9/DdP+De6K6414UkqEPN\nQvKqzii8Y68Lezxhwj4sRsU1toBmTEiaMeJAdb05jqjDVpZ4HZZ5jwEN/S2mWLJ1UdhCXdGgYlJQ\nTekORLsm+xLse7ClnIwWblwEBQedJVUHmxHJq9UKuovnPfl1xW9PhJIkxUrkkuaeHs04KrKRU4cy\nGtFgNKPGcsNsluhcDnIMs+hTlqJLVha2W5DhqgUa+97x/rie7b7/+zw6gfr0bWeHifPMlGemoKCg\noFDxUC8zBQUFBYWKx7TQjFYBsTXDfYISKWVBgwQicD3rWkH9uQHhdjcvAPWRdBDxl84xSR4S3xsc\nBKUSMxGx1T4TUYUl6iMiohEH+2aGEK0WNPC9tGR2YnHQepaJvvdlQCNt/7Hoj+N2edvmm/jccOG6\nD3QJ6rCYx7UbPrjSeZak7UoqIhpDvyajoXax42Prr/P+ZjrEPd/1y9e8zw2WpJxNgsKxbTHHC7EY\nqpowbDjix74NhqBwasO4N+RjFE0Jbb1T3PPf/+IVb9vx37/jtT980xqvvfzSufJc+L45AppZGxB9\niPXbNHhCUNb5AxAXyPSAcswXQE92JQW1evwQaH5fA/oeng3afOlHVxARkZ/p/5VsFc04EcqPpqsR\nlRlag9Fk/NEdlVhc/h6zO58fQzSXoDIMSWXaTBqLJeunu2ALjYuWi32Z78JytUdJlJX7oDF9SM4M\nelvdsYk/TiKOohnHohcnZBzZDippWkFBQUHhYsC0eGYBDbPaUk4EVNS1YrG7sxeCwMl8p9d29YNE\nRHT58kXettV/hO9FTARilLKiffAgZqHJBGanISbiaptiVnsqecLb1hCDJ9Rex8RW68Wqqsne+xkL\ns4Qjp2SAxyqioy+LWXYxdcT7XJuFgJVsH/KW2uaIGX2olmUX6fAUdQPbw9IrKDJP1M81iBTGxPIr\nZnh/D0u7G2EiwQ1h2I/FPOGBlPB02ti9WVCLfX1sIb4s8lvHxHrNEESLbWY3waCwpUgEc9qRPggR\nv/uLF7x2bY8IEmmug0iwlcf9d4riGNnj/eTPyWARNsPODoNp4Ok89oi4/uEBzOLD/fBQS8PYXviA\nCGgy5mKYYDrNCuOg89gJIppDncdOkCHzyPzMW9dMjIcak6sK+IW96axygb+Azx0m+Bw0pA1ZTJDc\nhb0GWud67YSswJFhXqCPjS8uCzIq5xFqzG65QDE5ZVkqh3lLzIsb1aYx22XwwKLR8lm6/A7rlzax\n4SnPTEFBQUGh4qFeZgoKCgoKFY9poRlTCQRt2FIZejCJBcpgFC5mOsNyraQLfeCdY9627k5Qg7EY\nqJ2WFpE30zwX7nP2OOiTk/2g/kIx4Uo3NIHCqYszik8/5bV9pjiHqTP5nyJy0pwSV6IWgSFLVoBa\nvHQe2rEwZI7qmkQfsllQUsUi+p4aBPVqF8W+IZNRi/Z7zde4eFBT4/f+DgyIPDK/jt87asB+Eg7o\naXKFLZhspX52DN8LBUAZFeV0sMCKKKZGQGWaIdCTrswJCms4b3MjbMn0MZrwpKhz190HqtyyQTPq\neojmE1F/VyeRDCzyBdDfMj1ORFRgxWLDUsJrKM3o715QnTUxfC+qCWre1vFMFpXZTYg3TnTjryvG\nME7V+TnFjlUPAAAgAElEQVTdx+g1n88vP8ePzOJ/KM+GmuYaMXbNZbXqWoOssHEY9pqTdc40VpMv\nwYq35ooY+2yZy2r4MRaZLKdVUH+zKDnST4akPQt52Bevh8Zz6AqyqKvNcmV5FYhQMMS+55PnAqxJ\nuF3KM1NQUFBQqHiol5mCgoKCQsVjWmhGnrOgy5L06RykhFqY5I9BoPO6ugQlmXRByyQToFp8QVAw\ngxnRrokhTyYYhesab0DJ+VBAXHZLXRvbxuVbEDlTLvNdKkHuyGWF75IJqO3H4yIf7sMfhZxVQOa0\nERG1tSJfzpTnO/gWQs2GWKRdPgnaypV0a00jvm+zKCaFsRGS9EjIDJAmf69UAnanM5rRx6KlXMlp\nWBZ+7xIr7hoJ4575ZTRaKgVK22SUSSzKlMylAn4mA9qdmKJ9fS2ooXxBUDc2u82lArOPjKAG+zs7\nKZUS28MRUEN1UfS9L4lnJhgUVLXrIGoxX8S1nzzR47XnnRTPVPNcPDu2A0pJYWxokVr89YpSAgX2\nDyvWQHZ5LxdUXJhFqJZYKGkkK6hBNwoKsLYettQWYzlptcIWBkZgo0f6YEuHB7Fd86S0mJQboz0D\nhngO9ncNedJmRSa9xwXveaximWYssahhTr0GR9GM4rguk4UzR6lrLaOxoDwzBQUFBYWKh3qZKSgo\nKChUPCZFMx48eJDuuusu+uIXv0hf+MIXqLu7m+655x6ybZuampro4YcfJtM0x/1+OsVkVjLCpY0x\nmZYSU3jWmXsbCghKQ2fRX7E6yFLZBtzxXFFQItleOO7zZsAdrQmBDqSSlIIZARVTF2GRgn4cI5uX\nLrgP53IM9P3oYdBPdS3ieCuvBM0YIij3l2zQS/mMcMKtEqIWizn8TgED9EEoItpMTJs0vbplhd6v\nzRERkVQhp5JFZQUqP5u/1dYg0jDswMZOJsU9LzAKMJVniax+2KgvIO6NVYLNzJwFWq6mAfJsA4OC\nqi6xfS32BJaK2F5OoM3nQOvZOZw3KyMUs8kUJYdEMr5rsUjEJtDtZTV2IqJ0RtA82QKup8REAPIs\nmfrYQSF51bi6Hdfrr27V/HNhd66kiN1CgVxJE/KCk87oypes6ZWw9DZZrHhn0MU90x1xT3tGsBzh\nOLjPHcNMNV9GMQ5nQPGNZLFvlkVGJ6Wt6Ow5cRnV6dNF+1TapfJyDN9Xc3mxUVwayYRux4HB82Kz\nPPnbLV/nqMKkNCEm9Myy2Sx961vfotWrV3vb/vEf/5E2bdpEP/zhD2nOnDm0bdu2ic+koDBJKJtT\nOB9QdlfZmNAzM02TnnzySXryySe9bbt376YHH3yQiIjWrVtHW7ZsoU2bNo17DCOAd2YuL97m6eNM\nNmcAs4vmdryNI1KCaoQFi8R8mKnWt2CW2N8vvRebBUkU8Hk+jZlKQBML7boBL29ogM22I5glDKbE\n+XJptmjvw/dOduInjDSJ3I1gFLJVPiZBlMvB+3ML4hgzZ+DzGuYd9rAcuUhULtrr2Fcbqy56leBc\n2BwRUXIwQVQr/mYGRQ5gHZOw4jXhigXMWh2fuP9ZDXaZYLJCsTh+fL+cMsZZManaGtzHWBSz+JFh\ncdxBluNjEOy1qR59KyPPcnh4kldR5h4Wiw6l02IBPs0CSwIBnNdmskEDkiVJsOPmSw5rY3tXp5DE\nGv3bVG+i2bmyu3IulfgrPTN2DxyHsSrc+5ABETxXy2LBFzFWqzEozXGAjWt5FqSkD8Nes9JuPAks\nInKYqxNhxy3KgDfbxrPB2QxXSrn5ijY58ngO98aYJ+ly8kjmbHJvzRlPPNi7ZiZUPAkiasKXmc/n\nI59v9G65XM5ztRsaGqi/v3+sryoovCcom1M4H1B2V9l436H5k6kc+sx3fkHzZwqx4Ff++dgEe1cW\n/gKMBO3afM/568hFhMlWq102ayMREV01/0/pqu/8qdj4nanq1fnBn/zo3853Fy4aTNbufvpnNxAR\n0Tt/c+tUdue84nf/5WPn5bzLHvjpuJ+9p5dZOBymfD5PwWCQent7qbm5+Yz7f+FeceG7fnCU1nx5\nNhERNcSRT2bksBgZbYSr7Mh8sEFWp8esw74NbXCVTV24xU11M7xtrSy3rKmW9VHWmPIz5eiSAUmf\nZBG5Nse6xcu35xQCNYZ6V3rtPW98koiITv34drrp218jIqJPfQw1qmo0UE5h31KvHQuKl7uroUqA\nZiEIITUA2qpoCfrI1lgAAKtR9ZWb/pqqHWdrc0REbx35Pn1wyd302/3/QNv/6TkiIjq5F7/bSBJ0\ncH8O1F+mXthgXQS26DuFfMFFzchPbJG5jHU1oAtnzIYN+kN4xAYTwsYO7DvqbYuGcJ8XL5jjtR1b\nUIf5LKhDhyUlJYYy9GfbX6F/vOVa6pNyVI6LHZpb8XyZTIrrzcOi1t6RPtDYBYfLDoFSXPnxK4mI\n6JNf/Yi3zdJwjluX/xVVO96L3d36vV/Rvm+tp+V/9b8RuMBovfHqfHlBIrzeGfsn5oICnlMjxq5B\nG/bD6ctICEssrtweYfJRIzncZxaL5/Whg+Xz8jpofkOjNx+4hT7wX7eTVpbl0vj18EAOHJekjY2u\nd8aDRcY+hrfttC2n4z2F5q9Zs4Z27NhBREQ7d+6k66677r0cRkFh0lA2p3A+oOyucjChZ7Zv3z76\n27/9W+rs7CSfz0c7duygv/u7v6N7772Xtm7dSu3t7bR+/frp6KvCRQJlcwrnA8ruKhsTvsyWL19O\n3//+90/b/vTTT0/+LCXInZjS3YyySDI/y+exinAxtYD4XjiIfQf74B7bOCwtuUSo5s9omOdt8/lA\nI+YzLAKNhGuuseieNIsUe/cYlPm7h0VbL6FfzjCOVe+CtlpUJ2WQsuhY0Qfq0CihYGI5cskMYd+W\nRuSkNcZne+1kRkTiFVikWcSHXLZqwzmxOSLySeLBR7pXRLPI8raSKVB4ORd29aGPriEiomVLQSe+\n/Mx2rz3QiXveJtXLa2KgGYtMhbzAVMIdKUdUKDC+0AbVMjgEqpukbJTLCjVm0th3eEScY2gkT7ZU\nt9d9sMueQVYIthbK6hQWtp9iclYFBwSNpYGeMsLimuxRqVDVG814ruwOPKE2ikorY7y1N287y+uy\nGT2ZZ7ZipcVY4mqgk/0B2GBLHGNfSEquzWEVGuY1Y/kjEsT9N+QpXjqMpZZfH8K4NSSLwjqaRkY5\nUpP10bI4XYhr8/YZRSeOnTzmjPHznJM8MwUFBQUFhQsd6mWmoKCgoFDxmBbV/DhLIq2pE5SH62NJ\ne7Vwjy0bNJBliYirNCt2aKSZgrMPkTyUkxRLDq605oOElc0U0AN+0eYq1CMJHMpNLvHaoZKQIwq5\noHACBqLVeoZf99pzfcJHnxlc7m0r6ThHjkWmjRRFAT9nCFF0mgNqqDaCtiMjNVNJUE5mBHJFCmMj\n4Ia9v61N84mIaI+NqNQEk05rX4YotTUfFlGnly6BjFNDGI/K//c/EA6fHBb3NJtBxODQAO5dkVHD\nrqwYkSpwehv2Ucco0IBMTuXFDIdT6G9R0jlFyyK/LCCbZ4rkiTxT9mcUes4Qtp8jRDMWmXxS1oKN\nGjFhd+EIqHJ7kuHpFzPK40rJRhygzgpyjhviPxYVx9wNthpDfhL36apaLMFcfuVVXrs5jp0deRBT\nB4U8q4klWDMq27JkodfFLd62ZA6f7zgiBCxsF1GHGqM/fYymdpkqvutdG6Ndbdi2zfpQ/pbLYxjH\noSQ5lGemoKCgoFDxmBbPzGAFfGwpcFliC+5Z9gLOpjFj9JvigzjL1Qqw2YVpYWE7YsyR55rvbXNy\nmF2E/JCgIlvKxrBiUW0x5Pi01l7jtXO2WCjPDGHR/1jfca9d53vba9e4YrY0uxl92N9zxGvrGrwp\nv6yfVWSCr3k2A8pFd6O7pvBAk3mWhzbcjetZ8XFSOB3ZZMn7qweErRSYM98+Z5bX/tgG3PMFi4V3\nb4ZgmMs+hBxBLg788pM/JyKi3x9B7phWwA62xWaisijTEPPA6utYTbUQFu1zSWF3qRF4ShkWN2JI\nsWvLMKhgiQ9G8gg8ybLnZH8nVCtODIh9U2w2zWWFCiyvKd4ogguiTGZtiD2fCmPDlb+tazueR+Lq\nEwR9EPLBuJyVy7xmgweTxeaKfcPwRwoZsDxDPjAFsbD43qF+MAa/OwCJwMxgl9cOt4oAOp1F/ZSy\nGKujUuA8qjuUL+eOaczeR10cC9Yr58Cx63UsJpPGcuR8MjCP+2KuO/GrSnlmCgoKCgoVD/UyU1BQ\nUFCoeEwLzej0wbW0+qXat85yzxi9YvqRP6VLZXDXAr/iMI6nuf0Kr+23FxMRUX8XeCQ/Ew21Qmyx\nsSiV8HOsnHwILrzOfpWaWpFrZMZB2ww1sXLejIJJ5kUUSW9un7ct2spKg9ugGQt5sRBv2AgycJlj\n3TP0ptcO+IWaen39ZehjCQEtCmPj1GAPrZJ/X33rVSIiapqPvJzbvvLHXvuSpTxwSFDKhQILkmD5\nj8uvRIDQ8TcEjfz81l9528wiKJ4So5EdV9hgTRD3eVYbgom4LFBa2igP5BguYLG/bFW9OY38fvG9\nlB/0pb8Wdnny1KDX7pFVIBpnI+Cl6xRoSKsEO9c18VwmE8hJy1tMxV9hTBjyOTZI82hETqONJ2dV\nbvO8rVGq+g6CNk5mRfvACKi6dwZPeu0aVoHBkXXDhlnts9IpSO75Eh1ee/3nBc3Y3wnqcX4N7FkP\niuNeMydGrx4X453BGNQaE4NnLABbCkixZo0VZeTSabksk5mTtQP7C2f3elKemYKCgoJCxUO9zBQU\nFBQUKh7TQjMunXml1148axUREdlMwbmtFhRPsAYRipqMlunvh7zUUAZ0oRFc4LXzeRGtmGPSWcEQ\nonu4xFAuI/J1MhnQSDaLbLRZ/kM8JtzqUBT0ZWc/ZIfyBuic7oyga6KD8LuNOnyvlOzw2mFduN11\nobneNp/JZGEKoF4jAUG9zmyF3JWfGD2lMCZa58/0/lpRQSlfcdXl3ucLLm/12raLqMGS1EkrsjxE\nYtJnZhSPzewV4p6kf/KCt81XYmXoM6DlTJlndsWll3jb5s5De4QV18z0Cdqlh0WS9WZZZJsh7PVE\ntkSGT9CA0VZQONfesgbf+/lvvXZXSdBHn/78jd623/xql9d+7UVE6nZK+rFUgLSaxvKIFMaGIalB\nw3XJkblSpsGWPFiuFZc7A+XI86vgb2gsVrBc6WCQ0dAms9FYno1t8hTRPGSp8i4iG0usP1ZCREn3\nnHyX9Rd9XL1OVEBZNbOWGuXSTHMUY/msBtCbIT+uIyiLxfJ6cTajXq0CnpNjPSLS8qmXO7xt3fnT\nlfT/TyjPTEFBQUGh4qFeZgoKCgoKFY9poRkvu/zDXvvKq24iIiI9igiZ2iCoOiOAiC2DhPv69ruQ\njBo8ATmiYz2Q9/H7BDUUirKk6hKisNwSaLuMjOqxWLE704SrnGXFQI92iGi1aBDftx38bOkSIiL7\nUyJqbH5prrdtqBM00YmO/ehvUfSzNorraZ+LSLsRC1SmIyPT6v2IOosG4M4rjI3atnrv7+3/+YtE\nRGSGMH8r6bjPOnE5HXF/QyH8xrxgoOXAbtrnCKpy0RJQwKfewn1ymTyb4ReUM6+k8PsjoPX6hkGL\n9/QLyrF/BPaVZBSfbggb7iwmKBoUNnb1OtTa+uDNV3vtXXtR3T17WES8RWphz5/847Ve++DbP0Hf\nXhdRuR/+JK6tda6SUZsIpqx2afp9pOnCbmpCGNeyTFm+nBxPxGScxlG7Mg0mDyUjJn2MIpwdxzmW\ntkAkYighaLsRJodWYvJRfUnQ279+8UUiIlp+1WpvWyCA8a4uGvb+zmoRcoFNjGasDbOIWw19C8vx\nU2fXUGTRjMNp9O3dk4IKt9mSkeZMTG8rz0xBQUFBoeIxLZ7ZgstWee15C0RumO3DW9lnYLHSsDFr\n1WTp7+w+zCI6TyJnZiiPdiwq8q6sHhw3zOr7NNcjr6YhLjygdJbnEWEWUMpjNpweFguleQeLoDqr\nX5/OI7cjLfdJslpRGpOx8WuQ13rnsPD4ahqxb8IHT8AfwXWkpYc5mMAMal4LREWvbPkCKZyOTCHl\n/Y3UC7tyCL8r97Y0NmO0CjK/kau8skX5Ipsx1raIe/bJz9zsbftRz8+8dnaY17MS9jyow34am+GN\npy14ZgWZ7+VjeYwhAzbY3CRsadZlLXT1aiG1dc2NCLTSatH39nn1XtuRuUqHD8Nb++THP+i1Fy9G\nDbc9b4gggFMdkE6bswB5kQpjIyLvWSQSJkMmYQ0xJfMsE362beaGSWHe0XlmsB8uCGzLsWblTHhg\naxey+1yArYzIUd5m+brZFGwtGocNlsWKr7rmQ/iceVtFWYsvGqklvdxNLgLMmiZj2UpSBPtUxylv\n229e3+u1X+/GOLh/WFznCMvX1H1KaFhBQUFB4SKAepkpKCgoKFQ8JkUzbt68mfbs2UOWZdGdd95J\nK1asoHvuuYds26ampiZ6+OGHyTTNcb8froEbGwiKRXBeip38rLS8i4XAoAzmKGWwoN57CDIsLgsi\naWpdRkREh9+FDEtOQ46XxvJ9fDOkbAyjjrpPdHjtTBY5GFlZg8xgeWiay5TDg1Cfdv2CEjjZA+qx\njknBzJo902sXpHx7rgjqsFhAO1aPRdW8pL2KSVADAYIaP6F8WtXg/docEZElaRXLKpLjra7jPvqY\ndJPFZYXkY8GVuksWqEVXZ/kxUkJq1mVzvW2hVuRKjuzv9NqaT9zTWVfP87Z96rabvHZ3L+i8vj5h\nV6kMaFFLw3Myo03kZn7x//oszZbSVEVG3SdyoOBnzgH95NOFPR49iH5FPofruWolcjfffOMQERHl\nmFy/XeK0afXhXNhdMpn0/pZ/ryJXwmd1vswxRmBex4t7GwaTO1vQIu7j569f5m0bycBGEyMYl+pk\nAEdnGuPHZctRBeLqD30E+9aLAJ+Qj9VvZOr3dXFB17fEgxSUnTd12OXgAMbqtw8gV+2lXa8REdEr\nL72CPvpAkdav+YTXzlri3I7GcsvYMs94mPBl9tprr9GhQ4do69atlEgk6NZbb6XVq1fTpk2b6Oab\nb6ZHH32Utm3bRps2bZrwZAoKk4GyOYXzAWV3lY0JacZVq1bRY489RkRE8Xiccrkc7d69m2644QYi\nIlq3bh3t2rXrTIdQUDgrKJtTOB9QdlfZmNAzMwyDwmERnbNt2zZau3Ytvfzyy56r3dDQQP39/Wc6\nBLE6geRKaZQSy8+ybLjHjgk60EkJ91ZLgzKx0sjLqmsCXVPoF9szfaD4LIcVmEuDOhyU+xpM1TmX\nS7E29k1lxbkNLqVvoL8z5zEV/zZBL7Hgn1Gq2JlSj9eeN1dIBPlsyFJliyj0qfsQ9VO0BSUZiYKm\ndJjSUrXhXNgcEYocaqSRJaOpfD7cc6amQ9ks7A70InawWSFBfxAUTFFOB0O1OG60HfRJTwZ2VSOl\n2prnI1erZi4iboPtKBC7QBPtUo5HzrJnQ2oUzVrYQLrMZdJY5FvAgBE2NqESRUzSRKYf9Hc4xqLZ\nPoicsrqfiJwjbmuhwLQEQJ8XnCu7K8oliaJtkyvviY9F42lMdoqx3mRJ38Jk0YyuhR1aoqA3b/2g\nkEGbyfIFsyxfrKUWkdF1cpxrjCB3bMliVH6I14CGLspqDQGD5V0ymnGor5to5nIa6uug4zIH97ev\nv+F9/rs3EKF4mBWsTcnx1yY8J3VXr/faOR7FLpcH/CzCmNxJhHe4k8Rzzz3nfvazn3WTyaR7zTXX\neNs7OjrcDRs2nPG72Vx+sqdRUPDwfmzOdV03Uxycyu4pVCner90d7E5MZfcuaiz9v3847meTmma9\n9NJL9Pjjj9NTTz1FsViMwuEw5fN5CgaD1NvbS83NzWf8/ruHO4iI6Irli2n3G0IFo2AhiMIy+rw2\n98wCMifoVz/a6W37yf/+tdeumwfPrCEistEPHDiE4zLPLBpDjpcpa/KcE89sqZh5vfSD/fQf//ka\nIiKKQ7+WzAA8Mx9b0AzSBJ5ZDfPMZLBIRIdnFmCzt7s/9guqNrxfmyMieqv/x3R1+x20u+tJKsfv\ncM/M9GO2l7d4/ln5XjNRVFbHy8/qkZXKnpkLD2vL3/7Ya2/9F9hu2TP7Tw99xtv2R59Z57ULRXhh\n5cn7mTyzD8+9g37d8SSVJ7AFJghrsJy0U2+Czfjm14XCx9KV+P0e+Bb6c2QflGf+6i+fIiKi276E\nAIFP3Hat114z6w6qNpwLu/vUI7+k/Q//O1rytf9Bri3GB32UZ8aUPMbwzPzjeGYr2hDQ9vm1on7j\nojaItB88gXunsfMVcyKobiSBcXZiz4zltzHPLDE4QKtXLqddb+w7p55ZzSLkOpJ8DrhayGQC7yd8\nmaVSKdq8eTP967/+K9XWCvpkzZo1tGPHDvr0pz9NO3fupOuuu+6Mx8ixhOR8Tsjw5Isoxma7aFtM\nxskicVHZESb5EsBN8kXQ/eEB8WMNdLOXgIvzWjaiJKOy4KaVZ5RTEZ9nc6AS8rYwAI3JXfmYGnTj\nTCSZLlgkXq49gxg8TAS2kaZjezEjrrO1bgV20FmhziheqO8eEAmXbU14IUcCSKatNpwLmyMiysnk\n1FzRJUM+GCYv2MqixrIFvDRyeWFvuj520nTEwIvL1nS5L0ukbgONaBmwG90vqL/6enxeYhUaiiyh\nW5cvT41tIx0DW1HS9EUnR5pMWuVRcKYB+ikaB81Y1yj60zYDtmbroBwbZuMYs+eL77ks9NinTZy8\nWqk4V3ZXjpIWf8X91ZjfYLKJcU0Y96kgaXGLKekbJdzzmVHY42JpYzkm8KAx6bRIEPd0zjxBWeuX\nYOIcMEFD22wsTg2IpZA9hw97295+G5PsN/fupV//fCv95Te/RUeOipdVKoWxymZ9d1gEeLmAZ7AB\nY1isCf1x+fdk5KJLXMJq4ijaCV9m27dvp0QiQXfffbe37aGHHqL777+ftm7dSu3t7bR+/fozHEFB\n4eygbE7hfEDZXWVjwpfZhg0baMOGDadtf/rppyd9EpvRfU75DW1igbLEy9MPI9dmqCRyJcINWFC/\n/ibMjLqykIg5OSTyZprmY8bhaJjJ2CV4XkUSC6WROGanfSdx3nwRntnCK6QLHsKMdXAEASm1zXD9\nSROz3lwa11vfhBmS5aK/jS1i0b2pCX3UdVAGwzl4Xk1SmijAaqf1dWE2VW04FzZHRJQv4a8uoz1K\nhJlsqYSZrMZyeExZe8lmFI/j4PM88+LyRXlc9iTFauC5GSZml36ZYxnw4z4XWI0yS2c0YkHYq48J\nrDI1I09o1iWNrJKYyWZzsPGCjhn/0BCer5xkIMIR2O3AEPKPLOYJRGRgSCaDbdls9UYenSu7C0hv\nPGD4ywpmtKgd9OT8tiavPacegQ/DaXGfRtK4XybLb4yVMH4UZX2vApOtisUwPoQZc1PW+41EcK4E\noxxfeOElr/3qq7uJiGj/AeSxDgyy80rG4Pd/OIB6ZFySi7EDBqvhZpiiP/4GVhvPRB+5RKBmlPM8\n8Wy47sR5ZkoBREFBQUGh4qFeZgoKCgoKFY9pSRopFvninTil5rD3qI1u+IOgCYMyVyKaASWZOoo8\nsquWwV2fv0z68zoWGIs5nON3v8H3BgYEDRCK4bjZHHI0apiU1GWrxOLpsT5Is1AMNGL7bIQu1tWJ\nYJBoBPRlzkLQR4rlMjmuOMepgX3etvpaTj8h96cmJBZ7SzlWNp1FtimMjUzR8v5aMmDCxyIYUylI\n/sQYBdPUIAMfWKCPy/IF+aJ7LivoXptFf9m8woIJWxmWEV3Hj4G2qWuDDRoh2KBrCzrPYZJbqTyo\n5XyxQHQJ0UB/wutbWZmciMhifT/BKPQRuVivs98hmcZ5dRf0ZC4vjnHoMKSvRpLVSzOeK1x/2ULv\nb21Y/IbzmxAJFmGBETU+2EpJRtrmIhh/rAwox0KWjZnl4CRGj4dNfO5n1TrSA0LiL92FQI1/2/2m\n1/7Btme99kCfWGLhOZgO83kcWVPP0QwvytFlwRmaH+O3yajOcr1IXzOCPojV9eMcukMy+GlU9QDG\nsY8D5ZkpKCgoKFQ81MtMQUFBQaHiMS00o11k9Jgsj+3zwQ3WfKBPYnFEWdk5QQN1ntjvbTu0D/kP\nseClXjtfL/IjckwmqyGEyBndQVRQU90iIiIKhBBpWGBq4DWNiJ4sSWmVVGrA2zZjJuhNjUlxvfgr\nEQnkD+NYzbNx7SaTGOrpEu580WbFRtOgJ+uDcMdrooKisHysgKRT3erl5wIpSZ+l0mky/YI+CzA1\ncNPkJd7xKGiyzQu2ZrOs5HxpVFgh/yM+58nLQdyz4WFBLz67/XlvW7zhFq899xKWvybzyyyWh5bN\ngVouX9vIcNrLS/KzXEjdQbu7FzZWlBGaPiZLVWRRm3YR5yjbWNcJVKIYHAQlqTA2bls1z/tbFk04\n3o0I6VdfRPTgMhYNrUkbLTLq8Mi7WIZYsHCR19Zl/tpwJ6IOMwlEpfZ0I1rx0BGxz8kBJgsYxvJI\n/QyIT7hyjLKLsDuLuTwFOb5GGlvIyop8zJAfdKDO6MA8K35sB8USSqgOUZ1lKp2IyGI0o0tSno3R\njLatohkVFBQUFC4CqJeZgoKCgkLFY1poRr8f7qRjSZqRJZPmbVB4Xb1/8NoHXn+LiIhiTD4oUkIE\nzP5f/95rB+YKl3QwD2ooPB904dyZiKw51SuoFO5K+1jBvRZGDTquoFWcLD4P66Cnjr0LLciXXhAF\n6GYuxc/qxFiEkQVZISspjlffhH07joEyODACWa+b1olE8daZoCQyFigDhbERksnPoYBJwaBocz3G\nYB0rGutjUXw5YUMjwyNsG5NDiyIyzZX0CKch+RQxUgO7+8CqlURE1HESNvPkP33fa1+/Fvp0l142\ni4iIalpga66LZ8ZnBL2/mqRlLGbP/aw44+EjHaf1zWZUKBc1yDF9yJCUT/KnYKMZphWpMDZyUroq\n5+93JYMAAA31SURBVPpoSBbMPNANSb5X9qHA8Cm2JNEQFc93DStWHGcR1yFW3eBUtxgzDx3HOLDn\n99BIPHQK1HAqL8/hgy195AMoznnLkku8dpkVDzIKvrMPlOWpPnHeTZ//PCXTYnno4NugQt/d86rX\n5nJWZpuI8HTYUoudxRhHGmxbl3TraJpRRTMqKCgoKFwEmBbPLFEq53hdQ6mCEALOsIls7/BbXrsr\n8aLXHugRs8tWP0qDN7A3eDKH2ae/R8yWzRxTC7cPeu3FH0GtqEFHfC/RhctvasOb/7JVbPYu848G\nBhBM0t+PGUUkipnTkiVC1T4+Exfn2kxQmWke9XSKxdHMEFuIL8CrHGYlzjuXiMXTSAyLp90D8GAV\nxoZfeix+skm3hUcRNODdcmFelwXUOFKeJxAAC2Ayzz3EAodSKeG520zIOhjG9yyCPc5fLGxw0Qrk\nQj67Ffb+kx+ipPxNGeHFXXUD7NZhArVl2alSySZNyra5LmayfX2YsafSsKtZc2bLbfAUevoQnODj\nIrgNoq37YXdplvekMDZe60rQDYsi9FpXwssH7e7F7x1mGuFDWWw/1iM8oPYYmKg/Xg/5vqUrLvfa\nZkiMOw1ts7xtzZcu9trrmJfeXC88utoQu7chdCIQhL1GZNvPRLbTBQQFDWXFc/Qnn/kEdQ8Lu/pN\nE/Jjc0z2rWsQNuhKpeHsEDxGpl9NoTCu2ZUFMLlnxvM8x4PyzBQUFBQUKh7qZaagoKCgUPGYHpox\nDTmdvuQBIiKyc6ArhtMIfHCYZE+NlILJjiC3LFLPFgrZQrw/KNzUeAmLpHoLXOm6JrjS8Rrhvp54\nFzSlxmrnDPXiHV+wxIJnSysKY57sBKU0OIDryBcEZdDMVFoCrP4ad5sLBUFrdR+ExEzEjy8uugK5\nH2lJOQ4k4Gr7AxMviF7ssGSemFXMkyVrm7HanBQOg3L0+0Ejlguxmmwbpzm4lJgjcyh1m0kQFXBv\nuDL/UELQLqvXojDi1R+6ymu/9iLqRh07Luj41pNYMA9EQcXUlAsqan6vtlkyCVtMpWGjC5fO99q1\ntSK/KF6HH2J4BDZo6Ng+e6HIdcwzGaVsUdGMEyExlCCimZQYSlC5TJfGcqpMDXZVZMFkrfXCxmYu\nuMLbdsnlq7x2rBZLGuVae/EoxpSWBtCMTEWNdKk+rzFaXSMWXMEpPFkTrWix4pxsaScscxnDpp9a\nasRzcvVVsOFAFEF3v/jVv3ntE13HxeEdVruSjXc6q/vnI1NeI86rTaKOnvLMFBQUFBQqHuplpqCg\noKBQ8ZgWmjGX6vHamiEip/wxRFjVhBn9dhTUYKxJuOalRkQPav56r91ev9xrn+oU5xg5hCjApTOQ\nSxGNwpWeNVPQMoNdOO7Rd5gqehLurREWdI0Zgnvc0o4+9JxCjpzjSCrB5e483PV4LSiFefOFEn7/\nYaj5WyyHLjkEeqqnW9BABRu0aAOT3FIYGxlZSDKTLVHJkrbEtHmKRdhdmBVf9XJaWHQgLzTI5dlK\nOXHcbBrRY72diOJqYZFedTXinmUZ9ThnBaTREnm0TSldlgYDSCUd5zBDop0rWmRbkkJlKuUtM0CL\nz70EdleUUW6sbi0VmTL/SBLPT0TmPYWC+G18YdBBCmOjrSbi/S1JWyppeF4DEbRPsOIXZo2wlevW\nXultq2eRjSVG/TlSNirNVO1MJncXA5PpwcfsWTewr6EzCq9sGFxeyjk9qtB1ydNwq42D/lw8H8sj\n77zb5rU7OwXNyGWrOKXNI3E9iTgWYTxxLKPyzBQUFBQUqgDqZaagoKCgUPGYkGbM5XJ077330uDg\nIBUKBbrrrrvo0ksvpXvuuYds26ampiZ6+OGHRyWVnnaMoQNeu1QUSXMFDS6kGQO91rYMyvFldXIr\nwIrDjSCCMdmHiK30sGjnukEHvvU7JE03xHGpul+47td8GLTM3HlIZK1vgu8fbxYUTaiBRd7oUJwe\n6IRbvXTpXNHHwAlvG5UYLePgNzLDUrIFDBDFooxGcJBMmZYUlsVopmAQkXjVhnNhc0REwyO5UX+J\niGwbckxZVuxUYwU1CzKillOLPLGUq+2ns4IuL1kgQmL1oF1WXw/KaPZcQbvoTK4oVo8E7CtWgRYP\nm8I243HYe4FwHeWIS0P3kSbppQCjbTgvk2fq/+UCnsEQ7CfGJJPMAK7NMGX1AJY0yz+vNpwru7uk\nMe79tR1hb8M+PNvZGtCMC+vqvPb8K0VS9IwZEGgosoKrhsHovtMaRA5LWB4tfSbsw2C+i8apRS4e\nUF4iGYfXcyT15ziOd74ACxGOM8GABbNxHUeOHiUiolND4M1dVpxT1zBOliMXddZH15mYaJzwZfbC\nCy/Q8uXL6Y477qDOzk768pe/TCtXrqRNmzbRzTffTI8++iht27aNNm3aNOHJFBQmA2VzCucDyu4q\nGxO+zG65BfWWuru7qaWlhXbv3k0PPvggERGtW7eOtmzZcsYb3MpkVCIhKZBKeCu7bOHSrMPss5gQ\nM8YsdC4psR+L62Yai6PxghDxtZiQbMHFLNyx4YUlesVMNcVqn10yDwv1hRJmzkMnxfn0NDoRjOIc\n8+ZBYmbFihXi+HnMXvv74WE5RVyzIRNBLr96LrbZCexL8DpzlvhNNPabafpklkQrE+fC5oiIHJmv\n4pBJ/nIdMx0zwHSGeWysjlcmLXKpDGaXdbUsKMg3KpGQiIiCLDCi1WT23oj6XyEpOm07OK7Pga34\n6nCMSEB4bH4fjlVi9cx0qQWk25onbZVMIXijwK5HY9fhk31zWeBAIMj64EcfMllxDJ3lQqVT8PKq\nDefK7hpjIe9vqSh+73QWY0p4Obz1WY3wvBdfIgKATOZB6X7cf1Y2jPzSHHneJM8d87GaaGUHh6dq\n6UyuaiwPqFxTjIiIaVJ7tfpKrkuu/J7BcnQjIdjKZSuQT1mQrt7Ol1/3tvWNwJZ01jnDi04aO0d3\nPEw6mnHjxo3U09NDjz/+OH3pS1/yXO2Ghgbq7++f4NsKCmcPZXMK5wPK7ioU7lngnXfecT/xiU+4\nV199tbeto6PD3bBhwxm/lxg6fjanUVDw8F5tznVdt3+kZyq7plDFeD92ly7YU9m1ixqz/8PfjfvZ\nhJ7Zvn37qKGhgdra2mjJkiVk2zZFIhHK5/MUDAapt7eXmpubz3iM1/7n3xAR0ce+8gT99JdfIyKi\nvlPImeob6vXaVhj0iK8oKA+rE/lgmX6WeKNj8ZwKYiG1YLIgiflwtQ1WC6jvsDj30FGct+kS0CvN\n87CvJXPHQnWgKYfykPRpbhWSP/fdto2eePFPiYjIH4H73NnV6bVPduJ8oag8XgmLyVYev4nPj99h\neEAqs7OS5HMWItftbz6HqgPVgHNhc0RET+78Lv3lZ/+avrPtftLpdJoiw0o3BLh0lcyFYfEfFIsx\nlXFGpeTKeUSsrLvL7hNpaOuSPilmsc0ugO8rZHDPA7JemUePEtHAEKju+rpauvdP/pYe+v7XyZGL\n9p2nTnmf51ldssY2BCwZkp/K5UB/1zcgIEFnF93VJWhvHlhgszyhf/nq01RNOFd2t6djmNYuqqff\nHByitKSGU0zijEuntdeBsg4ZUhrNYJQ2Swg02MpCQP7Ddh1N1Y0RwcGpRY0d12W0pu3ZM76fKcJG\nE/k8rVnQRK8e7qectF2bvUZyFq4zXQKN3z0gxrZf7ECViIPHkWNrhLBkZEiZK23UMytp9dOuil3f\nGT4jIqLXX3+dtmzZQkREAwMDlM1mac2aNbRjxw4iItq5cyddd911ZzqEgsJZQdmcwvmAsrvKxoSe\n2caNG+kb3/gGbdq0ifL5PD3wwAO0fPly+vrXv05bt26l9vZ2Wr9+/XT0VeEigbI5hfMBZXeVDc11\nJ1H1TEFBQUFB4QKGUgBRUFBQUKh4qJeZgoKCgkLFQ73MFBQUFBQqHuplpqCgoKBQ8VAvMwUFBQWF\niod6mSkoKCgoVDympdL0t7/9bdq7dy9pmkb33XcfXXbZZdNx2inF5s2bac+ePWRZFt155520YsWK\nsy4VoTC1qDa7UzZ34aPabI6oguxuqrW0du/e7X7lK19xXdd1Dx8+7N52221Tfcopx65du9zbb7/d\ndV3XHRoacq+//nr33nvvdbdv3+66rus+8sgj7jPPPHM+u3jRo9rsTtnchY9qsznXrSy7m3Kacdeu\nXXTjjTcSEdH8+fNpZGSE0un0BN+6sLFq1Sp67LHHiEgUT8zlcrR792664YYbiEiUiti1a9f57OJF\nj2qzO2VzFz6qzeaIKsvupvxlNjAwQHWsmmp9fX3Fl1EwDIPCYSE8u23bNlq7di3lcjlVKuICQrXZ\nnbK5Cx/VZnNElWV30x4A4laRetbzzz9P27ZtowceeGDU9mq6xmpBtdwTZXOVg2q6J5Vgd1P+Mmtu\nbqaBgQHv/76+Pmpqaprq0045XnrpJXr88cfpySefpFgsRuFwmPJ5UfplsqUiFKYO1Wh3yuYubFSj\nzRFVjt1N+cvs2muv9UoovP3229Tc3EzRaHSCb13YSKVStHnzZnriiSeotlbUglKlIi4sVJvdKZu7\n8FFtNkdUWXY35aH5K1eupGXLltHGjRtJ0zT65je/OdWnnHJs376dEokE3X333d62hx56iO6//35V\nKuICQbXZnbK5Cx/VZnNElWV3qgSMgoKCgkLFQymAKCgoKChUPNTLTEFBQUGh4qFeZgoKCgoKFQ/1\nMlNQUFBQqHiol5mCgoKCQsVDvcwUFBQUFCoe6mWmoKCgoFDxUC8zBQUFBYWKx/8PvM6ImO8Hzb8A\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7ff4b86c8208>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "shapes: Xtrain  (20000, 3, 32, 32)  Y_train : (20000, 10)  Xtest  (2000, 3, 32, 32)  Y_test : (2000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SjIfSrgGhQU0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Output**:\n",
        "The output should contain the following images as an indication that the dataset has been loaded and is ready to use:\n",
        "![alt text](https://github.com/TheIndianCoder/Choose-the-best-Optimization-Algorithm-in-keras/blob/master/utils/images/output1.PNG?raw=true)\n"
      ]
    },
    {
      "metadata": {
        "id": "DqIuxD6WjW_4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 3\n",
        " **Create CNN models for each of the 7 optimization algorithms**\n",
        "  \n",
        "   Each moel returns the accuracy, time taken and a [Keras History object](https://keras.io/callbacks/#history) which we'll need to plot the graphs."
      ]
    },
    {
      "metadata": {
        "id": "Hhm5bNhWjlgP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "def ConvModel_SGD(alpha=0.01,epochs=1,loss_fun='categorical_crossentropy'):\n",
        "    print(\"\\nUsing SGD with alpha : \",alpha)\n",
        "    \n",
        "    start=time.time()\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    # Compile model\n",
        "    lrate = alpha\n",
        "    decay = lrate/epochs\n",
        "    sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    \n",
        "    #print(model.summary())\n",
        "    \n",
        "    # Fit the model\n",
        "    hist=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=128)\n",
        "    \n",
        "    # Final evaluation of the model\n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    end=time.time()\n",
        "    \n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "    \n",
        "    return [scores[1]*100,(end-start),hist]\n",
        "\n",
        "def ConvModel_RMS(alpha=0.01,epochs=1,loss_fun='categorical_crossentropy'):\n",
        "    print(\"\\nUsing RMS with alpha : \",alpha)\n",
        "    \n",
        "    start=time.time()\n",
        "    \n",
        "    RMS=keras.optimizers.RMSprop\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    # Compile model\n",
        "    lrate = alpha\n",
        "    decay = lrate/epochs\n",
        "    rms=RMS(lr=lrate,decay=decay)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])\n",
        "   \n",
        "  # print(model.summary())\n",
        "    \n",
        "    # Fit the model\n",
        "    hist=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=128)\n",
        "    \n",
        "    # Final evaluation of the model\n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    end=time.time()\n",
        "    \n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "    \n",
        "    return [scores[1]*100,(end-start),hist]\n",
        "\n",
        "\n",
        "  \n",
        "def ConvModel_ADAGrad(alpha=0.01,epochs=1):\n",
        "    print(\"\\nUsing ADAGrad with alpha : \",alpha)\n",
        "    start=time.time()\n",
        "    ADAGRAD=keras.optimizers.Adagrad\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    # Compile model\n",
        "    \n",
        "    lrate = alpha\n",
        "    decay = lrate/epochs\n",
        "    ADA=ADAGRAD(lr=lrate,decay=decay)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=ADA, metrics=['accuracy'])\n",
        "   # print(model.summary())\n",
        "    \n",
        "    # Fit the model\n",
        "    hist=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=128)\n",
        "    # Final evaluation of the model\n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    \n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "    #print(\"Scores shape :\",scores.shape)\n",
        "    \n",
        "    end=time.time()\n",
        "    \n",
        "    return [scores[1]*100,(end-start),hist]\n",
        "\n",
        "\n",
        "  \n",
        "def ConvModel_ADADelta(alpha=0.01,epochs=1):\n",
        "    print(\"\\nUsing ADDelta with alpha : \",alpha)\n",
        "    start=time.time()\n",
        "    \n",
        "    ADADEL=keras.optimizers.Adadelta\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    # Compile model\n",
        "    \n",
        "    lrate = alpha\n",
        "    decay = lrate/epochs\n",
        "    ADADEL=ADADEL(lr=lrate,decay=decay)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=ADADEL, metrics=['accuracy'])\n",
        "    \n",
        "    #print(model.summary())\n",
        "    \n",
        "    # Fit the model\n",
        "    hist=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=128)\n",
        "    \n",
        "    # Final evaluation of the model\n",
        "    \n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "    \n",
        "    end=time.time()\n",
        "    \n",
        "    return [scores[1]*100,(end-start),hist]\n",
        "#ConvModel_ADADelta()\n",
        "\n",
        "\n",
        "def ConvModel_ADAM(alpha=0.01,epochs=1):\n",
        "    print(\"\\nUsing ADAM with alpha : \",alpha)\n",
        "    start=time.time()\n",
        "    \n",
        "    ADAM=optimizers.Adam\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    # Compile model\n",
        "    \n",
        "    lrate = alpha\n",
        "    decay = lrate/epochs\n",
        "    ADAM=ADAM(lr=lrate,decay=decay)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=ADAM, metrics=['accuracy'])\n",
        "    \n",
        "    #print(model.summary())\n",
        "    # Fit the model\n",
        "    \n",
        "    hist=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=128)\n",
        "    \n",
        "    # Final evaluation of the model\n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "    \n",
        "    end=time.time()\n",
        "    \n",
        "    return [scores[1]*100,(end-start),hist]\n",
        "#ConvModel_ADAM()\n",
        "\n",
        "\n",
        "\n",
        "def ConvModel_ADAMAX(alpha=0.01,epochs=1):\n",
        "\n",
        "    print(\"\\nUsing ADAMAX with alpha : \",alpha)\n",
        "    start=time.time()\n",
        "    \n",
        "    ADAMAX=keras.optimizers.Adamax\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    # Compile model\n",
        "    \n",
        "    lrate = alpha\n",
        "    decay = lrate/epochs\n",
        "    ADAMAX=ADAMAX(lr=lrate,decay=decay)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=ADAMAX, metrics=['accuracy'])\n",
        "    #print(model.summary())\n",
        "    \n",
        "    # Fit the model\n",
        "    hist=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=128)\n",
        "    \n",
        "    # Final evaluation of the model\n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "    \n",
        "    end=time.time()\n",
        "    \n",
        "    return [scores[1]*100,(end-start),hist]\n",
        "  \n",
        "  #ConvModel_ADAMAX()\n",
        "\n",
        "  \n",
        "def ConvModel_NADAM(alpha=0.01,epochs=1):\n",
        "    \n",
        "    print(\"\\nUsing NADAM with alpha : \",alpha)\n",
        "    start=time.time()\n",
        "    \n",
        "    NADAM=optimizers.Nadam\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    # Compile model\n",
        "    \n",
        "    lrate = alpha\n",
        "    NADAM=NADAM(lr=lrate)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=NADAM, metrics=['accuracy'])\n",
        "   # print(model.summary())\n",
        "    \n",
        "    # Fit the model\n",
        "    hist=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=128)\n",
        "    \n",
        "    # Final evaluation of the model\n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "    end=time.time()\n",
        "    return [scores[1]*100,(end-start),hist]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "thyZuHmYlD97",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After Compiling the models successfully, we now decide which **learning rate to choose for each Optimization algorithm** we choose the best from\n",
        "                        \n",
        "                        [0.00001, 0.0001, 0.001, 0.01, 0.1 , 1, 10] \n",
        "\n",
        "\n",
        "   by     **training each model for one epoch and comparing the results**"
      ]
    },
    {
      "metadata": {
        "id": "08nDQx-5zPJ3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Choose the best learning rates for each Algorithm by passing through one epoch each\n",
        "\n",
        "'''\n",
        "def CompareAlgos_alpha():\n",
        "    lr_list=[0.00001,0.0001,0.001,0.001,0.01,0.1,1,10]\n",
        "    '''\n",
        "      every algorithm in the deictionary has corresponding two values in the list : accuracy and time\n",
        "      \n",
        "      100*accuracy  - time_spent is used as a metric here to compare the results of different learning rates for each Algorithm. The reader is free to use another metric\n",
        "    '''\n",
        "    result_lr_list={'SGD':[0,0],'NADAM':[0,0],'ADAMAX':[0,0],'RMS':[0,0],'ADAM':[0,0],'ADAGrad':[0,0],'ADADelta':[0,0]}\n",
        "    \n",
        "    \n",
        "    for i in lr_list:\n",
        "        print(\"Calling SGD with lr= \",i)\n",
        "        [acc,time_spent,_]=ConvModel_SGD(alpha=i)\n",
        "        if(result_lr_list['SGD'][0]<(100*acc-time_spent)):\n",
        "            result_lr_list['SGD'][0]=100*acc-time_spent\n",
        "            result_lr_list['SGD'][1]=i\n",
        "    \n",
        "    for i in lr_list:\n",
        "        print(\"Calling RMS with lr= \",i)\n",
        "        [acc,time_spent,_]=ConvModel_RMS(alpha=i)\n",
        "        if(result_lr_list['RMS'][0]<(100*acc-time_spent)):\n",
        "            result_lr_list['RMS'][0]=100*acc-time_spent\n",
        "            result_lr_list['RMS'][1]=i\n",
        "   \n",
        "    for i in lr_list:\n",
        "        print(\"Calling ADAGrad with lr= \",i)\n",
        "        [acc,time_spent,_]=ConvModel_ADAGrad(alpha=i)\n",
        "        if(result_lr_list['ADAGrad'][0]<(100*acc-time_spent)):\n",
        "            result_lr_list['ADAGrad'][0]=100*acc-time_spent\n",
        "            result_lr_list['ADAGrad'][1]=i\n",
        "\n",
        "    for i in lr_list:\n",
        "        print(\"Calling ADADelta with lr= \",i)\n",
        "        [acc,time_spent,_]=ConvModel_ADADelta(alpha=i)\n",
        "        if(result_lr_list['ADADelta'][0]<(100*acc-time_spent)):\n",
        "            result_lr_list['ADADelta'][0]=100*acc-time_spent\n",
        "            result_lr_list['ADADelta'][1]=i\n",
        "            \n",
        "    for i in lr_list:\n",
        "        print(\"Calling ADAM with lr= \",i)\n",
        "        [acc,time_spent,_]=ConvModel_ADAM(alpha=i)\n",
        "        if(result_lr_list['ADAM'][0]<(100*acc-time_spent)):\n",
        "            result_lr_list['ADAM'][0]=100*acc-time_spent\n",
        "            result_lr_list['ADAM'][1]=i\n",
        "    \n",
        "    for i in lr_list:\n",
        "        print(\"Calling ADAMAX with lr= \",i)\n",
        "        [acc,time_spent,_]=ConvModel_ADAMAX(alpha=i)\n",
        "        if(result_lr_list['ADAMAX'][0]<(100*acc-time_spent)):\n",
        "            result_lr_list['ADAMAX'][0]=100*acc-time_spent\n",
        "            result_lr_list['ADAMAX'][1]=i\n",
        "    \n",
        "    for i in lr_list:\n",
        "        print(\"Calling NADAM with lr= \",i)\n",
        "        [acc,time_spent,_]=ConvModel_NADAM(alpha=i)\n",
        "        if(result_lr_list['NADAM'][0]<(100*acc-time_spent)):\n",
        "            result_lr_list['NADAM'][0]=100*acc-time_spent\n",
        "            result_lr_list['NADAM'][1]=i\n",
        "    print(result_lr_list)\n",
        "    return result_lr_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y0RMvTTQ0Ldk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We call the above function to get the best learning rates for each Algorithm."
      ]
    },
    {
      "metadata": {
        "id": "Up3mi-J10VJm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7183
        },
        "outputId": "f088a6cf-af0c-4ddc-fdd5-f2bdb44bc2a6"
      },
      "cell_type": "code",
      "source": [
        "result_lr=CompareAlgos_alpha()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling SGD with lr=  1e-05\n",
            "\n",
            "Using SGD with alpha :  1e-05\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 6s 322us/step - loss: 2.3520 - acc: 0.1077 - val_loss: 2.3174 - val_acc: 0.1025\n",
            "Accuracy: 10.25%\n",
            "Calling SGD with lr=  0.0001\n",
            "\n",
            "Using SGD with alpha :  0.0001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 4s 189us/step - loss: 2.2946 - acc: 0.1256 - val_loss: 2.2750 - val_acc: 0.1565\n",
            "Accuracy: 15.65%\n",
            "Calling SGD with lr=  0.001\n",
            "\n",
            "Using SGD with alpha :  0.001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 4s 195us/step - loss: 2.2142 - acc: 0.1735 - val_loss: 2.0848 - val_acc: 0.2860\n",
            "Accuracy: 28.60%\n",
            "Calling SGD with lr=  0.001\n",
            "\n",
            "Using SGD with alpha :  0.001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 4s 199us/step - loss: 2.2454 - acc: 0.1618 - val_loss: 2.1671 - val_acc: 0.2695\n",
            "Accuracy: 26.95%\n",
            "Calling SGD with lr=  0.01\n",
            "\n",
            "Using SGD with alpha :  0.01\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 2.0146 - acc: 0.2665 - val_loss: 1.8697 - val_acc: 0.3565\n",
            "Accuracy: 35.65%\n",
            "Calling SGD with lr=  0.1\n",
            "\n",
            "Using SGD with alpha :  0.1\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 2.0890 - acc: 0.2386 - val_loss: 1.9545 - val_acc: 0.3185\n",
            "Accuracy: 31.85%\n",
            "Calling SGD with lr=  1\n",
            "\n",
            "Using SGD with alpha :  1\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 4s 206us/step - loss: 2.1798 - acc: 0.1915 - val_loss: 1.9692 - val_acc: 0.3180\n",
            "Accuracy: 31.80%\n",
            "Calling SGD with lr=  10\n",
            "\n",
            "Using SGD with alpha :  10\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 4s 210us/step - loss: 2.0949 - acc: 0.2368 - val_loss: 1.9580 - val_acc: 0.3230\n",
            "Accuracy: 32.30%\n",
            "Calling RMS with lr=  1e-05\n",
            "\n",
            "Using RMS with alpha :  1e-05\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 5s 235us/step - loss: 2.2589 - acc: 0.1462 - val_loss: 2.2245 - val_acc: 0.2050\n",
            "Accuracy: 20.50%\n",
            "Calling RMS with lr=  0.0001\n",
            "\n",
            "Using RMS with alpha :  0.0001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 5s 234us/step - loss: 2.0619 - acc: 0.2505 - val_loss: 1.9418 - val_acc: 0.3745\n",
            "Accuracy: 37.45%\n",
            "Calling RMS with lr=  0.001\n",
            "\n",
            "Using RMS with alpha :  0.001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 5s 239us/step - loss: 1.9732 - acc: 0.3090 - val_loss: 1.6447 - val_acc: 0.4000\n",
            "Accuracy: 40.00%\n",
            "Calling RMS with lr=  0.001\n",
            "\n",
            "Using RMS with alpha :  0.001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 5s 244us/step - loss: 1.9041 - acc: 0.3259 - val_loss: 1.5132 - val_acc: 0.4730\n",
            "Accuracy: 47.30%\n",
            "Calling RMS with lr=  0.01\n",
            "\n",
            "Using RMS with alpha :  0.01\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 5s 246us/step - loss: 14.4630 - acc: 0.0977 - val_loss: 14.4821 - val_acc: 0.1015\n",
            "Accuracy: 10.15%\n",
            "Calling RMS with lr=  0.1\n",
            "\n",
            "Using RMS with alpha :  0.1\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 5s 252us/step - loss: 14.4212 - acc: 0.1004 - val_loss: 14.3693 - val_acc: 0.1085\n",
            "Accuracy: 10.85%\n",
            "Calling RMS with lr=  1\n",
            "\n",
            "Using RMS with alpha :  1\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 5s 252us/step - loss: 14.3831 - acc: 0.1029 - val_loss: 14.4096 - val_acc: 0.1060\n",
            "Accuracy: 10.60%\n",
            "Calling RMS with lr=  10\n",
            "\n",
            "Using RMS with alpha :  10\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 5s 258us/step - loss: 14.4124 - acc: 0.1009 - val_loss: 14.5627 - val_acc: 0.0965\n",
            "Accuracy: 9.65%\n",
            "Calling ADAGrad with lr=  1e-05\n",
            "\n",
            "Using ADAGrad with alpha :  1e-05\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 5s 261us/step - loss: 2.3084 - acc: 0.1138 - val_loss: 2.2894 - val_acc: 0.1660\n",
            "Accuracy: 16.60%\n",
            "Calling ADAGrad with lr=  0.0001\n",
            "\n",
            "Using ADAGrad with alpha :  0.0001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 5s 268us/step - loss: 2.1723 - acc: 0.2078 - val_loss: 2.1223 - val_acc: 0.3000\n",
            "Accuracy: 30.00%\n",
            "Calling ADAGrad with lr=  0.001\n",
            "\n",
            "Using ADAGrad with alpha :  0.001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 5s 266us/step - loss: 2.0247 - acc: 0.2692 - val_loss: 1.8833 - val_acc: 0.3620\n",
            "Accuracy: 36.20%\n",
            "Calling ADAGrad with lr=  0.001\n",
            "\n",
            "Using ADAGrad with alpha :  0.001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 6s 275us/step - loss: 1.9423 - acc: 0.2984 - val_loss: 1.8264 - val_acc: 0.3685\n",
            "Accuracy: 36.85%\n",
            "Calling ADAGrad with lr=  0.01\n",
            "\n",
            "Using ADAGrad with alpha :  0.01\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 6s 278us/step - loss: 2.0518 - acc: 0.2822 - val_loss: 1.6426 - val_acc: 0.4180\n",
            "Accuracy: 41.80%\n",
            "Calling ADAGrad with lr=  0.1\n",
            "\n",
            "Using ADAGrad with alpha :  0.1\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 6s 280us/step - loss: 14.4275 - acc: 0.1001 - val_loss: 14.5224 - val_acc: 0.0990\n",
            "Accuracy: 9.90%\n",
            "Calling ADAGrad with lr=  1\n",
            "\n",
            "Using ADAGrad with alpha :  1\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 6s 280us/step - loss: 14.4722 - acc: 0.0970 - val_loss: 14.5385 - val_acc: 0.0980\n",
            "Accuracy: 9.80%\n",
            "Calling ADAGrad with lr=  10\n",
            "\n",
            "Using ADAGrad with alpha :  10\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 6s 288us/step - loss: 14.4521 - acc: 0.0985 - val_loss: 14.4821 - val_acc: 0.1015\n",
            "Accuracy: 10.15%\n",
            "Calling ADADelta with lr=  1e-05\n",
            "\n",
            "Using ADDelta with alpha :  1e-05\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 7s 328us/step - loss: 2.3263 - acc: 0.0907 - val_loss: 2.3099 - val_acc: 0.0810\n",
            "Accuracy: 8.10%\n",
            "Calling ADADelta with lr=  0.0001\n",
            "\n",
            "Using ADDelta with alpha :  0.0001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 7s 337us/step - loss: 2.3548 - acc: 0.1038 - val_loss: 2.3127 - val_acc: 0.1200\n",
            "Accuracy: 12.00%\n",
            "Calling ADADelta with lr=  0.001\n",
            "\n",
            "Using ADDelta with alpha :  0.001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 7s 345us/step - loss: 2.3091 - acc: 0.1067 - val_loss: 2.2969 - val_acc: 0.1380\n",
            "Accuracy: 13.80%\n",
            "Calling ADADelta with lr=  0.001\n",
            "\n",
            "Using ADDelta with alpha :  0.001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 7s 350us/step - loss: 2.3246 - acc: 0.0948 - val_loss: 2.3019 - val_acc: 0.0985\n",
            "Accuracy: 9.85%\n",
            "Calling ADADelta with lr=  0.01\n",
            "\n",
            "Using ADDelta with alpha :  0.01\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 7s 355us/step - loss: 2.2859 - acc: 0.1338 - val_loss: 2.2495 - val_acc: 0.2275\n",
            "Accuracy: 22.75%\n",
            "Calling ADADelta with lr=  0.1\n",
            "\n",
            "Using ADDelta with alpha :  0.1\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 7s 360us/step - loss: 2.1881 - acc: 0.1923 - val_loss: 2.1442 - val_acc: 0.2920\n",
            "Accuracy: 29.20%\n",
            "Calling ADADelta with lr=  1\n",
            "\n",
            "Using ADDelta with alpha :  1\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 7s 365us/step - loss: 2.2046 - acc: 0.1931 - val_loss: 2.1807 - val_acc: 0.2710\n",
            "Accuracy: 27.10%\n",
            "Calling ADADelta with lr=  10\n",
            "\n",
            "Using ADDelta with alpha :  10\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 7s 372us/step - loss: 2.2389 - acc: 0.1730 - val_loss: 2.1962 - val_acc: 0.2410\n",
            "Accuracy: 24.10%\n",
            "Calling ADAM with lr=  1e-05\n",
            "\n",
            "Using ADAM with alpha :  1e-05\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 7s 371us/step - loss: 2.2512 - acc: 0.1578 - val_loss: 2.1947 - val_acc: 0.2195\n",
            "Accuracy: 21.95%\n",
            "Calling ADAM with lr=  0.0001\n",
            "\n",
            "Using ADAM with alpha :  0.0001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 8s 376us/step - loss: 2.0767 - acc: 0.2416 - val_loss: 1.9351 - val_acc: 0.3400\n",
            "Accuracy: 34.00%\n",
            "Calling ADAM with lr=  0.001\n",
            "\n",
            "Using ADAM with alpha :  0.001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 8s 383us/step - loss: 1.9384 - acc: 0.2982 - val_loss: 1.5929 - val_acc: 0.4635\n",
            "Accuracy: 46.35%\n",
            "Calling ADAM with lr=  0.001\n",
            "\n",
            "Using ADAM with alpha :  0.001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 8s 392us/step - loss: 1.8550 - acc: 0.3274 - val_loss: 1.5199 - val_acc: 0.4695\n",
            "Accuracy: 46.95%\n",
            "Calling ADAM with lr=  0.01\n",
            "\n",
            "Using ADAM with alpha :  0.01\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 8s 398us/step - loss: 2.3447 - acc: 0.1008 - val_loss: 2.3028 - val_acc: 0.0995\n",
            "Accuracy: 9.95%\n",
            "Calling ADAM with lr=  0.1\n",
            "\n",
            "Using ADAM with alpha :  0.1\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 8s 407us/step - loss: 2.4985 - acc: 0.1018 - val_loss: 2.3019 - val_acc: 0.1080\n",
            "Accuracy: 10.80%\n",
            "Calling ADAM with lr=  1\n",
            "\n",
            "Using ADAM with alpha :  1\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 8s 412us/step - loss: 14.4013 - acc: 0.1017 - val_loss: 14.3773 - val_acc: 0.1080\n",
            "Accuracy: 10.80%\n",
            "Calling ADAM with lr=  10\n",
            "\n",
            "Using ADAM with alpha :  10\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 8s 418us/step - loss: 14.4622 - acc: 0.0980 - val_loss: 14.4821 - val_acc: 0.1015\n",
            "Accuracy: 10.15%\n",
            "Calling ADAMAX with lr=  1e-05\n",
            "\n",
            "Using ADAMAX with alpha :  1e-05\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 8s 411us/step - loss: 2.3058 - acc: 0.1202 - val_loss: 2.2758 - val_acc: 0.1680\n",
            "Accuracy: 16.80%\n",
            "Calling ADAMAX with lr=  0.0001\n",
            "\n",
            "Using ADAMAX with alpha :  0.0001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 8s 423us/step - loss: 2.1611 - acc: 0.2086 - val_loss: 2.0729 - val_acc: 0.2835\n",
            "Accuracy: 28.35%\n",
            "Calling ADAMAX with lr=  0.001\n",
            "\n",
            "Using ADAMAX with alpha :  0.001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 9s 431us/step - loss: 1.9989 - acc: 0.2699 - val_loss: 1.8030 - val_acc: 0.3975\n",
            "Accuracy: 39.75%\n",
            "Calling ADAMAX with lr=  0.001\n",
            "\n",
            "Using ADAMAX with alpha :  0.001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 9s 425us/step - loss: 1.8643 - acc: 0.3294 - val_loss: 1.6588 - val_acc: 0.4385\n",
            "Accuracy: 43.85%\n",
            "Calling ADAMAX with lr=  0.01\n",
            "\n",
            "Using ADAMAX with alpha :  0.01\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 9s 428us/step - loss: 2.1393 - acc: 0.2368 - val_loss: 1.9633 - val_acc: 0.2920\n",
            "Accuracy: 29.20%\n",
            "Calling ADAMAX with lr=  0.1\n",
            "\n",
            "Using ADAMAX with alpha :  0.1\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 9s 439us/step - loss: 14.4829 - acc: 0.0969 - val_loss: 14.6272 - val_acc: 0.0925\n",
            "Accuracy: 9.25%\n",
            "Calling ADAMAX with lr=  1\n",
            "\n",
            "Using ADAMAX with alpha :  1\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 9s 448us/step - loss: 14.4382 - acc: 0.0993 - val_loss: 14.5224 - val_acc: 0.0990\n",
            "Accuracy: 9.90%\n",
            "Calling ADAMAX with lr=  10\n",
            "\n",
            "Using ADAMAX with alpha :  10\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 9s 455us/step - loss: 14.4310 - acc: 0.1000 - val_loss: 14.5143 - val_acc: 0.0995\n",
            "Accuracy: 9.95%\n",
            "Calling NADAM with lr=  1e-05\n",
            "\n",
            "Using NADAM with alpha :  1e-05\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 10s 495us/step - loss: 2.2539 - acc: 0.1556 - val_loss: 2.2108 - val_acc: 0.1990\n",
            "Accuracy: 19.90%\n",
            "Calling NADAM with lr=  0.0001\n",
            "\n",
            "Using NADAM with alpha :  0.0001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 10s 499us/step - loss: 2.0150 - acc: 0.2723 - val_loss: 1.9655 - val_acc: 0.2805\n",
            "Accuracy: 28.05%\n",
            "Calling NADAM with lr=  0.001\n",
            "\n",
            "Using NADAM with alpha :  0.001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 10s 506us/step - loss: 1.9270 - acc: 0.3154 - val_loss: 1.6441 - val_acc: 0.4120\n",
            "Accuracy: 41.20%\n",
            "Calling NADAM with lr=  0.001\n",
            "\n",
            "Using NADAM with alpha :  0.001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 10s 519us/step - loss: 1.8045 - acc: 0.3479 - val_loss: 1.5670 - val_acc: 0.4540\n",
            "Accuracy: 45.40%\n",
            "Calling NADAM with lr=  0.01\n",
            "\n",
            "Using NADAM with alpha :  0.01\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 10s 525us/step - loss: 2.3621 - acc: 0.0971 - val_loss: 2.3028 - val_acc: 0.1080\n",
            "Accuracy: 10.80%\n",
            "Calling NADAM with lr=  0.1\n",
            "\n",
            "Using NADAM with alpha :  0.1\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 11s 532us/step - loss: 14.4463 - acc: 0.0989 - val_loss: 14.5224 - val_acc: 0.0990\n",
            "Accuracy: 9.90%\n",
            "Calling NADAM with lr=  1\n",
            "\n",
            "Using NADAM with alpha :  1\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 11s 539us/step - loss: 14.4430 - acc: 0.0994 - val_loss: 14.5143 - val_acc: 0.0995\n",
            "Accuracy: 9.95%\n",
            "Calling NADAM with lr=  10\n",
            "\n",
            "Using NADAM with alpha :  10\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 11s 553us/step - loss: 14.4346 - acc: 0.0998 - val_loss: 14.5385 - val_acc: 0.0980\n",
            "Accuracy: 9.80%\n",
            "{'SGD': [3560.2491455078125, 0.01], 'NADAM': [4528.566457986832, 0.001], 'ADAMAX': [4375.603558063507, 0.001], 'RMS': [4724.087929725647, 0.001], 'ADAM': [4686.18185710907, 0.001], 'ADAGrad': [4173.383383512497, 0.01], 'ADADelta': [2911.8569810390472, 0.1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NgpJHxhR0sqq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 4:\n",
        "  Now that we have the ebst learning rates.,\n",
        "        we choose the best optimization algorithm among the seven.\n",
        "    \n",
        "   **For this , we create the function CompareAlgos_main() as shown below**"
      ]
    },
    {
      "metadata": {
        "id": "bT3oSK5SIVi1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def CompareAlgos_main(result_dict):\n",
        "  \n",
        "    '''\n",
        "    Input: result_dict: A python dictionary conataining the result returned from ComapareAlgos_alpha().\n",
        "    \n",
        "    Output: Dictionary containing the list which is of the form [accuracy, time_spent, model_history]\n",
        "  \n",
        "    '''\n",
        "    result=result_dict\n",
        "    result_algos={'SGD':[0,0],'NADAM':[0,0],'ADAMAX':[0,0],'RMS':[0,0],'ADAM':[0,0],'ADAGrad':[0,0],'ADADelta':[0,0]}\n",
        "\n",
        "    temp=ConvModel_ADAMAX(result['ADAMAX'][1],epochs=50)\n",
        "    result_algos['ADAMAX'][0]=temp[0]\n",
        "    result_algos['ADAMAX'][1]=temp[1]\n",
        "    result_algos['ADAMAX'].append(temp[2])\n",
        "\n",
        "    temp=ConvModel_SGD(result['SGD'][1],epochs=50)\n",
        "    result_algos['SGD'][0]=temp[0]\n",
        "    result_algos['SGD'][1]=temp[1]\n",
        "    result_algos['SGD'].append(temp[2])\n",
        "\n",
        "    temp=ConvModel_RMS(result['RMS'][1],epochs=50)\n",
        "    result_algos['RMS'][0]=temp[0]\n",
        "    result_algos['RMS'][1]=temp[1]\n",
        "    result_algos['RMS'].append(temp[2])\n",
        "\n",
        "    temp=ConvModel_ADAM(result['ADAM'][1],epochs=50)\n",
        "    result_algos['ADAM'][0]=temp[0]\n",
        "    result_algos['ADAM'][1]=temp[1]\n",
        "    result_algos['ADAM'].append(temp[2])\n",
        "\n",
        "    temp=ConvModel_ADAGrad(result['ADAGrad'][1],epochs=50)\n",
        "    result_algos['ADAGrad'][0]=temp[0]\n",
        "    result_algos['ADAGrad'][1]=temp[1]\n",
        "    result_algos['ADAGrad'].append(temp[2])\n",
        "\n",
        "    temp=ConvModel_ADADelta(result['ADADelta'][1],epochs=50)\n",
        "    result_algos['ADADelta'][0]=temp[0]\n",
        "    result_algos['ADADelta'][1]=temp[1]\n",
        "    result_algos['ADADelta'].append(temp[2])\n",
        "\n",
        "    temp=ConvModel_NADAM(result['NADAM'][1],epochs=50)\n",
        "    result_algos['NADAM'][0]=temp[0]\n",
        "    result_algos['NADAM'][1]=temp[1]\n",
        "    result_algos['NADAM'].append(temp[2])\n",
        "\n",
        "    return result_algos\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtGNxWHH2NkH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we call the above fnction to get the best Algorithm for our model."
      ]
    },
    {
      "metadata": {
        "id": "LKn4hJ0X2Wjw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8508
        },
        "outputId": "76ff2e3e-e65b-4c16-aa27-d9696510b54d"
      },
      "cell_type": "code",
      "source": [
        "result_algo=CompareAlgos_main(result_lr)\n",
        "\n",
        "print(\"\\n\\nThe accuracy order is \")\n",
        "for i in sorted(result_algos.items(),key=lambda kv:kv[1][0],reverse=True):\n",
        "   print(i[0],\" :\" ,i[1][0])\n",
        "\n",
        "print(\"\\n\\n\\nThe time order is \")\n",
        "for i in sorted(result_algos.items(),key=lambda kv:kv[1][1],reverse=True):\n",
        "   print(i[0],\" :\" ,i[1][1]) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using ADAMAX with alpha :  0.001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "20000/20000 [==============================] - 10s 513us/step - loss: 1.8835 - acc: 0.3162 - val_loss: 1.7075 - val_acc: 0.4395\n",
            "Epoch 2/50\n",
            "20000/20000 [==============================] - 4s 204us/step - loss: 1.5833 - acc: 0.4323 - val_loss: 1.5115 - val_acc: 0.4850\n",
            "Epoch 3/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 1.4408 - acc: 0.4841 - val_loss: 1.3911 - val_acc: 0.5180\n",
            "Epoch 4/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 1.3446 - acc: 0.5182 - val_loss: 1.3041 - val_acc: 0.5495\n",
            "Epoch 5/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 1.2522 - acc: 0.5559 - val_loss: 1.2453 - val_acc: 0.5670\n",
            "Epoch 6/50\n",
            "20000/20000 [==============================] - 4s 205us/step - loss: 1.1953 - acc: 0.5808 - val_loss: 1.2001 - val_acc: 0.5810\n",
            "Epoch 7/50\n",
            "20000/20000 [==============================] - 4s 208us/step - loss: 1.1320 - acc: 0.6012 - val_loss: 1.1773 - val_acc: 0.5870\n",
            "Epoch 8/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 1.0842 - acc: 0.6179 - val_loss: 1.1212 - val_acc: 0.6075\n",
            "Epoch 9/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 1.0389 - acc: 0.6350 - val_loss: 1.0946 - val_acc: 0.6140\n",
            "Epoch 10/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 0.9886 - acc: 0.6518 - val_loss: 1.1105 - val_acc: 0.6260\n",
            "Epoch 11/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.9462 - acc: 0.6691 - val_loss: 1.1030 - val_acc: 0.6195\n",
            "Epoch 12/50\n",
            "20000/20000 [==============================] - 4s 206us/step - loss: 0.8936 - acc: 0.6880 - val_loss: 1.0376 - val_acc: 0.6385\n",
            "Epoch 13/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 0.8531 - acc: 0.7042 - val_loss: 1.0422 - val_acc: 0.6235\n",
            "Epoch 14/50\n",
            "20000/20000 [==============================] - 4s 204us/step - loss: 0.8101 - acc: 0.7149 - val_loss: 1.0055 - val_acc: 0.6475\n",
            "Epoch 15/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.7702 - acc: 0.7329 - val_loss: 0.9887 - val_acc: 0.6535\n",
            "Epoch 16/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.7305 - acc: 0.7485 - val_loss: 0.9946 - val_acc: 0.6520\n",
            "Epoch 17/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 0.6906 - acc: 0.7571 - val_loss: 1.0259 - val_acc: 0.6430\n",
            "Epoch 18/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 0.6496 - acc: 0.7738 - val_loss: 1.0052 - val_acc: 0.6460\n",
            "Epoch 19/50\n",
            "20000/20000 [==============================] - 4s 204us/step - loss: 0.6109 - acc: 0.7859 - val_loss: 0.9946 - val_acc: 0.6655\n",
            "Epoch 20/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.5808 - acc: 0.7992 - val_loss: 1.0130 - val_acc: 0.6480\n",
            "Epoch 21/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.5488 - acc: 0.8093 - val_loss: 0.9913 - val_acc: 0.6575\n",
            "Epoch 22/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.5147 - acc: 0.8230 - val_loss: 1.0188 - val_acc: 0.6585\n",
            "Epoch 23/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.4738 - acc: 0.8361 - val_loss: 1.0221 - val_acc: 0.6595\n",
            "Epoch 24/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.4494 - acc: 0.8487 - val_loss: 1.0204 - val_acc: 0.6665\n",
            "Epoch 25/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.4119 - acc: 0.8614 - val_loss: 1.0386 - val_acc: 0.6675\n",
            "Epoch 26/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.3930 - acc: 0.8677 - val_loss: 1.0423 - val_acc: 0.6570\n",
            "Epoch 27/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.3726 - acc: 0.8740 - val_loss: 1.0572 - val_acc: 0.6625\n",
            "Epoch 28/50\n",
            "20000/20000 [==============================] - 4s 204us/step - loss: 0.3373 - acc: 0.8877 - val_loss: 1.0808 - val_acc: 0.6615\n",
            "Epoch 29/50\n",
            "20000/20000 [==============================] - 4s 204us/step - loss: 0.3157 - acc: 0.8963 - val_loss: 1.0915 - val_acc: 0.6630\n",
            "Epoch 30/50\n",
            "20000/20000 [==============================] - 4s 204us/step - loss: 0.3048 - acc: 0.8991 - val_loss: 1.0970 - val_acc: 0.6595\n",
            "Epoch 31/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.2769 - acc: 0.9084 - val_loss: 1.1120 - val_acc: 0.6625\n",
            "Epoch 32/50\n",
            "20000/20000 [==============================] - 4s 204us/step - loss: 0.2605 - acc: 0.9141 - val_loss: 1.1436 - val_acc: 0.6720\n",
            "Epoch 33/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 0.2515 - acc: 0.9153 - val_loss: 1.1572 - val_acc: 0.6635\n",
            "Epoch 34/50\n",
            "20000/20000 [==============================] - 4s 204us/step - loss: 0.2417 - acc: 0.9216 - val_loss: 1.1580 - val_acc: 0.6665\n",
            "Epoch 35/50\n",
            "20000/20000 [==============================] - 4s 204us/step - loss: 0.2231 - acc: 0.9286 - val_loss: 1.1689 - val_acc: 0.6665\n",
            "Epoch 36/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 0.2047 - acc: 0.9335 - val_loss: 1.1734 - val_acc: 0.6720\n",
            "Epoch 37/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.1972 - acc: 0.9356 - val_loss: 1.1983 - val_acc: 0.6595\n",
            "Epoch 38/50\n",
            "20000/20000 [==============================] - 4s 205us/step - loss: 0.1802 - acc: 0.9429 - val_loss: 1.1923 - val_acc: 0.6760\n",
            "Epoch 39/50\n",
            "20000/20000 [==============================] - 4s 204us/step - loss: 0.1744 - acc: 0.9452 - val_loss: 1.2271 - val_acc: 0.6645\n",
            "Epoch 40/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.1676 - acc: 0.9464 - val_loss: 1.2214 - val_acc: 0.6625\n",
            "Epoch 41/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.1576 - acc: 0.9503 - val_loss: 1.2618 - val_acc: 0.6675\n",
            "Epoch 42/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 0.1459 - acc: 0.9542 - val_loss: 1.2800 - val_acc: 0.6650\n",
            "Epoch 43/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.1478 - acc: 0.9537 - val_loss: 1.2599 - val_acc: 0.6680\n",
            "Epoch 44/50\n",
            "20000/20000 [==============================] - 4s 205us/step - loss: 0.1296 - acc: 0.9606 - val_loss: 1.2769 - val_acc: 0.6690\n",
            "Epoch 45/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.1247 - acc: 0.9605 - val_loss: 1.3297 - val_acc: 0.6645\n",
            "Epoch 46/50\n",
            "20000/20000 [==============================] - 4s 207us/step - loss: 0.1162 - acc: 0.9638 - val_loss: 1.3392 - val_acc: 0.6645\n",
            "Epoch 47/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.1159 - acc: 0.9636 - val_loss: 1.3516 - val_acc: 0.6670\n",
            "Epoch 48/50\n",
            "20000/20000 [==============================] - 4s 204us/step - loss: 0.1125 - acc: 0.9642 - val_loss: 1.3489 - val_acc: 0.6660\n",
            "Epoch 49/50\n",
            "20000/20000 [==============================] - 4s 207us/step - loss: 0.1112 - acc: 0.9656 - val_loss: 1.3544 - val_acc: 0.6605\n",
            "Epoch 50/50\n",
            "20000/20000 [==============================] - 4s 204us/step - loss: 0.1013 - acc: 0.9682 - val_loss: 1.3805 - val_acc: 0.6690\n",
            "Accuracy: 66.90%\n",
            "\n",
            "Using SGD with alpha :  0.01\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "20000/20000 [==============================] - 10s 503us/step - loss: 1.9763 - acc: 0.2837 - val_loss: 1.7347 - val_acc: 0.4030\n",
            "Epoch 2/50\n",
            "20000/20000 [==============================] - 4s 183us/step - loss: 1.6723 - acc: 0.4004 - val_loss: 1.5545 - val_acc: 0.4510\n",
            "Epoch 3/50\n",
            "20000/20000 [==============================] - 4s 180us/step - loss: 1.5469 - acc: 0.4444 - val_loss: 1.4456 - val_acc: 0.4845\n",
            "Epoch 4/50\n",
            "20000/20000 [==============================] - 4s 181us/step - loss: 1.4368 - acc: 0.4814 - val_loss: 1.3954 - val_acc: 0.5035\n",
            "Epoch 5/50\n",
            "20000/20000 [==============================] - 4s 183us/step - loss: 1.3562 - acc: 0.5111 - val_loss: 1.2970 - val_acc: 0.5430\n",
            "Epoch 6/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 1.2868 - acc: 0.5413 - val_loss: 1.2426 - val_acc: 0.5545\n",
            "Epoch 7/50\n",
            "20000/20000 [==============================] - 4s 181us/step - loss: 1.2258 - acc: 0.5574 - val_loss: 1.2580 - val_acc: 0.5510\n",
            "Epoch 8/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 1.1630 - acc: 0.5846 - val_loss: 1.1999 - val_acc: 0.5830\n",
            "Epoch 9/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 1.1100 - acc: 0.6024 - val_loss: 1.1779 - val_acc: 0.5755\n",
            "Epoch 10/50\n",
            "20000/20000 [==============================] - 4s 180us/step - loss: 1.0547 - acc: 0.6229 - val_loss: 1.1407 - val_acc: 0.5925\n",
            "Epoch 11/50\n",
            "20000/20000 [==============================] - 4s 181us/step - loss: 1.0098 - acc: 0.6406 - val_loss: 1.1540 - val_acc: 0.5915\n",
            "Epoch 12/50\n",
            "20000/20000 [==============================] - 4s 183us/step - loss: 0.9441 - acc: 0.6651 - val_loss: 1.1343 - val_acc: 0.5995\n",
            "Epoch 13/50\n",
            "20000/20000 [==============================] - 4s 181us/step - loss: 0.8845 - acc: 0.6881 - val_loss: 1.1247 - val_acc: 0.6055\n",
            "Epoch 14/50\n",
            "20000/20000 [==============================] - 4s 180us/step - loss: 0.8462 - acc: 0.6995 - val_loss: 1.1025 - val_acc: 0.6160\n",
            "Epoch 15/50\n",
            "20000/20000 [==============================] - 4s 180us/step - loss: 0.7821 - acc: 0.7252 - val_loss: 1.1103 - val_acc: 0.6150\n",
            "Epoch 16/50\n",
            "20000/20000 [==============================] - 4s 180us/step - loss: 0.7368 - acc: 0.7385 - val_loss: 1.1144 - val_acc: 0.6130\n",
            "Epoch 17/50\n",
            "20000/20000 [==============================] - 4s 180us/step - loss: 0.6902 - acc: 0.7545 - val_loss: 1.1265 - val_acc: 0.6120\n",
            "Epoch 18/50\n",
            "20000/20000 [==============================] - 4s 181us/step - loss: 0.6466 - acc: 0.7712 - val_loss: 1.1376 - val_acc: 0.6105\n",
            "Epoch 19/50\n",
            "20000/20000 [==============================] - 4s 181us/step - loss: 0.6063 - acc: 0.7844 - val_loss: 1.1582 - val_acc: 0.6235\n",
            "Epoch 20/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 0.5559 - acc: 0.8050 - val_loss: 1.1698 - val_acc: 0.6215\n",
            "Epoch 21/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 0.5289 - acc: 0.8115 - val_loss: 1.1911 - val_acc: 0.6215\n",
            "Epoch 22/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 0.4863 - acc: 0.8269 - val_loss: 1.2069 - val_acc: 0.6270\n",
            "Epoch 23/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 0.4482 - acc: 0.8453 - val_loss: 1.1975 - val_acc: 0.6210\n",
            "Epoch 24/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 0.4174 - acc: 0.8514 - val_loss: 1.2406 - val_acc: 0.6285\n",
            "Epoch 25/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 0.3866 - acc: 0.8637 - val_loss: 1.2601 - val_acc: 0.6210\n",
            "Epoch 26/50\n",
            "20000/20000 [==============================] - 4s 181us/step - loss: 0.3576 - acc: 0.8749 - val_loss: 1.2400 - val_acc: 0.6325\n",
            "Epoch 27/50\n",
            "20000/20000 [==============================] - 4s 181us/step - loss: 0.3402 - acc: 0.8832 - val_loss: 1.2663 - val_acc: 0.6340\n",
            "Epoch 28/50\n",
            "20000/20000 [==============================] - 4s 180us/step - loss: 0.3291 - acc: 0.8861 - val_loss: 1.3023 - val_acc: 0.6345\n",
            "Epoch 29/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 0.3001 - acc: 0.8942 - val_loss: 1.3325 - val_acc: 0.6265\n",
            "Epoch 30/50\n",
            "20000/20000 [==============================] - 4s 183us/step - loss: 0.2692 - acc: 0.9050 - val_loss: 1.3527 - val_acc: 0.6275\n",
            "Epoch 31/50\n",
            "20000/20000 [==============================] - 4s 183us/step - loss: 0.2587 - acc: 0.9115 - val_loss: 1.3394 - val_acc: 0.6330\n",
            "Epoch 32/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 0.2493 - acc: 0.9138 - val_loss: 1.3953 - val_acc: 0.6255\n",
            "Epoch 33/50\n",
            "20000/20000 [==============================] - 4s 185us/step - loss: 0.2319 - acc: 0.9204 - val_loss: 1.4609 - val_acc: 0.6185\n",
            "Epoch 34/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 0.2151 - acc: 0.9224 - val_loss: 1.4127 - val_acc: 0.6320\n",
            "Epoch 35/50\n",
            "20000/20000 [==============================] - 4s 181us/step - loss: 0.2088 - acc: 0.9277 - val_loss: 1.4485 - val_acc: 0.6195\n",
            "Epoch 36/50\n",
            "20000/20000 [==============================] - 4s 181us/step - loss: 0.1922 - acc: 0.9335 - val_loss: 1.4575 - val_acc: 0.6340\n",
            "Epoch 37/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 0.1800 - acc: 0.9388 - val_loss: 1.4990 - val_acc: 0.6285\n",
            "Epoch 38/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 0.1773 - acc: 0.9388 - val_loss: 1.4965 - val_acc: 0.6280\n",
            "Epoch 39/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 0.1676 - acc: 0.9429 - val_loss: 1.5062 - val_acc: 0.6355\n",
            "Epoch 40/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 0.1561 - acc: 0.9476 - val_loss: 1.5249 - val_acc: 0.6400\n",
            "Epoch 41/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 0.1508 - acc: 0.9492 - val_loss: 1.5629 - val_acc: 0.6350\n",
            "Epoch 42/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 0.1412 - acc: 0.9509 - val_loss: 1.5334 - val_acc: 0.6390\n",
            "Epoch 43/50\n",
            "20000/20000 [==============================] - 4s 183us/step - loss: 0.1370 - acc: 0.9538 - val_loss: 1.5908 - val_acc: 0.6290\n",
            "Epoch 44/50\n",
            "20000/20000 [==============================] - 4s 181us/step - loss: 0.1346 - acc: 0.9554 - val_loss: 1.5880 - val_acc: 0.6380\n",
            "Epoch 45/50\n",
            "20000/20000 [==============================] - 4s 181us/step - loss: 0.1285 - acc: 0.9564 - val_loss: 1.5627 - val_acc: 0.6420\n",
            "Epoch 46/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 0.1267 - acc: 0.9569 - val_loss: 1.6039 - val_acc: 0.6330\n",
            "Epoch 47/50\n",
            "20000/20000 [==============================] - 4s 182us/step - loss: 0.1221 - acc: 0.9593 - val_loss: 1.6046 - val_acc: 0.6285\n",
            "Epoch 48/50\n",
            "20000/20000 [==============================] - 4s 181us/step - loss: 0.1122 - acc: 0.9636 - val_loss: 1.6127 - val_acc: 0.6320\n",
            "Epoch 49/50\n",
            "20000/20000 [==============================] - 4s 181us/step - loss: 0.1121 - acc: 0.9629 - val_loss: 1.6016 - val_acc: 0.6345\n",
            "Epoch 50/50\n",
            "20000/20000 [==============================] - 4s 181us/step - loss: 0.1038 - acc: 0.9647 - val_loss: 1.5819 - val_acc: 0.6325\n",
            "Accuracy: 63.25%\n",
            "\n",
            "Using RMS with alpha :  0.001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "20000/20000 [==============================] - 11s 531us/step - loss: 1.9995 - acc: 0.3060 - val_loss: 1.6322 - val_acc: 0.4400\n",
            "Epoch 2/50\n",
            "20000/20000 [==============================] - 4s 199us/step - loss: 1.4585 - acc: 0.4879 - val_loss: 1.3249 - val_acc: 0.5455\n",
            "Epoch 3/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 1.2632 - acc: 0.5557 - val_loss: 1.2537 - val_acc: 0.5610\n",
            "Epoch 4/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 1.1119 - acc: 0.6092 - val_loss: 1.1176 - val_acc: 0.6080\n",
            "Epoch 5/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 1.0037 - acc: 0.6501 - val_loss: 1.1327 - val_acc: 0.5965\n",
            "Epoch 6/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.8986 - acc: 0.6857 - val_loss: 1.0603 - val_acc: 0.6225\n",
            "Epoch 7/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 0.8211 - acc: 0.7153 - val_loss: 1.0929 - val_acc: 0.6160\n",
            "Epoch 8/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.7430 - acc: 0.7389 - val_loss: 1.0813 - val_acc: 0.6240\n",
            "Epoch 9/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.6758 - acc: 0.7664 - val_loss: 1.0881 - val_acc: 0.6265\n",
            "Epoch 10/50\n",
            "20000/20000 [==============================] - 4s 203us/step - loss: 0.6092 - acc: 0.7909 - val_loss: 1.0441 - val_acc: 0.6450\n",
            "Epoch 11/50\n",
            "20000/20000 [==============================] - 4s 199us/step - loss: 0.5409 - acc: 0.8170 - val_loss: 1.1343 - val_acc: 0.6245\n",
            "Epoch 12/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.4830 - acc: 0.8340 - val_loss: 1.0370 - val_acc: 0.6535\n",
            "Epoch 13/50\n",
            "20000/20000 [==============================] - 4s 200us/step - loss: 0.4375 - acc: 0.8501 - val_loss: 1.0795 - val_acc: 0.6665\n",
            "Epoch 14/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 0.4015 - acc: 0.8620 - val_loss: 1.2296 - val_acc: 0.6395\n",
            "Epoch 15/50\n",
            "20000/20000 [==============================] - 4s 200us/step - loss: 0.3527 - acc: 0.8800 - val_loss: 1.1835 - val_acc: 0.6475\n",
            "Epoch 16/50\n",
            "20000/20000 [==============================] - 4s 204us/step - loss: 0.3239 - acc: 0.8899 - val_loss: 1.2690 - val_acc: 0.6360\n",
            "Epoch 17/50\n",
            "20000/20000 [==============================] - 4s 197us/step - loss: 0.2985 - acc: 0.8986 - val_loss: 1.1644 - val_acc: 0.6430\n",
            "Epoch 18/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.2732 - acc: 0.9094 - val_loss: 1.2481 - val_acc: 0.6460\n",
            "Epoch 19/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.2608 - acc: 0.9140 - val_loss: 1.3729 - val_acc: 0.6380\n",
            "Epoch 20/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 0.2428 - acc: 0.9200 - val_loss: 1.3832 - val_acc: 0.6660\n",
            "Epoch 21/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.2330 - acc: 0.9241 - val_loss: 1.4461 - val_acc: 0.6440\n",
            "Epoch 22/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.2173 - acc: 0.9273 - val_loss: 1.2917 - val_acc: 0.6510\n",
            "Epoch 23/50\n",
            "20000/20000 [==============================] - 4s 200us/step - loss: 0.2100 - acc: 0.9330 - val_loss: 1.3026 - val_acc: 0.6525\n",
            "Epoch 24/50\n",
            "20000/20000 [==============================] - 4s 199us/step - loss: 0.2039 - acc: 0.9330 - val_loss: 1.4129 - val_acc: 0.6530\n",
            "Epoch 25/50\n",
            "20000/20000 [==============================] - 4s 200us/step - loss: 0.1983 - acc: 0.9362 - val_loss: 1.3395 - val_acc: 0.6430\n",
            "Epoch 26/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 0.1934 - acc: 0.9358 - val_loss: 1.2774 - val_acc: 0.6395\n",
            "Epoch 27/50\n",
            "20000/20000 [==============================] - 4s 204us/step - loss: 0.1863 - acc: 0.9398 - val_loss: 1.2626 - val_acc: 0.6755\n",
            "Epoch 28/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 0.1855 - acc: 0.9399 - val_loss: 1.2729 - val_acc: 0.6580\n",
            "Epoch 29/50\n",
            "20000/20000 [==============================] - 4s 198us/step - loss: 0.1832 - acc: 0.9426 - val_loss: 1.3009 - val_acc: 0.6535\n",
            "Epoch 30/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.1764 - acc: 0.9429 - val_loss: 1.3446 - val_acc: 0.6460\n",
            "Epoch 31/50\n",
            "20000/20000 [==============================] - 4s 200us/step - loss: 0.1814 - acc: 0.9424 - val_loss: 1.2650 - val_acc: 0.6640\n",
            "Epoch 32/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.1767 - acc: 0.9429 - val_loss: 1.3344 - val_acc: 0.6665\n",
            "Epoch 33/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.1778 - acc: 0.9415 - val_loss: 1.3499 - val_acc: 0.6410\n",
            "Epoch 34/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.1715 - acc: 0.9458 - val_loss: 1.4529 - val_acc: 0.6525\n",
            "Epoch 35/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 0.1694 - acc: 0.9466 - val_loss: 1.4141 - val_acc: 0.6375\n",
            "Epoch 36/50\n",
            "20000/20000 [==============================] - 4s 200us/step - loss: 0.1778 - acc: 0.9443 - val_loss: 1.3885 - val_acc: 0.6630\n",
            "Epoch 37/50\n",
            "20000/20000 [==============================] - 4s 200us/step - loss: 0.1759 - acc: 0.9456 - val_loss: 1.4748 - val_acc: 0.6675\n",
            "Epoch 38/50\n",
            "20000/20000 [==============================] - 4s 200us/step - loss: 0.1663 - acc: 0.9466 - val_loss: 1.4144 - val_acc: 0.6585\n",
            "Epoch 39/50\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 0.1746 - acc: 0.9448 - val_loss: 1.2565 - val_acc: 0.6420\n",
            "Epoch 40/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.1763 - acc: 0.9434 - val_loss: 1.4955 - val_acc: 0.6740\n",
            "Epoch 41/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.1697 - acc: 0.9465 - val_loss: 1.2368 - val_acc: 0.6450\n",
            "Epoch 42/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.1709 - acc: 0.9460 - val_loss: 1.4324 - val_acc: 0.6620\n",
            "Epoch 43/50\n",
            "20000/20000 [==============================] - 4s 200us/step - loss: 0.1627 - acc: 0.9487 - val_loss: 1.6106 - val_acc: 0.6695\n",
            "Epoch 44/50\n",
            "20000/20000 [==============================] - 4s 199us/step - loss: 0.1654 - acc: 0.9466 - val_loss: 1.4588 - val_acc: 0.6710\n",
            "Epoch 45/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.1700 - acc: 0.9456 - val_loss: 1.3590 - val_acc: 0.6695\n",
            "Epoch 46/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.1665 - acc: 0.9476 - val_loss: 1.4437 - val_acc: 0.6680\n",
            "Epoch 47/50\n",
            "20000/20000 [==============================] - 4s 200us/step - loss: 0.1684 - acc: 0.9464 - val_loss: 1.2769 - val_acc: 0.6480\n",
            "Epoch 48/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.1699 - acc: 0.9462 - val_loss: 1.3886 - val_acc: 0.6505\n",
            "Epoch 49/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.1668 - acc: 0.9477 - val_loss: 1.5865 - val_acc: 0.6630\n",
            "Epoch 50/50\n",
            "20000/20000 [==============================] - 4s 201us/step - loss: 0.1671 - acc: 0.9470 - val_loss: 1.2618 - val_acc: 0.6620\n",
            "Accuracy: 66.20%\n",
            "\n",
            "Using ADAM with alpha :  0.001\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "20000/20000 [==============================] - 11s 556us/step - loss: 1.7968 - acc: 0.3491 - val_loss: 1.5042 - val_acc: 0.4760\n",
            "Epoch 2/50\n",
            "20000/20000 [==============================] - 4s 216us/step - loss: 1.4149 - acc: 0.4914 - val_loss: 1.2868 - val_acc: 0.5645\n",
            "Epoch 3/50\n",
            "20000/20000 [==============================] - 4s 216us/step - loss: 1.2295 - acc: 0.5579 - val_loss: 1.1376 - val_acc: 0.5930\n",
            "Epoch 4/50\n",
            "20000/20000 [==============================] - 4s 214us/step - loss: 1.0831 - acc: 0.6117 - val_loss: 1.0814 - val_acc: 0.6145\n",
            "Epoch 5/50\n",
            "20000/20000 [==============================] - 4s 217us/step - loss: 0.9880 - acc: 0.6498 - val_loss: 1.0269 - val_acc: 0.6305\n",
            "Epoch 6/50\n",
            "20000/20000 [==============================] - 4s 218us/step - loss: 0.9025 - acc: 0.6840 - val_loss: 1.0068 - val_acc: 0.6455\n",
            "Epoch 7/50\n",
            "20000/20000 [==============================] - 4s 217us/step - loss: 0.8370 - acc: 0.7055 - val_loss: 0.9936 - val_acc: 0.6530\n",
            "Epoch 8/50\n",
            "20000/20000 [==============================] - 4s 214us/step - loss: 0.7718 - acc: 0.7264 - val_loss: 0.9880 - val_acc: 0.6475\n",
            "Epoch 9/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.7179 - acc: 0.7441 - val_loss: 0.9796 - val_acc: 0.6550\n",
            "Epoch 10/50\n",
            "20000/20000 [==============================] - 4s 214us/step - loss: 0.6622 - acc: 0.7669 - val_loss: 0.9607 - val_acc: 0.6580\n",
            "Epoch 11/50\n",
            "20000/20000 [==============================] - 4s 214us/step - loss: 0.6208 - acc: 0.7814 - val_loss: 0.9612 - val_acc: 0.6625\n",
            "Epoch 12/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.5744 - acc: 0.7989 - val_loss: 0.9850 - val_acc: 0.6590\n",
            "Epoch 13/50\n",
            "20000/20000 [==============================] - 4s 219us/step - loss: 0.5320 - acc: 0.8154 - val_loss: 0.9750 - val_acc: 0.6590\n",
            "Epoch 14/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.4924 - acc: 0.8271 - val_loss: 1.0022 - val_acc: 0.6560\n",
            "Epoch 15/50\n",
            "20000/20000 [==============================] - 4s 216us/step - loss: 0.4595 - acc: 0.8418 - val_loss: 1.0236 - val_acc: 0.6580\n",
            "Epoch 16/50\n",
            "20000/20000 [==============================] - 4s 218us/step - loss: 0.4373 - acc: 0.8463 - val_loss: 1.0207 - val_acc: 0.6580\n",
            "Epoch 17/50\n",
            "20000/20000 [==============================] - 4s 217us/step - loss: 0.4038 - acc: 0.8611 - val_loss: 0.9932 - val_acc: 0.6710\n",
            "Epoch 18/50\n",
            "20000/20000 [==============================] - 4s 214us/step - loss: 0.3997 - acc: 0.8605 - val_loss: 1.0056 - val_acc: 0.6685\n",
            "Epoch 19/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.3729 - acc: 0.8705 - val_loss: 1.0504 - val_acc: 0.6620\n",
            "Epoch 20/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.3432 - acc: 0.8827 - val_loss: 1.0717 - val_acc: 0.6595\n",
            "Epoch 21/50\n",
            "20000/20000 [==============================] - 4s 216us/step - loss: 0.3279 - acc: 0.8868 - val_loss: 1.0550 - val_acc: 0.6675\n",
            "Epoch 22/50\n",
            "20000/20000 [==============================] - 4s 216us/step - loss: 0.3158 - acc: 0.8922 - val_loss: 1.0851 - val_acc: 0.6605\n",
            "Epoch 23/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.3077 - acc: 0.8954 - val_loss: 1.1369 - val_acc: 0.6505\n",
            "Epoch 24/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.2926 - acc: 0.8998 - val_loss: 1.1030 - val_acc: 0.6640\n",
            "Epoch 25/50\n",
            "20000/20000 [==============================] - 4s 214us/step - loss: 0.2700 - acc: 0.9075 - val_loss: 1.0926 - val_acc: 0.6675\n",
            "Epoch 26/50\n",
            "20000/20000 [==============================] - 4s 214us/step - loss: 0.2752 - acc: 0.9055 - val_loss: 1.1003 - val_acc: 0.6720\n",
            "Epoch 27/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.2631 - acc: 0.9103 - val_loss: 1.1502 - val_acc: 0.6620\n",
            "Epoch 28/50\n",
            "20000/20000 [==============================] - 4s 216us/step - loss: 0.2464 - acc: 0.9140 - val_loss: 1.1618 - val_acc: 0.6550\n",
            "Epoch 29/50\n",
            "20000/20000 [==============================] - 4s 214us/step - loss: 0.2419 - acc: 0.9176 - val_loss: 1.1711 - val_acc: 0.6585\n",
            "Epoch 30/50\n",
            "20000/20000 [==============================] - 4s 214us/step - loss: 0.2387 - acc: 0.9176 - val_loss: 1.1779 - val_acc: 0.6500\n",
            "Epoch 31/50\n",
            "20000/20000 [==============================] - 4s 214us/step - loss: 0.2303 - acc: 0.9189 - val_loss: 1.2044 - val_acc: 0.6540\n",
            "Epoch 32/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.2178 - acc: 0.9226 - val_loss: 1.1813 - val_acc: 0.6735\n",
            "Epoch 33/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.2269 - acc: 0.9212 - val_loss: 1.1969 - val_acc: 0.6610\n",
            "Epoch 34/50\n",
            "20000/20000 [==============================] - 4s 216us/step - loss: 0.2124 - acc: 0.9294 - val_loss: 1.2398 - val_acc: 0.6575\n",
            "Epoch 35/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.2065 - acc: 0.9301 - val_loss: 1.1670 - val_acc: 0.6700\n",
            "Epoch 36/50\n",
            "20000/20000 [==============================] - 4s 214us/step - loss: 0.2070 - acc: 0.9295 - val_loss: 1.2291 - val_acc: 0.6700\n",
            "Epoch 37/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.1955 - acc: 0.9343 - val_loss: 1.2025 - val_acc: 0.6670\n",
            "Epoch 38/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.1914 - acc: 0.9351 - val_loss: 1.1732 - val_acc: 0.6780\n",
            "Epoch 39/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.1890 - acc: 0.9358 - val_loss: 1.1888 - val_acc: 0.6735\n",
            "Epoch 40/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.1816 - acc: 0.9385 - val_loss: 1.2526 - val_acc: 0.6650\n",
            "Epoch 41/50\n",
            "20000/20000 [==============================] - 4s 221us/step - loss: 0.1789 - acc: 0.9407 - val_loss: 1.2384 - val_acc: 0.6605\n",
            "Epoch 42/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.1727 - acc: 0.9412 - val_loss: 1.2526 - val_acc: 0.6630\n",
            "Epoch 43/50\n",
            "20000/20000 [==============================] - 4s 216us/step - loss: 0.1660 - acc: 0.9418 - val_loss: 1.2792 - val_acc: 0.6670\n",
            "Epoch 44/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.1703 - acc: 0.9438 - val_loss: 1.2655 - val_acc: 0.6655\n",
            "Epoch 45/50\n",
            "20000/20000 [==============================] - 4s 214us/step - loss: 0.1669 - acc: 0.9436 - val_loss: 1.2896 - val_acc: 0.6695\n",
            "Epoch 46/50\n",
            "20000/20000 [==============================] - 4s 214us/step - loss: 0.1610 - acc: 0.9460 - val_loss: 1.2747 - val_acc: 0.6655\n",
            "Epoch 47/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.1709 - acc: 0.9429 - val_loss: 1.2690 - val_acc: 0.6765\n",
            "Epoch 48/50\n",
            "20000/20000 [==============================] - 4s 216us/step - loss: 0.1634 - acc: 0.9434 - val_loss: 1.2618 - val_acc: 0.6740\n",
            "Epoch 49/50\n",
            "20000/20000 [==============================] - 4s 216us/step - loss: 0.1545 - acc: 0.9477 - val_loss: 1.2688 - val_acc: 0.6820\n",
            "Epoch 50/50\n",
            "20000/20000 [==============================] - 4s 215us/step - loss: 0.1464 - acc: 0.9500 - val_loss: 1.2619 - val_acc: 0.6665\n",
            "Accuracy: 66.65%\n",
            "\n",
            "Using ADAGrad with alpha :  0.01\n",
            "Train on 20000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "20000/20000 [==============================] - 11s 531us/step - loss: 1.9731 - acc: 0.2974 - val_loss: 1.9016 - val_acc: 0.3270\n",
            "Epoch 2/50\n",
            "20000/20000 [==============================] - 4s 193us/step - loss: 1.4787 - acc: 0.4714 - val_loss: 1.4255 - val_acc: 0.4945\n",
            "Epoch 3/50\n",
            "20000/20000 [==============================] - 4s 194us/step - loss: 1.3290 - acc: 0.5244 - val_loss: 1.3198 - val_acc: 0.5310\n",
            "Epoch 4/50\n",
            "20000/20000 [==============================] - 4s 195us/step - loss: 1.2156 - acc: 0.5675 - val_loss: 1.2901 - val_acc: 0.5280\n",
            "Epoch 5/50\n",
            "20000/20000 [==============================] - 4s 194us/step - loss: 1.1254 - acc: 0.6023 - val_loss: 1.1978 - val_acc: 0.5720\n",
            "Epoch 6/50\n",
            "20000/20000 [==============================] - 4s 195us/step - loss: 1.0517 - acc: 0.6301 - val_loss: 1.1361 - val_acc: 0.5875\n",
            "Epoch 7/50\n",
            "20000/20000 [==============================] - 4s 196us/step - loss: 0.9840 - acc: 0.6533 - val_loss: 1.0910 - val_acc: 0.6235\n",
            "Epoch 8/50\n",
            "20000/20000 [==============================] - 4s 194us/step - loss: 0.9192 - acc: 0.6741 - val_loss: 1.0798 - val_acc: 0.6185\n",
            "Epoch 9/50\n",
            "20000/20000 [==============================] - 4s 193us/step - loss: 0.8622 - acc: 0.7008 - val_loss: 1.1530 - val_acc: 0.5945\n",
            "Epoch 10/50\n",
            "20000/20000 [==============================] - 4s 194us/step - loss: 0.8207 - acc: 0.7158 - val_loss: 1.0578 - val_acc: 0.6295\n",
            "Epoch 11/50\n",
            "20000/20000 [==============================] - 4s 194us/step - loss: 0.7661 - acc: 0.7349 - val_loss: 1.0536 - val_acc: 0.6360\n",
            "Epoch 12/50\n",
            "20000/20000 [==============================] - 4s 194us/step - loss: 0.7285 - acc: 0.7509 - val_loss: 1.0541 - val_acc: 0.6450\n",
            "Epoch 13/50\n",
            "20000/20000 [==============================] - 4s 194us/step - loss: 0.6895 - acc: 0.7654 - val_loss: 1.0549 - val_acc: 0.6450\n",
            "Epoch 14/50\n",
            "20000/20000 [==============================] - 4s 193us/step - loss: 0.6523 - acc: 0.7787 - val_loss: 1.0411 - val_acc: 0.6430\n",
            "Epoch 15/50\n",
            "20000/20000 [==============================] - 4s 195us/step - loss: 0.6143 - acc: 0.7919 - val_loss: 1.0369 - val_acc: 0.6400\n",
            "Epoch 16/50\n",
            "20000/20000 [==============================] - 4s 193us/step - loss: 0.5907 - acc: 0.8000 - val_loss: 1.0143 - val_acc: 0.6475\n",
            "Epoch 17/50\n",
            "20000/20000 [==============================] - 4s 194us/step - loss: 0.5592 - acc: 0.8129 - val_loss: 1.0088 - val_acc: 0.6520\n",
            "Epoch 18/50\n",
            "20000/20000 [==============================] - 4s 194us/step - loss: 0.5352 - acc: 0.8208 - val_loss: 1.0364 - val_acc: 0.6505\n",
            "Epoch 19/50\n",
            "20000/20000 [==============================] - 4s 193us/step - loss: 0.5094 - acc: 0.8291 - val_loss: 1.0520 - val_acc: 0.6475\n",
            "Epoch 20/50\n",
            "20000/20000 [==============================] - 4s 195us/step - loss: 0.4838 - acc: 0.8365 - val_loss: 1.0183 - val_acc: 0.6540\n",
            "Epoch 21/50\n",
            "20000/20000 [==============================] - 4s 194us/step - loss: 0.4686 - acc: 0.8464 - val_loss: 1.0447 - val_acc: 0.6580\n",
            "Epoch 22/50\n",
            "20000/20000 [==============================] - 4s 193us/step - loss: 0.4431 - acc: 0.8547 - val_loss: 1.0428 - val_acc: 0.6655\n",
            "Epoch 23/50\n",
            "20000/20000 [==============================] - 4s 193us/step - loss: 0.4201 - acc: 0.8601 - val_loss: 1.0499 - val_acc: 0.6550\n",
            "Epoch 24/50\n",
            "18560/20000 [==========================>...] - ETA: 0s - loss: 0.4118 - acc: 0.8636"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lrLx6zt44z1Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 5:\n",
        "   **Plot the result**\n",
        "            The history objects will come in handy now\n",
        "            \n",
        "            First, lets plot the accuracy graph."
      ]
    },
    {
      "metadata": {
        "id": "sSHXxN4d5AX0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hist_ADADelta=result_algo['ADADelta'][2]\n",
        "hist_ADAGrad=result_algo['ADAGrad'][2]\n",
        "hist_ADAM=result_algo['ADAM'][2]\n",
        "hist_ADAMAX=result_algo['ADAMAX'][2]\n",
        "hist_NADAM=result_algo['NADAM'][2]\n",
        "hist_RMS=result_algo['RMS'][2]\n",
        "hist_SGD=result_algo['SGD'][2]\n",
        "\n",
        "\n",
        "plt.plot(hist_ADADelta.history['acc'])\n",
        "#plt.plot(hist_ADADelta.history['val_acc'])\n",
        "plt.plot(hist_ADAGrad.history['acc'])\n",
        "plt.plot(hist_ADAM.history['acc'])\n",
        "plt.plot(hist_ADAMAX.history['acc'])\n",
        "plt.plot(hist_NADAM.history['acc'])\n",
        "plt.plot(hist_RMS.history['acc'])\n",
        "plt.plot(hist_SGD.history['acc'])\n",
        "\n",
        "\n",
        "#plt.plot(hist_ADAGrad.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['ADADelta', 'ADAGrad','ADAM','ADAMAX','NADAM','RMS','SGD'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ttp7MXoT5ciq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, lets plot the error/loss graph:"
      ]
    },
    {
      "metadata": {
        "id": "sX2OQf6G5kxx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(hist_ADADelta.history['loss'])\n",
        "#plt.plot(hist_ADADelta.history['val_acc'])\n",
        "plt.plot(hist_ADAGrad.history['loss'])\n",
        "plt.plot(hist_ADAM.history['loss'])\n",
        "plt.plot(hist_ADAMAX.history['loss'])\n",
        "plt.plot(hist_NADAM.history['loss'])\n",
        "plt.plot(hist_RMS.history['loss'])\n",
        "plt.plot(hist_SGD.history['loss'])\n",
        "\n",
        "\n",
        "#plt.plot(hist_ADAGrad.history['val_acc'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['ADADelta', 'ADAGrad','ADAM','ADAMAX','NADAM','RMS','SGD'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "85irfz4x52X1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Great..! The best optimisation algorithm can now be chosen easily, just by a look at the graph!"
      ]
    },
    {
      "metadata": {
        "id": "eui6vpJr6PYs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Thanks to **Google Colab** for the online GPUs for training.\n",
        "\n",
        "Again, the CNN model used here as an example has been taken from  [Jason Brownie at Machine Learning Mastery website](https://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/ ).\n"
      ]
    },
    {
      "metadata": {
        "id": "9QMe6mBY6YTL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}